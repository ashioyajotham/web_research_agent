# Research Results: Download Epoch AI's dataset of large-scale AI models. From this, extract a time series showing how the maximum amount of compute used to train any AI system has changed over time. Each entry in your response must represent a training run which, at the time it happened, set a new record for the maximum amount of compute used to train an AI system.

## Plan

1. **Search for: Download Epoch dataset large-scale models extract time series showing maximum** (using search)
2. **Extract content from search result 0** (using browser)
3. **Extract content from search result 1** (using browser)
4. **Extract content from search result 2** (using browser)
5. **Extract content from search result 3** (using browser)
6. **Organize and present findings** (using present)

## Results

### 1. 1
**Status**: success

**Search Query**: Download Epoch dataset large-scale models extract time series showing maximum
**Found**: 0 results

1. [Tracking Large-Scale AI Models](https://epoch.ai/blog/tracking-large-scale-ai-models)
   We present a dataset of 81 large-scale models, from AlphaGo to Gemini, developed across 18 countries, at the leading edge of scale and capabilities.

2. [Data on the Trajectory of AI | Epoch AI Database](https://epoch.ai/data)
   Our public database, the largest of its kind, tracks over 3000 machine learning models from 1950 to today. Explore data and graphs showing the trajectory of AI.

3. [Visualizing epoched data — MNE 1.10.1 documentation](https://mne.tools/stable/auto_tutorials/epochs/20_visualize_epochs.html)
   This tutorial shows how to plot epoched data as time series, how to plot the spectral density of epoched data, how to plot epochs as an imagemap,

4. [thuml/Large-Time-Series-Model: Official code, datasets ...](https://github.com/thuml/Large-Time-Series-Model)
   This repo provides official code, datasets and checkpoints for Timer: Generative Pre-trained Transformers Are Large Time Series Models.

5. [Optimal batch size and epochs for large models [closed]](https://stackoverflow.com/questions/61304854/optimal-batch-size-and-epochs-for-large-models)
   I read that generally between 50 and 100 epochs are common practice, but if my results are tapering off after 25 is there value to adding more.

6. [MONSTER Monash Scalable Time Series Evaluation ...](https://arxiv.org/html/2502.15122v1)
   We introduce Monster—the MONash Scalable Time Series Evaluation Repository—a collection of large datasets for time series classification.

7. [COSMOS How to extract the Gaia ancillary data using datalink](https://www.cosmos.esa.int/web/gaia-users/archive/datalink-products)
   This intermediate-level tutorial introduces the concepts needed to retrieve these non-tabular products using the Gaia ESA Archive web interface via its ...

8. [Time-series data analysis using LSTM (Tutorial)](https://www.kaggle.com/code/amirrezaeian/time-series-data-analysis-using-lstm-tutorial)
   This Notebook is a sort of tutorial for the beginners in Deep Learning and time-series data analysis.

9. [How to Select a Model For Time Series Prediction Task ...](https://neptune.ai/blog/select-model-for-time-series-prediction-task)
   This guide explains how to select and evaluate time series models based on predictive performance—including classical, supervised, and deep learning-based ...

10. [Time series forecasting | TensorFlow Core](https://www.tensorflow.org/tutorials/structured_data/time_series)
   This tutorial is an introduction to time series forecasting using TensorFlow. It builds a few different styles of models including Convolutional and Recurrent ...

### 2. 2
**Status**: success

```json
{
  "status": "success",
  "url": "https://epoch.ai/blog/tracking-large-scale-ai-models",
  "title": "Tracking Large-Scale AI Models | 81 models across 18 countries | Epoch AI",
  "extracted_text": "Tracking Large-Scale AI Models | 81 models across 18 countries | Epoch AI Publications & Commentary Publications Newsletter Podcast Data & Resources Datasets Overview AI Benchmarking AI Models ML Hardware GPU Clusters Resources AI Trends & Statistics Data Insights Projects FrontierMath GATE Playground Distributed Training Model Counts About About Us Our Team Careers Consultations Our Funding Donate Latest Contact Search epoch.ai Search Enter a query to search for results Placeholder Article Tracking Large-Scale AI Models report Tracking Large-Scale AI Models We present a dataset of 81 large-scale models, from AlphaGo to Gemini, developed across 18 countries, at the leading edge of scale and capabilities. Cite Published Apr 05, 2024 Authors Robi Rahman, David Owen, Josh You Resources Data Contents Results There are few models at the leading edge, but the frontier advances rapidly Most large-scale AI models are language models Most large-scale models are developed by US companies Downloadable models are common, but have lower training compute Methods for finding large-scale models Benchmarks and Repositories Non-English news and websites Other sources Unconfirmed large-scale models Outcomes and limitations Conclusion Appendix Dataset Growth of the compute frontier Credits Notes Update Explore our Large-scale AI models dataset through interactive visualizations and documentation on our dedicated data page. We present a new dataset tracking AI models with training compute over 10 23 floating point operations (FLOP). This corresponds to training costs of hundreds of thousands of dollars or more. 1 We have identified 81 such models, and another 86 models that may exceed the 10 23 FLOP threshold but don\u2019t have confirmed training details. Our previous work has examined the crucial role of training compute in the development of modern AI, and how it drives model capabilities . Existing AI regulation explicitly acknowledges the importance of training compute: both the recent US Executive Order on AI development and the EU AI Act establish reporting requirements based on compute thresholds. Motivated by these developments, we plan to track models with training compute above 10 23 FLOP by updating this dataset on an ongoing basis. We call models above this threshold \u201clarge-scale models\u201d. The dataset offers insight into several recent trends in AI development. We share our findings in more detail below, including these: The pace of large-scale model releases is accelerating. Only 11 models exceeded 10 23 FLOP in 2020. By 2024, this grew to 81 models in our dataset, and the trend shows no sign of slowing. More. The large majority of large-scale models are language models, but many others are multimodal or process images. Despite early breakthroughs in game-playing, language and image generation have dominated since 2021. More. Over half of known large-scale models were developed in the United States. A quarter were developed in China, with this proportion growing in recent years. More. Almost half of the large-scale models in our dataset had published, downloadable weights, mostly with training compute between 10 23 and 10 24 FLOP. Publicly released model weights are common, but are trained with less compute than the very largest proprietary models. More. {\"title\": \"Large-scale models\", \"xAxis\": {\"label\": \"Publication date\", \"lim\": [2016.638105022831, 2024.5997945205481], \"scaleType\": \"linear\", \"ticks\": [2016.0, 2017.0, 2018.0, 2019.0, 2020.0, 2021.0, 2022.0, 2023.0, 2024.0, 2025.0], \"tickLabels\": [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"], \"nice\": false}, \"yAxis\": {\"label\": \"Training compute (FLOP)\", \"lim\": [7.40608926488071e+22, 6.818713385008506e+25], \"scaleType\": \"log\", \"ticks\": [1e+21, 1e+22, 1.0000000000000001e+23, 1e+24, 1e+25, 1e+26, 1e+27], \"tickLabels\": [\"$\\\\mathdefault{10^{21}}$\", \"$\\\\mathdefault{10^{22}}$\", \"$\\\\mathdefault{10^{23}}$\", \"$\\\\mathdefault{10^{24}}$\", \"$\\\\mathdefault{10^{25}}$\", \"$\\\\mathdefault{10^{26}}$\", \"$\\\\mathdefault{10^{27}}$\"]}, \"showLegend\": true, \"legendPosition\": \"header\", \"showFrame\": true, \"addDataPadding\": false, \"objects\": [{\"type\": \"scatter\", \"zOrder\": 1, \"points\": [{\"x\": 2017.0, \"y\": 1.5e+23, \"tooltipData\": {\"Model\": \"AlphaGo Master\", \"Domain\": \"Games\", \"Training compute <br>(FLOP)\": \"1.50e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2017-01-01\"}, \"size\": 8}, {\"x\": 2017.7965753424658, \"y\": 3.41e+23, \"tooltipData\": {\"Model\": \"AlphaGo Zero\", \"Domain\": \"Games\", \"Training compute <br>(FLOP)\": \"3.41e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2017-10-18\"}, \"size\": 8}, {\"x\": 2020.0739726027398, \"y\": 1.12e+23, \"tooltipData\": {\"Model\": \"Meena\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.12e+23\", \"Organization\": \"Google Brain\", \"Publication date\": \"2020-01-28\"}, \"size\": 8}, {\"x\": 2020.407305936073, \"y\": 3.14e+23, \"tooltipData\": {\"Model\": \"GPT-3 175B (davinci)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.14e+23\", \"Organization\": \"OpenAI\", \"Publication date\": \"2020-05-28\"}, \"size\": 8}, {\"x\": 2021.6107305936073, \"y\": 3.7e+23, \"tooltipData\": {\"Model\": \"Jurassic-1-Jumbo\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.70e+23\", \"Organization\": \"AI21 Labs\", \"Publication date\": \"2021-08-11\"}, \"size\": 8}, {\"x\": 2021.6913242009134, \"y\": 1.476e+23, \"tooltipData\": {\"Model\": \"HyperCLOVA\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.48e+23\", \"Organization\": \"NAVER,Search Solutions\", \"Publication date\": \"2021-09-10\"}, \"size\": 8}, {\"x\": 2021.777397260274, \"y\": 1.17e+24, \"tooltipData\": {\"Model\": \"Megatron-Turing NLG 530B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.17e+24\", \"Organization\": \"Microsoft,NVIDIA\", \"Publication date\": \"2021-10-11\"}, \"size\": 8}, {\"x\": 2021.7801369863014, \"y\": 3.5380000000001e+23, \"tooltipData\": {\"Model\": \"Yuan 1.0\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.54e+23\", \"Organization\": \"Inspur\", \"Publication date\": \"2021-10-12\"}, \"size\": 8}, {\"x\": 2021.9358447488585, \"y\": 6.31e+23, \"tooltipData\": {\"Model\": \"Gopher (280B)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"6.31e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2021-12-08\"}, \"size\": 8}, {\"x\": 2021.9495433789955, \"y\": 3.74e+23, \"tooltipData\": {\"Model\": \"GLaM\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.74e+23\", \"Organization\": \"Google\", \"Publication date\": \"2021-12-13\"}, \"size\": 8}, {\"x\": 2021.9769406392695, \"y\": 1.0421e+24, \"tooltipData\": {\"Model\": \"ERNIE 3.0 Titan\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.04e+24\", \"Organization\": \"Baidu,Peng Cheng Laboratory\", \"Publication date\": \"2021-12-23\"}, \"size\": 8}, {\"x\": 2022.0860730593606, \"y\": 1.568160000001e+23, \"tooltipData\": {\"Model\": \"AlphaCode\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.57e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2022-02-02\"}, \"size\": 8}, {\"x\": 2022.10799086758, \"y\": 3.55e+23, \"tooltipData\": {\"Model\": \"LaMDA\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.55e+23\", \"Organization\": \"Google\", \"Publication date\": \"2022-02-10\"}, \"size\": 8}, {\"x\": 2022.1271689497717, \"y\": 2.9e+23, \"tooltipData\": {\"Model\": \"ST-MoE\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.90e+23\", \"Organization\": \"Google\", \"Publication date\": \"2022-02-17\"}, \"size\": 8}, {\"x\": 2022.2433789954339, \"y\": 5.76e+23, \"tooltipData\": {\"Model\": \"Chinchilla\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"5.76e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2022-03-29\"}, \"size\": 8}, {\"x\": 2022.2582191780823, \"y\": 2.5272e+24, \"tooltipData\": {\"Model\": \"PaLM (540B)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.53e+24\", \"Organization\": \"Google Research\", \"Publication date\": \"2022-04-04\"}, \"size\": 8}, {\"x\": 2022.3267123287671, \"y\": 2.7e+23, \"tooltipData\": {\"Model\": \"Flamingo\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"2.70e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2022-04-29\"}, \"size\": 8}, {\"x\": 2022.3360730593606, \"y\": 4.3e+23, \"tooltipData\": {\"Model\": \"OPT-175B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"4.30e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2022-05-02\"}, \"size\": 8}, {\"x\": 2022.35799086758, \"y\": 1.2e+23, \"tooltipData\": {\"Model\": \"UL2\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.20e+23\", \"Organization\": \"Google Research,Google Brain\", \"Publication date\": \"2022-05-10\"}, \"size\": 8}, {\"x\": 2022.4385844748858, \"y\": 5.6e+23, \"tooltipData\": {\"Model\": \"BIG-G 137B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"5.60e+23\", \"Organization\": \"Google\", \"Publication date\": \"2022-06-09\"}, \"size\": 8}, {\"x\": 2022.4714611872148, \"y\": 1.100000000001e+23, \"tooltipData\": {\"Model\": \"OPT-66B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.10e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2022-06-21\"}, \"size\": 8}, {\"x\": 2022.4742009132422, \"y\": 3.962895376192635e+23, \"tooltipData\": {\"Model\": \"Parti\", \"Domain\": \"Image generation\", \"Training compute <br>(FLOP)\": \"3.96e+23\", \"Organization\": \"Google Research\", \"Publication date\": \"2022-06-22\"}, \"size\": 8}, {\"x\": 2022.4769406392695, \"y\": 2.2e+23, \"tooltipData\": {\"Model\": \"YaLM\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.20e+23\", \"Organization\": \"Yandex\", \"Publication date\": \"2022-06-23\"}, \"size\": 8}, {\"x\": 2022.4933789954339, \"y\": 2.7415e+24, \"tooltipData\": {\"Model\": \"Minerva (540B)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.74e+24\", \"Organization\": \"Google\", \"Publication date\": \"2022-06-29\"}, \"size\": 8}, {\"x\": 2022.5860730593606, \"y\": 2.04374016e+23, \"tooltipData\": {\"Model\": \"AlexaTM 20B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.04e+23\", \"Organization\": \"Amazon\", \"Publication date\": \"2022-08-02\"}, \"size\": 8}, {\"x\": 2022.5915525114156, \"y\": 3.778e+23, \"tooltipData\": {\"Model\": \"GLM-130B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.78e+23\", \"Organization\": \"Tsinghua University\", \"Publication date\": \"2022-08-04\"}, \"size\": 8}, {\"x\": 2022.60799086758, \"y\": 4.3e+23, \"tooltipData\": {\"Model\": \"BlenderBot 3\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"4.30e+23\", \"Organization\": \"McGill University,Meta AI,Mila- Quebec AI\", \"Publication date\": \"2022-08-10\"}, \"size\": 8}, {\"x\": 2022.6216894977167, \"y\": 2.8e+23, \"tooltipData\": {\"Model\": \"Luminous-supreme\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.80e+23\", \"Organization\": \"Aleph Alpha\", \"Publication date\": \"2022-08-15\"}, \"size\": 8}, {\"x\": 2022.8020547945205, \"y\": 2.53e+24, \"tooltipData\": {\"Model\": \"U-PaLM (540B)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.53e+24\", \"Organization\": \"Google\", \"Publication date\": \"2022-10-20\"}, \"size\": 8}, {\"x\": 2022.852511415525, \"y\": 5.7700000000001e+23, \"tooltipData\": {\"Model\": \"BLOOM-176B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"5.77e+23\", \"Organization\": \"Hugging Face,BigScience\", \"Publication date\": \"2022-11-08\"}, \"size\": 8}, {\"x\": 2022.8744292237443, \"y\": 3.24e+23, \"tooltipData\": {\"Model\": \"Galactica\", \"Domain\": \"Language,Biology\", \"Training compute <br>(FLOP)\": \"3.24e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2022-11-16\"}, \"size\": 8}, {\"x\": 2022.907305936073, \"y\": 2.578e+24, \"tooltipData\": {\"Model\": \"GPT-3.5 (text-davinci-003)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.58e+24\", \"Organization\": \"OpenAI\", \"Publication date\": \"2022-11-28\"}, \"size\": 8}, {\"x\": 2022.9276255707764, \"y\": 1.1e+23, \"tooltipData\": {\"Model\": \"Whisper v2\", \"Domain\": \"Speech\", \"Training compute <br>(FLOP)\": \"1.10e+23\", \"Organization\": \"OpenAI\", \"Publication date\": \"2022-12-05\"}, \"size\": 8}, {\"x\": 2023.0027397260274, \"y\": 1.58e+23, \"tooltipData\": {\"Model\": \"SparseOPT-175B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.58e+23\", \"Organization\": \"Institute of Science and Technology Austria (ISTA),Neural Magic\", \"Publication date\": \"2023-01-02\"}, \"size\": 8}, {\"x\": 2023.10799086758, \"y\": 4.0001e+23, \"tooltipData\": {\"Model\": \"ViT-22B\", \"Domain\": \"Vision\", \"Training compute <br>(FLOP)\": \"4.00e+23\", \"Organization\": \"Google\", \"Publication date\": \"2023-02-10\"}, \"size\": 8}, {\"x\": 2023.1463470319634, \"y\": 5.5e+23, \"tooltipData\": {\"Model\": \"LLaMA-65B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"5.50e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-02-24\"}, \"size\": 8}, {\"x\": 2023.1545662100457, \"y\": 2.7300000000001e+23, \"tooltipData\": {\"Model\": \"LLaMA-33B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.73e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-02-27\"}, \"size\": 8}, {\"x\": 2023.2050228310502, \"y\": 2.4e+23, \"tooltipData\": {\"Model\": \"Falcon-40B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.40e+23\", \"Organization\": \"Technology Innovation Institute\", \"Publication date\": \"2023-03-15\"}, \"size\": 8}, {\"x\": 2023.2050228310502, \"y\": 2.1e+25, \"tooltipData\": {\"Model\": \"GPT-4\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"2.10e+25\", \"Organization\": \"OpenAI\", \"Publication date\": \"2023-03-15\"}, \"size\": 8}, {\"x\": 2023.2187214611872, \"y\": 4.67e+23, \"tooltipData\": {\"Model\": \"PanGu-\\u03a3\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"4.67e+23\", \"Organization\": \"Huawei Noah's Ark Lab\", \"Publication date\": \"2023-03-20\"}, \"size\": 8}, {\"x\": 2023.2214611872148, \"y\": 2.4e+23, \"tooltipData\": {\"Model\": \"LightOn Mini\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.40e+23\", \"Organization\": \"LightOn\", \"Publication date\": \"2023-03-21\"}, \"size\": 8}, {\"x\": 2023.2461187214612, \"y\": 2.36e+23, \"tooltipData\": {\"Model\": \"BloombergGPT\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.36e+23\", \"Organization\": \"Bloomberg,Johns Hopkins University\", \"Publication date\": \"2023-03-30\"}, \"size\": 8}, {\"x\": 2023.35799086758, \"y\": 7.34e+24, \"tooltipData\": {\"Model\": \"PaLM 2\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"7.34e+24\", \"Organization\": \"Google\", \"Publication date\": \"2023-05-10\"}, \"size\": 8}, {\"x\": 2023.4742009132422, \"y\": 1.8e+23, \"tooltipData\": {\"Model\": \"MPT-30B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.80e+23\", \"Organization\": \"MosaicML\", \"Publication date\": \"2023-06-22\"}, \"size\": 8}, {\"x\": 2023.4769406392695, \"y\": 1.0001e+24, \"tooltipData\": {\"Model\": \"Inflection-1\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.00e+24\", \"Organization\": \"Inflection AI\", \"Publication date\": \"2023-06-23\"}, \"size\": 8}, {\"x\": 2023.513698630137, \"y\": 6.0001e+23, \"tooltipData\": {\"Model\": \"xTrimoPGLM -100B\", \"Domain\": \"Biology\", \"Training compute <br>(FLOP)\": \"6.00e+23\", \"Organization\": \"Tsinghua University,BioMap Research\", \"Publication date\": \"2023-07-06\"}, \"size\": 8}, {\"x\": 2023.527397260274, \"y\": 3.866e+24, \"tooltipData\": {\"Model\": \"Claude 2\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.87e+24\", \"Organization\": \"Anthropic\", \"Publication date\": \"2023-07-11\"}, \"size\": 8}, {\"x\": 2023.5465753424658, \"y\": 8.1e+23, \"tooltipData\": {\"Model\": \"Llama 2-70B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"8.10e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-07-18\"}, \"size\": 8}, {\"x\": 2023.5465753424658, \"y\": 4.08e+23, \"tooltipData\": {\"Model\": \"Llama 2-34B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"4.08e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-07-18\"}, \"size\": 8}, {\"x\": 2023.5465753424658, \"y\": 1.6e+23, \"tooltipData\": {\"Model\": \"Llama 2-13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.60e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-07-18\"}, \"size\": 8}, {\"x\": 2023.6189497716894, \"y\": 1.1e+23, \"tooltipData\": {\"Model\": \"Code Llama-7B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.10e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-08-14\"}, \"size\": 8}, {\"x\": 2023.6189497716894, \"y\": 5.3e+23, \"tooltipData\": {\"Model\": \"Code Llama-34B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"5.30e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-08-14\"}, \"size\": 8}, {\"x\": 2023.6408675799087, \"y\": 1.1593580544e+23, \"tooltipData\": {\"Model\": \"IDEFICS\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"1.16e+23\", \"Organization\": \"Hugging Face\", \"Publication date\": \"2023-08-22\"}, \"size\": 8}, {\"x\": 2023.6803652968038, \"y\": 1.02e+24, \"tooltipData\": {\"Model\": \"TigerBot-70B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.02e+24\", \"Organization\": \"Tigerobo\", \"Publication date\": \"2023-09-06\"}, \"size\": 8}, {\"x\": 2023.6803652968038, \"y\": 3.76e+24, \"tooltipData\": {\"Model\": \"Falcon-180B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.76e+24\", \"Organization\": \"Technology Innovation Institute\", \"Publication date\": \"2023-09-06\"}, \"size\": 8}, {\"x\": 2023.6803652968038, \"y\": 2.03e+23, \"tooltipData\": {\"Model\": \"Baichuan2-13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.03e+23\", \"Organization\": \"Baichuan\", \"Publication date\": \"2023-09-06\"}, \"size\": 8}, {\"x\": 2023.7406392694065, \"y\": 1.17e+23, \"tooltipData\": {\"Model\": \"PLaMo-13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.17e+23\", \"Organization\": \"Preferred Networks Inc\", \"Publication date\": \"2023-09-28\"}, \"size\": 8}, {\"x\": 2023.7406392694065, \"y\": 1.01e+23, \"tooltipData\": {\"Model\": \"Qwen-7B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.01e+23\", \"Organization\": \"Alibaba\", \"Publication date\": \"2023-09-28\"}, \"size\": 8}, {\"x\": 2023.7406392694065, \"y\": 2.5e+23, \"tooltipData\": {\"Model\": \"Qwen-14B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.50e+23\", \"Organization\": \"Alibaba\", \"Publication date\": \"2023-09-28\"}, \"size\": 8}, {\"x\": 2023.7746575342467, \"y\": 3.3e+23, \"tooltipData\": {\"Model\": \"CodeFuse-13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.30e+23\", \"Organization\": \"Ant Group\", \"Publication date\": \"2023-10-10\"}, \"size\": 8}, {\"x\": 2023.8212328767124, \"y\": 1.09200000000001e+24, \"tooltipData\": {\"Model\": \"ChatGLM3\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"1.09e+24\", \"Organization\": \"Zhipu AI\", \"Publication date\": \"2023-10-27\"}, \"size\": 8}, {\"x\": 2023.8294520547945, \"y\": 2.5e+23, \"tooltipData\": {\"Model\": \"Skywork-13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.50e+23\", \"Organization\": \"Kunlun Inc.\", \"Publication date\": \"2023-10-30\"}, \"size\": 8}, {\"x\": 2023.8321917808219, \"y\": 1.0920000000001e+23, \"tooltipData\": {\"Model\": \"BlueLM 13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.09e+23\", \"Organization\": \"vivo AI lab\", \"Publication date\": \"2023-10-31\"}, \"size\": 8}, {\"x\": 2023.8333333333333, \"y\": 2.4e+23, \"tooltipData\": {\"Model\": \"Nanbeige-16B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.40e+23\", \"Organization\": \"Nanbeige LLM Lab\", \"Publication date\": \"2023-11-01\"}, \"size\": 8}, {\"x\": 2023.8360730593606, \"y\": 3.96e+23, \"tooltipData\": {\"Model\": \"DeepSeek Coder 33B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.96e+23\", \"Organization\": \"DeepSeek\", \"Publication date\": \"2023-11-02\"}, \"size\": 8}, {\"x\": 2023.8360730593606, \"y\": 6.1e+23, \"tooltipData\": {\"Model\": \"Yi-34B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"6.10e+23\", \"Organization\": \"01.AI\", \"Publication date\": \"2023-11-02\"}, \"size\": 8}, {\"x\": 2023.8470319634703, \"y\": 2.7e+23, \"tooltipData\": {\"Model\": \"Whisper v3\", \"Domain\": \"Speech\", \"Training compute <br>(FLOP)\": \"2.70e+23\", \"Organization\": \"OpenAI\", \"Publication date\": \"2023-11-06\"}, \"size\": 8}, {\"x\": 2023.8716894977167, \"y\": 1.8e+23, \"tooltipData\": {\"Model\": \"Nemotron-3-8B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.80e+23\", \"Organization\": \"NVIDIA\", \"Publication date\": \"2023-11-15\"}, \"size\": 8}, {\"x\": 2023.8908675799087, \"y\": 1.001e+25, \"tooltipData\": {\"Model\": \"Inflection-2\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.00e+25\", \"Organization\": \"Inflection AI\", \"Publication date\": \"2023-11-22\"}, \"size\": 8}, {\"x\": 2023.9127853881278, \"y\": 2.44e+23, \"tooltipData\": {\"Model\": \"Granite 13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.44e+23\", \"Organization\": \"IBM\", \"Publication date\": \"2023-11-30\"}, \"size\": 8}, {\"x\": 2023.9127853881278, \"y\": 1.3e+24, \"tooltipData\": {\"Model\": \"Qwen-72B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.30e+24\", \"Organization\": \"Alibaba\", \"Publication date\": \"2023-11-30\"}, \"size\": 8}, {\"x\": 2023.9303652968038, \"y\": 5.0000000001e+25, \"tooltipData\": {\"Model\": \"Gemini Ultra\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"5.00e+25\", \"Organization\": \"Google DeepMind\", \"Publication date\": \"2023-12-06\"}, \"size\": 8}, {\"x\": 2023.9358447488585, \"y\": 1.24800000000001e+24, \"tooltipData\": {\"Model\": \"XVERSE-65B-2\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.25e+24\", \"Organization\": \"XVERSE Technology\", \"Publication date\": \"2023-12-08\"}, \"size\": 8}, {\"x\": 2023.9522831050228, \"y\": 1.53e+23, \"tooltipData\": {\"Model\": \"Poro34B (700B token checkpoint)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.53e+23\", \"Organization\": \"High-Performance Language Technologies (HPLT),University of Turku\", \"Publication date\": \"2023-12-14\"}, \"size\": 8}, {\"x\": 2024.0109589041097, \"y\": 8.04e+23, \"tooltipData\": {\"Model\": \"DeepSeek LLM 67B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"8.04e+23\", \"Organization\": \"DeepSeek\", \"Publication date\": \"2024-01-05\"}, \"size\": 8}, {\"x\": 2024.0767123287671, \"y\": 1.230000000001e+24, \"tooltipData\": {\"Model\": \"Code Llama-70B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.23e+24\", \"Organization\": \"Meta AI\", \"Publication date\": \"2024-01-29\"}, \"size\": 8}, {\"x\": 2024.1381278538813, \"y\": 2.52e+23, \"tooltipData\": {\"Model\": \"Gemma 7B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.52e+23\", \"Organization\": \"Google DeepMind\", \"Publication date\": \"2024-02-21\"}, \"size\": 8}, {\"x\": 2024.151826484018, \"y\": 2.0000000001e+25, \"tooltipData\": {\"Model\": \"Mistral Large\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.00e+25\", \"Organization\": \"Mistral AI\", \"Publication date\": \"2024-02-26\"}, \"size\": 8}, {\"x\": 2024.1600456621004, \"y\": 3.87e+23, \"tooltipData\": {\"Model\": \"StarCoder 2 15B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.87e+23\", \"Organization\": \"Hugging Face,ServiceNow,NVIDIA,BigCode\", \"Publication date\": \"2024-02-29\"}, \"size\": 8}, {\"x\": 2024.1600456621004, \"y\": 1.55e+23, \"tooltipData\": {\"Model\": \"StarCoder 2 7B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.55e+23\", \"Organization\": \"Hugging Face,ServiceNow,NVIDIA,BigCode\", \"Publication date\": \"2024-02-29\"}, \"size\": 8}, {\"x\": 2024.2378995433792, \"y\": 2.6e+24, \"tooltipData\": {\"Model\": \"DBRX\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.60e+24\", \"Organization\": \"Databricks\", \"Publication date\": \"2024-03-27\"}, \"size\": 8}], \"fillColor\": \"#00A5A6\", \"marker\": \"M 0.0,-0.5 C 0.13260155,-0.5 0.25978993539242673,-0.44731684579412084 0.3535533905932738,-0.3535533905932738 C 0.44731684579412084,-0.25978993539242673 0.5,-0.13260155 0.5,0.0 C 0.5,0.13260155 0.44731684579412084,0.25978993539242673 0.3535533905932738,0.3535533905932738 C 0.25978993539242673,0.44731684579412084 0.13260155,0.5 0.0,0.5 C -0.13260155,0.5 -0.25978993539242673,0.44731684579412084 -0.3535533905932738,0.3535533905932738 C -0.44731684579412084,0.25978993539242673 -0.5,0.13260155 -0.5,0.0 C -0.5,-0.13260155 -0.44731684579412084,-0.25978993539242673 -0.3535533905932738,-0.3535533905932738 C -0.25978993539242673,-0.44731684579412084 -0.13260155,-0.5 0.0,-0.5 Z 0.0,-0.5\", \"isFilled\": true}, {\"type\": \"annotation\", \"text\": \"AlphaGo Zero\", \"x\": 2018.0965753424657, \"y\": 5.797e+23, \"ha\": \"center\", \"va\": \"bottom\", \"targetX\": 2017.7965753424658, \"targetY\": 3.41e+23, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2018.1265753424657, 5.2173e+23], 3, 0.5, [2017.8765753424657, 3.5805000000000005e+23]]}, {\"type\": \"annotation\", \"text\": \"GPT-3\", \"x\": 2020.707305936073, \"y\": 5.338e+23, \"ha\": \"center\", \"va\": \"bottom\", \"targetX\": 2020.407305936073, \"targetY\": 3.14e+23, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2020.737305936073, 4.8042e+23], 3, 0.5, [2020.487305936073, 3.297e+23]]}, {\"type\": \"annotation\", \"text\": \"GPT-4\", \"x\": 2023.1050228310503, \"y\": 1.05e+25, \"ha\": \"center\", \"va\": \"bottom\", \"targetX\": 2023.2050228310502, \"targetY\": 2.1e+25, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2023.0750228310503, 1.365e+25], 3, 0.5, [2023.1450228310503, 1.89e+25]]}, {\"type\": \"annotation\", \"text\": \"Gemini Ultra\", \"x\": 2023.6303652968038, \"y\": 3.50000000007e+25, \"ha\": \"right\", \"va\": \"center\", \"targetX\": 2023.9303652968038, \"targetY\": 5.0000000001e+25, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2023.6603652968038, 3.50000000007e+25], -3, 0.5, [2023.8703652968038, 4.5000000000900005e+25]]}, {\"type\": \"annotation\", \"text\": \"PaLM\", \"x\": 2022.1582191780824, \"y\": 4.80168e+24, \"ha\": \"center\", \"va\": \"bottom\", \"targetX\": 2022.2582191780823, \"targetY\": 2.5272e+24, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2022.1882191780824, 4.321512e+24], 3, 0.5, [2022.2582191780823, 2.90628e+24]]}, {\"type\": \"annotation\", \"text\": \"AlphaGo Master\", \"x\": 2017.3, \"y\": 1.2000000000000002e+23, \"ha\": \"left\", \"va\": \"center\", \"targetX\": 2017.0, \"targetY\": 1.5e+23, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2017.27, 1.2000000000000002e+23], 3, 0.5, [2017.08, 1.3800000000000001e+23]]}], \"hud\": [], \"originalDataAspectRatio\": 0.6623655913978495, \"additionalLegendItems\": [], \"tooltipKeyWidth\": 120, \"tooltipMinWidth\": 250} Figure 1: Interactive plot showing the 81 largest models in our dataset, with estimated training compute above 10 23 FLOP. While compiling this dataset, we have conducted an exhaustive search process relying on existing benchmarks and repositories, an automated search for non-English model announcements, and other sources. We explain our methods in depth later in the article. To download the data, see the Appendix or visit our online database . Results There are few models at the leading edge, but the frontier advances rapidly {\"title\": \"Number of models larger than 10<sup>23</sup> FLOP released by year\", \"xAxis\": {\"label\": \"Publication date\", \"lim\": [-0.8, 7.8], \"scaleType\": \"linear\", \"ticks\": [0, 1, 2, 3, 4, 5, 6, 7], \"tickLabels\": [\"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"], \"nice\": false}, \"yAxis\": {\"label\": \"Number of models\", \"lim\": [0.0, 43.05], \"scaleType\": \"linear\", \"ticks\": [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0], \"tickLabels\": [\"0\", \"5\", \"10\", \"15\", \"20\", \"25\", \"30\", \"35\", \"40\", \"45\"]}, \"showLegend\": true, \"legendPosition\": \"header\", \"showFrame\": false, \"addDataPadding\": false, \"showXGrid\": false, \"tooltipType\": \"header-footer\", \"objects\": [{\"type\": \"bar\", \"color\": \"#00A5A6\", \"points\": [{\"x\": 0.0, \"y\": 2.0, \"tooltipData\": {\"2017\": 2.0}}, {\"x\": 1.0, \"y\": 0.0, \"tooltipData\": {\"2018\": 0.0}}, {\"x\": 2.0, \"y\": 0.0, \"tooltipData\": {\"2019\": 0.0}}, {\"x\": 3.0, \"y\": 2.0, \"tooltipData\": {\"2020\": 2.0}}, {\"x\": 4.0, \"y\": 7.0, \"tooltipData\": {\"2021\": 7.0}}, {\"x\": 5.0, \"y\": 22.0, \"tooltipData\": {\"2022\": 22.0}}, {\"x\": 6.0, \"y\": 41.0, \"tooltipData\": {\"2023\": 41.0}}, {\"x\": 7.0, \"y\": 7.0, \"tooltipData\": {\"2024\": 7.0}}], \"relativeBarWidth\": 0.6}, {\"type\": \"annotation\", \"text\": \"As of\\nMarch\\n2024\", \"x\": 7, \"y\": 12, \"ha\": \"center\", \"va\": \"bottom\", \"targetX\": 7, \"targetY\": 12}], \"hud\": [], \"originalDataAspectRatio\": 0.7451612903225805, \"additionalLegendItems\": []} {\"title\": \"Model count by training compute\", \"xAxis\": {\"label\": \"Training compute (FLOP)\", \"lim\": [22.0, 26.0], \"scaleType\": \"linear\", \"ticks\": [22, 23, 24, 25], \"tickLabels\": [\"$10^{22}$\", \"$10^{23}$\", \"$10^{24}$\", \"$10^{25}$\"], \"nice\": false}, \"yAxis\": {\"label\": \"Number of models\", \"lim\": [-5.0, 210.0], \"scaleType\": \"linear\", \"ticks\": [0, 50, 100, 150, 200], \"tickLabels\": [\"0\", \"50\", \"100\", \"150\", \"200\"]}, \"showLegend\": true, \"legendPosition\": \"header\", \"showFrame\": true, \"addDataPadding\": false, \"objects\": [{\"type\": \"line\", \"color\": \"#00A5A6\", \"zOrder\": 2, \"strokeWidth\": 1.5, \"lineStyle\": \"-\", \"points\": [{\"x\": 0.0, \"y\": 570}, {\"x\": 0.12999999999999998, \"y\": 570}, {\"x\": 0.25999999999999995, \"y\": 570}, {\"x\": 0.39, \"y\": 570}, {\"x\": 0.52, \"y\": 570}, {\"x\": 0.65, \"y\": 570}, {\"x\": 0.78, \"y\": 570}, {\"x\": 0.91, \"y\": 570}, {\"x\": 1.04, \"y\": 570}, {\"x\": 1.17, \"y\": 570}, {\"x\": 1.3, \"y\": 570}, {\"x\": 1.4300000000000002, \"y\": 570}, {\"x\": 1.56, \"y\": 570}, {\"x\": 1.69, \"y\": 569}, {\"x\": 1.82, \"y\": 569}, {\"x\": 1.9500000000000002, \"y\": 569}, {\"x\": 2.08, \"y\": 569}, {\"x\": 2.21, \"y\": 569}, {\"x\": 2.34, \"y\": 569}, {\"x\": 2.47, \"y\": 569}, {\"x\": 2.6, \"y\": 569}, {\"x\": 2.73, \"y\": 569}, {\"x\": 2.8600000000000003, \"y\": 569}, {\"x\": 2.99, \"y\": 569}, {\"x\": 3.12, \"y\": 569}, {\"x\": 3.25, \"y\": 569}, {\"x\": 3.38, \"y\": 569}, {\"x\": 3.5100000000000002, \"y\": 569}, {\"x\": 3.64, \"y\": 569}, {\"x\": 3.77, \"y\": 569}, {\"x\": 3.9000000000000004, \"y\": 569}, {\"x\": 4.03, \"y\": 568}, {\"x\": 4.16, \"y\": 568}, {\"x\": 4.29, \"y\": 568}, {\"x\": 4.42, \"y\": 568}, {\"x\": 4.55, \"y\": 568}, {\"x\": 4.68, \"y\": 568}, {\"x\": 4.8100000000000005, \"y\": 568}, {\"x\": 4.94, \"y\": 568}, {\"x\": 5.07, \"y\": 568}, {\"x\": 5.2, \"y\": 568}, {\"x\": 5.33, \"y\": 568}, {\"x\": 5.46, \"y\": 568}, {\"x\": 5.59, \"y\": 568}, {\"x\": 5.720000000000001, \"y\": 568}, {\"x\": 5.8500000000000005, \"y\": 567}, {\"x\": 5.98, \"y\": 567}, {\"x\": 6.11, \"y\": 567}, {\"x\": 6.24, \"y\": 567}, {\"x\": 6.37, \"y\": 567}, {\"x\": 6.5, \"y\": 567}, {\"x\": 6.63, \"y\": 567}, {\"x\": 6.76, \"y\": 567}, {\"x\": 6.890000000000001, \"y\": 567}, {\"x\": 7.0200000000000005, \"y\": 567}, {\"x\": 7.15, \"y\": 567}, {\"x\": 7.28, \"y\": 567}, {\"x\": 7.41, \"y\": 567}, {\"x\": 7.54, \"y\": 567}, {\"x\": 7.67, \"y\": 567}, {\"x\": 7.800000000000001, \"y\": 567}, {\"x\": 7.930000000000001, \"y\": 567}, {\"x\": 8.06, \"y\": 567}, {\"x\": 8.19, \"y\": 565}, {\"x\": 8.32, \"y\": 565}, {\"x\": 8.450000000000001, \"y\": 564}, {\"x\": 8.58, \"y\": 564}, {\"x\": 8.71, \"y\": 563}, {\"x\": 8.84, \"y\": 562}, {\"x\": 8.97, \"y\": 561}, {\"x\": 9.1, \"y\": 561}, {\"x\": 9.23, \"y\": 560}, {\"x\": 9.36, \"y\": 560}, {\"x\": 9.49, \"y\": 560}, {\"x\": 9.620000000000001, \"y\": 560}, {\"x\": 9.75, \"y\": 560}, {\"x\": 9.88, \"y\": 560}, {\"x\": 10.01, \"y\": 560}, {\"x\": 10.14, \"y\": 559}, {\"x\": 10.27, \"y\": 559}, {\"x\": 10.4, \"y\": 559}, {\"x\": 10.530000000000001, \"y\": 557}, {\"x\": 10.66, \"y\": 556}, {\"x\": 10.790000000000001, \"y\": 556}, {\"x\": 10.92, \"y\": 555}, {\"x\": 11.05, \"y\": 555}, {\"x\": 11.18, \"y\": 555}, {\"x\": 11.31, \"y\": 555}, {\"x\": 11.440000000000001, \"y\": 554}, {\"x\": 11.57, \"y\": 554}, {\"x\": 11.700000000000001, \"y\": 554}, {\"x\": 11.83, \"y\": 554}, {\"x\": 11.96, \"y\": 554}, {\"x\": 12.09, \"y\": 554}, {\"x\": 12.22, \"y\": 554}, {\"x\": 12.35, \"y\": 554}, {\"x\": 12.48, \"y\": 553}, {\"x\": 12.610000000000001, \"y\": 553}, {\"x\": 12.74, \"y\": 553}, {\"x\": 12.870000000000001, \"y\": 553}, {\"x\": 13.0, \"y\": 552}, {\"x\": 13.13, \"y\": 551}, {\"x\": 13.26, \"y\": 551}, {\"x\": 13.39, \"y\": 548}, {\"x\": 13.52, \"y\": 548}, {\"x\": 13.65, \"y\": 548}, {\"x\": 13.780000000000001, \"y\": 548}, {\"x\": 13.91, \"y\": 547}, {\"x\": 14.040000000000001, \"y\": 546}, {\"x\": 14.17, \"y\": 545}, {\"x\": 14.3, \"y\": 545}, {\"x\": 14.43, \"y\": 543}, {\"x\": 14.56, \"y\": 541}, {\"x\": 14.690000000000001, \"y\": 540}, {\"x\": 14.82, \"y\": 539}, {\"x\": 14.950000000000001, \"y\": 539}, {\"x\": 15.08, \"y\": 537}, {\"x\": 15.21, \"y\": 535}, {\"x\": 15.34, \"y\": 533}, {\"x\": 15.47, \"y\": 531}, {\"x\": 15.600000000000001, \"y\": 523}, {\"x\": 15.73, \"y\": 521}, {\"x\": 15.860000000000001, \"y\": 519}, {\"x\": 15.99, \"y\": 517}, {\"x\": 16.12, \"y\": 513}, {\"x\": 16.25, \"y\": 510}, {\"x\": 16.38, \"y\": 505}, {\"x\": 16.51, \"y\": 502}, {\"x\": 16.64, \"y\": 500}, {\"x\": 16.77, \"y\": 498}, {\"x\": 16.900000000000002, \"y\": 490}, {\"x\": 17.03, \"y\": 484}, {\"x\": 17.16, \"y\": 479}, {\"x\": 17.29, \"y\": 478}, {\"x\": 17.42, \"y\": 477}, {\"x\": 17.55, \"y\": 473}, {\"x\": 17.68, \"y\": 463}, {\"x\": 17.810000000000002, \"y\": 458}, {\"x\": 17.94, \"y\": 454}, {\"x\": 18.07, \"y\": 451}, {\"x\": 18.2, \"y\": 444}, {\"x\": 18.330000000000002, \"y\": 442}, {\"x\": 18.46, \"y\": 437}, {\"x\": 18.59, \"y\": 427}, {\"x\": 18.72, \"y\": 420}, {\"x\": 18.85, \"y\": 415}, {\"x\": 18.98, \"y\": 402}, {\"x\": 19.11, \"y\": 390}, {\"x\": 19.240000000000002, \"y\": 389}, {\"x\": 19.37, \"y\": 384}, {\"x\": 19.5, \"y\": 368}, {\"x\": 19.63, \"y\": 363}, {\"x\": 19.76, \"y\": 355}, {\"x\": 19.89, \"y\": 344}, {\"x\": 20.02, \"y\": 337}, {\"x\": 20.150000000000002, \"y\": 325}, {\"x\": 20.28, \"y\": 316}, {\"x\": 20.41, \"y\": 310}, {\"x\": 20.54, \"y\": 302}, {\"x\": 20.67, \"y\": 292}, {\"x\": 20.8, \"y\": 283}, {\"x\": 20.93, \"y\": 272}, {\"x\": 21.060000000000002, \"y\": 266}, {\"x\": 21.19, \"y\": 257}, {\"x\": 21.32, \"y\": 249}, {\"x\": 21.45, \"y\": 240}, {\"x\": 21.580000000000002, \"y\": 227}, {\"x\": 21.71, \"y\": 216}, {\"x\": 21.84, \"y\": 204}, {\"x\": 21.970000000000002, \"y\": 188}, {\"x\": 22.1, \"y\": 176}, {\"x\": 22.195899652409235, \"y\": 165}, {\"x\": 22.23, \"y\": 162}, {\"x\": 22.36, \"y\": 147}, {\"x\": 22.490000000000002, \"y\": 139}, {\"x\": 22.56442071922308, \"y\": 131}, {\"x\": 22.62, \"y\": 122}, {\"x\": 22.75, \"y\": 106}, {\"x\": 22.880000000000003, \"y\": 92}, {\"x\": 23.01, \"y\": 80}, {\"x\": 23.041392685158225, \"y\": 77}, {\"x\": 23.14, \"y\": 72}, {\"x\": 23.27, \"y\": 63}, {\"x\": 23.400000000000002, \"y\": 53}, {\"x\": 23.496929648073216, \"y\": 46}, {\"x\": 23.53, \"y\": 44}, {\"x\": 23.66, \"y\": 31}, {\"x\": 23.79, \"y\": 23}, {\"x\": 23.90848501887865, \"y\": 20}, {\"x\": 23.92, \"y\": 20}, {\"x\": 24.05, \"y\": 16}, {\"x\": 24.18, \"y\": 12}, {\"x\": 24.310000000000002, \"y\": 12}, {\"x\": 24.44, \"y\": 7}, {\"x\": 24.57, \"y\": 7}, {\"x\": 24.57518784492766, \"y\": 6}, {\"x\": 24.7, \"y\": 5}, {\"x\": 24.830000000000002, \"y\": 5}, {\"x\": 24.96, \"y\": 4}, {\"x\": 25.09, \"y\": 3}, {\"x\": 25.220000000000002, \"y\": 3}, {\"x\": 25.32221929473392, \"y\": 1}, {\"x\": 25.35, \"y\": 1}, {\"x\": 25.48, \"y\": 1}, {\"x\": 25.61, \"y\": 1}, {\"x\": 25.698970004344705, \"y\": 0}]}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"Gemini\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"bottom\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 25.698970004344705, \"targetY\": 0, \"relDx\": -0.02, \"relDy\": 0.04, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"GPT-4\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"bottom\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 25.32221929473392, \"targetY\": 1, \"relDx\": -0.01, \"relDy\": 0.04, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"Falcon 180B\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"bottom\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 24.57518784492766, \"targetY\": 6, \"relDx\": -0.01, \"relDy\": 0.04, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"Llama 2 70B\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"bottom\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 23.90848501887865, \"targetY\": 20, \"relDx\": -0.01, \"relDy\": 0.04, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"GPT-3\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"bottom\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 23.496929648073216, \"targetY\": 46, \"relDx\": 0.015, \"relDy\": 0, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"Turing-NLG\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"bottom\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 22.195899652409235, \"targetY\": 165, \"relDx\": 0.015, \"relDy\": 0, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"AlphaZero\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"bottom\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 22.56442071922308, \"targetY\": 131, \"relDx\": 0.015, \"relDy\": 0, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"Whisper v2\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"bottom\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 23.041392685158225, \"targetY\": 77, \"relDx\": 0.015, \"relDy\": 0, \"targetCoords\": \"data\"}, {\"type\": \"scatter\", \"alpha\": 1, \"zOrder\": 1, \"points\": [{\"x\": 25.698970004344705, \"y\": 0.0, \"size\": 8}, {\"x\": 25.32221929473392, \"y\": 1.0, \"size\": 8}, {\"x\": 24.57518784492766, \"y\": 6.0, \"size\": 8}, {\"x\": 23.90848501887865, \"y\": 20.0, \"size\": 8}, {\"x\": 23.496929648073216, \"y\": 46.0, \"size\": 8}, {\"x\": 22.195899652409235, \"y\": 165.0, \"size\": 8}, {\"x\": 22.56442071922308, \"y\": 131.0, \"size\": 8}, {\"x\": 23.041392685158225, \"y\": 77.0, \"size\": 8}], \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"fillAlpha\": 1.0, \"strokeAlpha\": 1.0, \"marker\": \"M 0.0,-0.5 C 0.13260155,-0.5 0.25978993539242673,-0.44731684579412084 0.3535533905932738,-0.3535533905932738 C 0.44731684579412084,-0.25978993539242673 0.5,-0.13260155 0.5,0.0 C 0.5,0.13260155 0.44731684579412084,0.25978993539242673 0.3535533905932738,0.3535533905932738 C 0.25978993539242673,0.44731684579412084 0.13260155,0.5 0.0,0.5 C -0.13260155,0.5 -0.25978993539242673,0.44731684579412084 -0.3535533905932738,0.3535533905932738 C -0.44731684579412084,0.25978993539242673 -0.5,0.13260155 -0.5,0.0 C -0.5,-0.13260155 -0.44731684579412084,-0.25978993539242673 -0.3535533905932738,-0.3535533905932738 C -0.25978993539242673,-0.44731684579412084 -0.13260155,-0.5 0.0,-0.5 Z 0.0,-0.5\", \"isFilled\": true}], \"hud\": [], \"originalDataAspectRatio\": 0.5961290322580646, \"additionalLegendItems\": []} Figure 2: a) Number of models with training compute of at least 10 23 FLOP published in each year, 2017 through 2024. b) Number of models with training compute over different thresholds as of 2024 March 31. Supplemental data on models below 10 23 FLOP is taken from our broader database of AI models . Figure 2b shows how there are few models at the frontier of training compute, currently 10 25 FLOP or more. However, as shown in Figure 2a, the frontier advances rapidly over time. In 2020, only two models were trained with more than 10 23 FLOP. This increased exponentially over the subsequent three years, and over 40 models trained at this scale were released in 2023. The rapid advance of the frontier is consistent with exponential increases in ML R&D investment and hardware performance . If training compute continues to increase 4x per year , the top models will surpass 10 26 FLOP in 2024, and models at the 10 24 scale will be over 1000x smaller than the top models by 2026. Meanwhile, the number of models above smaller thresholds is also quickly expanding, so compute thresholds for monitoring models may need to rise correspondingly over time, if they are to remain focused on models at the cutting edge of capabilities. Most large-scale AI models are language models {\"title\": \"Large-scale models by domain\", \"xAxis\": {\"label\": \"Number of models\", \"lim\": [0.0, 140.0], \"scaleType\": \"linear\", \"ticks\": [0.0, 20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0], \"tickLabels\": [\"0\", \"20\", \"40\", \"60\", \"80\", \"100\", \"120\", \"140\"], \"nice\": false}, \"yAxis\": {\"lim\": [-0.36, 10.65], \"scaleType\": \"linear\", \"ticks\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"tickLabels\": [\"Medicine\", \"Robotics\", \"Vision\", \"Audio\", \"Games\", \"Biology\", \"Speech\", \"Video generation\", \"Image generation\", \"Vision-language\", \"Language\"], \"hideTicks\": true}, \"showLegend\": true, \"legendPosition\": \"header\", \"showFrame\": false, \"addDataPadding\": false, \"showYGrid\": false, \"showYBasis\": true, \"tooltipType\": \"group-multitooltip\", \"objects\": [{\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Medicine\", \"points\": [{\"x\": 0, \"y\": -0.35}, {\"x\": 0, \"y\": -0.35}, {\"x\": 0, \"y\": 0.35}, {\"x\": 0, \"y\": 0.35}, {\"x\": 0, \"y\": -0.35}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Robotics\", \"points\": [{\"x\": 0, \"y\": 0.65}, {\"x\": 0, \"y\": 0.65}, {\"x\": 0, \"y\": 1.35}, {\"x\": 0, \"y\": 1.35}, {\"x\": 0, \"y\": 0.65}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Vision\", \"points\": [{\"x\": 0, \"y\": 1.65}, {\"x\": 1, \"y\": 1.65}, {\"x\": 1, \"y\": 2.3499999999999996}, {\"x\": 0, \"y\": 2.3499999999999996}, {\"x\": 0, \"y\": 1.65}], \"closed\": true, \"tooltipData\": 1}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Audio\", \"points\": [{\"x\": 0, \"y\": 2.65}, {\"x\": 0, \"y\": 2.65}, {\"x\": 0, \"y\": 3.3499999999999996}, {\"x\": 0, \"y\": 3.3499999999999996}, {\"x\": 0, \"y\": 2.65}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Games\", \"points\": [{\"x\": 0, \"y\": 3.65}, {\"x\": 2, \"y\": 3.65}, {\"x\": 2, \"y\": 4.35}, {\"x\": 0, \"y\": 4.35}, {\"x\": 0, \"y\": 3.65}], \"closed\": true, \"tooltipData\": 2}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Biology\", \"points\": [{\"x\": 0, \"y\": 4.65}, {\"x\": 2, \"y\": 4.65}, {\"x\": 2, \"y\": 5.3500000000000005}, {\"x\": 0, \"y\": 5.3500000000000005}, {\"x\": 0, \"y\": 4.65}], \"closed\": true, \"tooltipData\": 2}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Speech\", \"points\": [{\"x\": 0, \"y\": 5.65}, {\"x\": 2, \"y\": 5.65}, {\"x\": 2, \"y\": 6.3500000000000005}, {\"x\": 0, \"y\": 6.3500000000000005}, {\"x\": 0, \"y\": 5.65}], \"closed\": true, \"tooltipData\": 2}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Video generation\", \"points\": [{\"x\": 0, \"y\": 6.65}, {\"x\": 0, \"y\": 6.65}, {\"x\": 0, \"y\": 7.3500000000000005}, {\"x\": 0, \"y\": 7.3500000000000005}, {\"x\": 0, \"y\": 6.65}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Image generation\", \"points\": [{\"x\": 0, \"y\": 7.65}, {\"x\": 1, \"y\": 7.65}, {\"x\": 1, \"y\": 8.35}, {\"x\": 0, \"y\": 8.35}, {\"x\": 0, \"y\": 7.65}], \"closed\": true, \"tooltipData\": 1}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Vision-language\", \"points\": [{\"x\": 0, \"y\": 8.65}, {\"x\": 5, \"y\": 8.65}, {\"x\": 5, \"y\": 9.35}, {\"x\": 0, \"y\": 9.35}, {\"x\": 0, \"y\": 8.65}], \"closed\": true, \"tooltipData\": 5}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Language\", \"points\": [{\"x\": 0, \"y\": 9.65}, {\"x\": 69, \"y\": 9.65}, {\"x\": 69, \"y\": 10.35}, {\"x\": 0, \"y\": 10.35}, {\"x\": 0, \"y\": 9.65}], \"closed\": true, \"tooltipData\": 69}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Medicine\", \"points\": [{\"x\": 0, \"y\": -0.35}, {\"x\": 1, \"y\": -0.35}, {\"x\": 1, \"y\": 0.35}, {\"x\": 0, \"y\": 0.35}, {\"x\": 0, \"y\": -0.35}], \"closed\": true, \"tooltipData\": 1}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Robotics\", \"points\": [{\"x\": 0, \"y\": 0.65}, {\"x\": 1, \"y\": 0.65}, {\"x\": 1, \"y\": 1.35}, {\"x\": 0, \"y\": 1.35}, {\"x\": 0, \"y\": 0.65}], \"closed\": true, \"tooltipData\": 1}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Vision\", \"points\": [{\"x\": 1, \"y\": 1.65}, {\"x\": 1, \"y\": 1.65}, {\"x\": 1, \"y\": 2.3499999999999996}, {\"x\": 1, \"y\": 2.3499999999999996}, {\"x\": 1, \"y\": 1.65}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Audio\", \"points\": [{\"x\": 0, \"y\": 2.65}, {\"x\": 2, \"y\": 2.65}, {\"x\": 2, \"y\": 3.3499999999999996}, {\"x\": 0, \"y\": 3.3499999999999996}, {\"x\": 0, \"y\": 2.65}], \"closed\": true, \"tooltipData\": 2}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Games\", \"points\": [{\"x\": 2, \"y\": 3.65}, {\"x\": 2, \"y\": 3.65}, {\"x\": 2, \"y\": 4.35}, {\"x\": 2, \"y\": 4.35}, {\"x\": 2, \"y\": 3.65}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Biology\", \"points\": [{\"x\": 2, \"y\": 4.65}, {\"x\": 2, \"y\": 4.65}, {\"x\": 2, \"y\": 5.3500000000000005}, {\"x\": 2, \"y\": 5.3500000000000005}, {\"x\": 2, \"y\": 4.65}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Speech\", \"points\": [{\"x\": 2, \"y\": 5.65}, {\"x\": 2, \"y\": 5.65}, {\"x\": 2, \"y\": 6.3500000000000005}, {\"x\": 2, \"y\": 6.3500000000000005}, {\"x\": 2, \"y\": 5.65}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Video generation\", \"points\": [{\"x\": 0, \"y\": 6.65}, {\"x\": 6, \"y\": 6.65}, {\"x\": 6, \"y\": 7.3500000000000005}, {\"x\": 0, \"y\": 7.3500000000000005}, {\"x\": 0, \"y\": 6.65}], \"closed\": true, \"tooltipData\": 6}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Image generation\", \"points\": [{\"x\": 1, \"y\": 7.65}, {\"x\": 7, \"y\": 7.65}, {\"x\": 7, \"y\": 8.35}, {\"x\": 1, \"y\": 8.35}, {\"x\": 1, \"y\": 7.65}], \"closed\": true, \"tooltipData\": 6}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Vision-language\", \"points\": [{\"x\": 5, \"y\": 8.65}, {\"x\": 21, \"y\": 8.65}, {\"x\": 21, \"y\": 9.35}, {\"x\": 5, \"y\": 9.35}, {\"x\": 5, \"y\": 8.65}], \"closed\": true, \"tooltipData\": 16}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Language\", \"points\": [{\"x\": 69, \"y\": 9.65}, {\"x\": 124, \"y\": 9.65}, {\"x\": 124, \"y\": 10.35}, {\"x\": 69, \"y\": 10.35}, {\"x\": 69, \"y\": 9.65}], \"closed\": true, \"tooltipData\": 55}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"1\", \"x\": 2, \"y\": 0, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"1\", \"x\": 2, \"y\": 1, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"1\", \"x\": 2, \"y\": 2, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"2\", \"x\": 3, \"y\": 3, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"2\", \"x\": 3, \"y\": 4, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"2\", \"x\": 3, \"y\": 5, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"2\", \"x\": 3, \"y\": 6, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"6\", \"x\": 3.0, \"y\": 7, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"6\", \"x\": 7, \"y\": 7, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"6\", \"x\": 4.0, \"y\": 8, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"7\", \"x\": 8, \"y\": 8, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"5\", \"x\": 2.5, \"y\": 9, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"16\", \"x\": 13.0, \"y\": 9, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"21\", \"x\": 22, \"y\": 9, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"69\", \"x\": 34.5, \"y\": 10, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"55\", \"x\": 96.5, \"y\": 10, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"124\", \"x\": 125, \"y\": 10, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}], \"hud\": [], \"originalDataAspectRatio\": 0.5961290322580646, \"additionalLegendItems\": []} Figure 3: Large-scale AI models categorized by domains of machine learning. Language models trained for both general-purpose language tasks and biology tasks, such as Galactica, are counted within both language and biology. Figure 3 shows how a large majority (69 out of 81, or 85%) of large-scale models in our dataset are language models, and a few more (5 out of 81, about 6%) are vision-language models. Common applications for these models include general language modeling, chat, and code generation. Among language models, the majority are used for natural-language tasks, like chatting with users and following text-based instructions, and some are trained for tasks such as code generation and protein-sequence prediction. Only a small minority of large-scale models (7 out of 81, about 9%) are not trained on language or text data, instead using audio, image, or game data. To gain a fuller picture of domains for large-scale models, Figure 3 also shows domains for models with unconfirmed training compute. These models lack public information about their training process, but may be trained with over 10 23 FLOP, based on what we know about them (see Unconfirmed large-scale models ). The domain breakdown is broadly similar, but a higher proportion of them deal with other applications such as image generation, video generation, and robotics. Proprietary models, such as Runway\u2019s Gen-2 video generator, typically don\u2019t disclose details of their training datasets and hardware. Robotics systems, such as Nvidia\u2019s CALM, typically require the authors to gather custom, proprietary datasets rather than using open-source datasets which are publicly available and well-documented. {\"title\": \"Large-scale models by domain and publication date\", \"xAxis\": {\"label\": \"Publication date\", \"lim\": [2016.638105022831, 2024.5997945205481], \"scaleType\": \"linear\", \"ticks\": [2016.0, 2017.0, 2018.0, 2019.0, 2020.0, 2021.0, 2022.0, 2023.0, 2024.0, 2025.0], \"tickLabels\": [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"], \"nice\": false}, \"yAxis\": {\"label\": \"Training compute (FLOP)\", \"lim\": [7.40608926488071e+22, 6.818713385008506e+25], \"scaleType\": \"log\", \"ticks\": [1e+21, 1e+22, 1.0000000000000001e+23, 1e+24, 1e+25, 1e+26, 1e+27], \"tickLabels\": [\"$\\\\mathdefault{10^{21}}$\", \"$\\\\mathdefault{10^{22}}$\", \"$\\\\mathdefault{10^{23}}$\", \"$\\\\mathdefault{10^{24}}$\", \"$\\\\mathdefault{10^{25}}$\", \"$\\\\mathdefault{10^{26}}$\", \"$\\\\mathdefault{10^{27}}$\"]}, \"showLegend\": true, \"legendPosition\": \"header\", \"showFrame\": true, \"addDataPadding\": false, \"objects\": [{\"type\": \"scatter\", \"zOrder\": 1, \"points\": [{\"x\": 2023.2214611872148, \"y\": 2.4e+23, \"tooltipData\": {\"Model\": \"LightOn Mini\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.40e+23\", \"Organization\": \"LightOn\", \"Publication date\": \"2023-03-21\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.7746575342467, \"y\": 3.3e+23, \"tooltipData\": {\"Model\": \"CodeFuse-13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.30e+23\", \"Organization\": \"Ant Group\", \"Publication date\": \"2023-10-10\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.7406392694065, \"y\": 1.01e+23, \"tooltipData\": {\"Model\": \"Qwen-7B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.01e+23\", \"Organization\": \"Alibaba\", \"Publication date\": \"2023-09-28\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.7406392694065, \"y\": 1.17e+23, \"tooltipData\": {\"Model\": \"PLaMo-13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.17e+23\", \"Organization\": \"Preferred Networks Inc\", \"Publication date\": \"2023-09-28\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.6803652968038, \"y\": 2.03e+23, \"tooltipData\": {\"Model\": \"Baichuan2-13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.03e+23\", \"Organization\": \"Baichuan\", \"Publication date\": \"2023-09-06\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.6803652968038, \"y\": 3.76e+24, \"tooltipData\": {\"Model\": \"Falcon-180B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.76e+24\", \"Organization\": \"Technology Innovation Institute\", \"Publication date\": \"2023-09-06\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.6803652968038, \"y\": 1.02e+24, \"tooltipData\": {\"Model\": \"TigerBot-70B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.02e+24\", \"Organization\": \"Tigerobo\", \"Publication date\": \"2023-09-06\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.6189497716894, \"y\": 5.3e+23, \"tooltipData\": {\"Model\": \"Code Llama-34B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"5.30e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-08-14\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.6189497716894, \"y\": 1.1e+23, \"tooltipData\": {\"Model\": \"Code Llama-7B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.10e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-08-14\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.5465753424658, \"y\": 1.6e+23, \"tooltipData\": {\"Model\": \"Llama 2-13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.60e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-07-18\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.5465753424658, \"y\": 4.08e+23, \"tooltipData\": {\"Model\": \"Llama 2-34B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"4.08e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-07-18\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.5465753424658, \"y\": 8.1e+23, \"tooltipData\": {\"Model\": \"Llama 2-70B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"8.10e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-07-18\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.527397260274, \"y\": 3.866e+24, \"tooltipData\": {\"Model\": \"Claude 2\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.87e+24\", \"Organization\": \"Anthropic\", \"Publication date\": \"2023-07-11\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.4769406392695, \"y\": 1.0001e+24, \"tooltipData\": {\"Model\": \"Inflection-1\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.00e+24\", \"Organization\": \"Inflection AI\", \"Publication date\": \"2023-06-23\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.4742009132422, \"y\": 1.8e+23, \"tooltipData\": {\"Model\": \"MPT-30B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.80e+23\", \"Organization\": \"MosaicML\", \"Publication date\": \"2023-06-22\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.35799086758, \"y\": 7.34e+24, \"tooltipData\": {\"Model\": \"PaLM 2\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"7.34e+24\", \"Organization\": \"Google\", \"Publication date\": \"2023-05-10\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.2461187214612, \"y\": 2.36e+23, \"tooltipData\": {\"Model\": \"BloombergGPT\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.36e+23\", \"Organization\": \"Bloomberg,Johns Hopkins University\", \"Publication date\": \"2023-03-30\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.7406392694065, \"y\": 2.5e+23, \"tooltipData\": {\"Model\": \"Qwen-14B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.50e+23\", \"Organization\": \"Alibaba\", \"Publication date\": \"2023-09-28\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.8294520547945, \"y\": 2.5e+23, \"tooltipData\": {\"Model\": \"Skywork-13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.50e+23\", \"Organization\": \"Kunlun Inc.\", \"Publication date\": \"2023-10-30\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.2050228310502, \"y\": 2.4e+23, \"tooltipData\": {\"Model\": \"Falcon-40B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.40e+23\", \"Organization\": \"Technology Innovation Institute\", \"Publication date\": \"2023-03-15\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.8321917808219, \"y\": 1.0920000000001e+23, \"tooltipData\": {\"Model\": \"BlueLM 13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.09e+23\", \"Organization\": \"vivo AI lab\", \"Publication date\": \"2023-10-31\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2024.1600456621004, \"y\": 1.55e+23, \"tooltipData\": {\"Model\": \"StarCoder 2 7B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.55e+23\", \"Organization\": \"Hugging Face,ServiceNow,NVIDIA,BigCode\", \"Publication date\": \"2024-02-29\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2024.1600456621004, \"y\": 3.87e+23, \"tooltipData\": {\"Model\": \"StarCoder 2 15B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.87e+23\", \"Organization\": \"Hugging Face,ServiceNow,NVIDIA,BigCode\", \"Publication date\": \"2024-02-29\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2024.151826484018, \"y\": 2.0000000001e+25, \"tooltipData\": {\"Model\": \"Mistral Large\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.00e+25\", \"Organization\": \"Mistral AI\", \"Publication date\": \"2024-02-26\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2024.1381278538813, \"y\": 2.52e+23, \"tooltipData\": {\"Model\": \"Gemma 7B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.52e+23\", \"Organization\": \"Google DeepMind\", \"Publication date\": \"2024-02-21\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2024.0767123287671, \"y\": 1.230000000001e+24, \"tooltipData\": {\"Model\": \"Code Llama-70B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.23e+24\", \"Organization\": \"Meta AI\", \"Publication date\": \"2024-01-29\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2024.0109589041097, \"y\": 8.04e+23, \"tooltipData\": {\"Model\": \"DeepSeek LLM 67B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"8.04e+23\", \"Organization\": \"DeepSeek\", \"Publication date\": \"2024-01-05\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.9522831050228, \"y\": 1.53e+23, \"tooltipData\": {\"Model\": \"Poro34B (700B token checkpoint)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.53e+23\", \"Organization\": \"High-Performance Language Technologies (HPLT),University of Turku\", \"Publication date\": \"2023-12-14\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.9358447488585, \"y\": 1.24800000000001e+24, \"tooltipData\": {\"Model\": \"XVERSE-65B-2\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.25e+24\", \"Organization\": \"XVERSE Technology\", \"Publication date\": \"2023-12-08\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.9127853881278, \"y\": 1.3e+24, \"tooltipData\": {\"Model\": \"Qwen-72B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.30e+24\", \"Organization\": \"Alibaba\", \"Publication date\": \"2023-11-30\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.9127853881278, \"y\": 2.44e+23, \"tooltipData\": {\"Model\": \"Granite 13B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.44e+23\", \"Organization\": \"IBM\", \"Publication date\": \"2023-11-30\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.8908675799087, \"y\": 1.001e+25, \"tooltipData\": {\"Model\": \"Inflection-2\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.00e+25\", \"Organization\": \"Inflection AI\", \"Publication date\": \"2023-11-22\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.8716894977167, \"y\": 1.8e+23, \"tooltipData\": {\"Model\": \"Nemotron-3-8B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.80e+23\", \"Organization\": \"NVIDIA\", \"Publication date\": \"2023-11-15\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.8360730593606, \"y\": 6.1e+23, \"tooltipData\": {\"Model\": \"Yi-34B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"6.10e+23\", \"Organization\": \"01.AI\", \"Publication date\": \"2023-11-02\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.8360730593606, \"y\": 3.96e+23, \"tooltipData\": {\"Model\": \"DeepSeek Coder 33B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.96e+23\", \"Organization\": \"DeepSeek\", \"Publication date\": \"2023-11-02\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.8333333333333, \"y\": 2.4e+23, \"tooltipData\": {\"Model\": \"Nanbeige-16B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.40e+23\", \"Organization\": \"Nanbeige LLM Lab\", \"Publication date\": \"2023-11-01\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.2187214611872, \"y\": 4.67e+23, \"tooltipData\": {\"Model\": \"PanGu-\\u03a3\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"4.67e+23\", \"Organization\": \"Huawei Noah's Ark Lab\", \"Publication date\": \"2023-03-20\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2024.2378995433792, \"y\": 2.6e+24, \"tooltipData\": {\"Model\": \"DBRX\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.60e+24\", \"Organization\": \"Databricks\", \"Publication date\": \"2024-03-27\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.4385844748858, \"y\": 5.6e+23, \"tooltipData\": {\"Model\": \"BIG-G 137B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"5.60e+23\", \"Organization\": \"Google\", \"Publication date\": \"2022-06-09\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.0860730593606, \"y\": 1.568160000001e+23, \"tooltipData\": {\"Model\": \"AlphaCode\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.57e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2022-02-02\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.1271689497717, \"y\": 2.9e+23, \"tooltipData\": {\"Model\": \"ST-MoE\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.90e+23\", \"Organization\": \"Google\", \"Publication date\": \"2022-02-17\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2021.9769406392695, \"y\": 1.0421e+24, \"tooltipData\": {\"Model\": \"ERNIE 3.0 Titan\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.04e+24\", \"Organization\": \"Baidu,Peng Cheng Laboratory\", \"Publication date\": \"2021-12-23\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.2433789954339, \"y\": 5.76e+23, \"tooltipData\": {\"Model\": \"Chinchilla\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"5.76e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2022-03-29\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.2582191780823, \"y\": 2.5272e+24, \"tooltipData\": {\"Model\": \"PaLM (540B)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.53e+24\", \"Organization\": \"Google Research\", \"Publication date\": \"2022-04-04\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.3360730593606, \"y\": 4.3e+23, \"tooltipData\": {\"Model\": \"OPT-175B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"4.30e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2022-05-02\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.35799086758, \"y\": 1.2e+23, \"tooltipData\": {\"Model\": \"UL2\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.20e+23\", \"Organization\": \"Google Research,Google Brain\", \"Publication date\": \"2022-05-10\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2021.9495433789955, \"y\": 3.74e+23, \"tooltipData\": {\"Model\": \"GLaM\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.74e+23\", \"Organization\": \"Google\", \"Publication date\": \"2021-12-13\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.1545662100457, \"y\": 2.7300000000001e+23, \"tooltipData\": {\"Model\": \"LLaMA-33B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.73e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-02-27\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.4714611872148, \"y\": 1.100000000001e+23, \"tooltipData\": {\"Model\": \"OPT-66B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.10e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2022-06-21\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2021.9358447488585, \"y\": 6.31e+23, \"tooltipData\": {\"Model\": \"Gopher (280B)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"6.31e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2021-12-08\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.4769406392695, \"y\": 2.2e+23, \"tooltipData\": {\"Model\": \"YaLM\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.20e+23\", \"Organization\": \"Yandex\", \"Publication date\": \"2022-06-23\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.4933789954339, \"y\": 2.7415e+24, \"tooltipData\": {\"Model\": \"Minerva (540B)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.74e+24\", \"Organization\": \"Google\", \"Publication date\": \"2022-06-29\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.5860730593606, \"y\": 2.04374016e+23, \"tooltipData\": {\"Model\": \"AlexaTM 20B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.04e+23\", \"Organization\": \"Amazon\", \"Publication date\": \"2022-08-02\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.5915525114156, \"y\": 3.778e+23, \"tooltipData\": {\"Model\": \"GLM-130B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.78e+23\", \"Organization\": \"Tsinghua University\", \"Publication date\": \"2022-08-04\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2021.7801369863014, \"y\": 3.5380000000001e+23, \"tooltipData\": {\"Model\": \"Yuan 1.0\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.54e+23\", \"Organization\": \"Inspur\", \"Publication date\": \"2021-10-12\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.60799086758, \"y\": 4.3e+23, \"tooltipData\": {\"Model\": \"BlenderBot 3\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"4.30e+23\", \"Organization\": \"McGill University,Meta AI,Mila- Quebec AI\", \"Publication date\": \"2022-08-10\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.6216894977167, \"y\": 2.8e+23, \"tooltipData\": {\"Model\": \"Luminous-supreme\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.80e+23\", \"Organization\": \"Aleph Alpha\", \"Publication date\": \"2022-08-15\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.8020547945205, \"y\": 2.53e+24, \"tooltipData\": {\"Model\": \"U-PaLM (540B)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.53e+24\", \"Organization\": \"Google\", \"Publication date\": \"2022-10-20\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.852511415525, \"y\": 5.7700000000001e+23, \"tooltipData\": {\"Model\": \"BLOOM-176B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"5.77e+23\", \"Organization\": \"Hugging Face,BigScience\", \"Publication date\": \"2022-11-08\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2021.777397260274, \"y\": 1.17e+24, \"tooltipData\": {\"Model\": \"Megatron-Turing NLG 530B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.17e+24\", \"Organization\": \"Microsoft,NVIDIA\", \"Publication date\": \"2021-10-11\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.907305936073, \"y\": 2.578e+24, \"tooltipData\": {\"Model\": \"GPT-3.5 (text-davinci-003)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"2.58e+24\", \"Organization\": \"OpenAI\", \"Publication date\": \"2022-11-28\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2021.6913242009134, \"y\": 1.476e+23, \"tooltipData\": {\"Model\": \"HyperCLOVA\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.48e+23\", \"Organization\": \"NAVER,Search Solutions\", \"Publication date\": \"2021-09-10\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2021.6107305936073, \"y\": 3.7e+23, \"tooltipData\": {\"Model\": \"Jurassic-1-Jumbo\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.70e+23\", \"Organization\": \"AI21 Labs\", \"Publication date\": \"2021-08-11\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.0027397260274, \"y\": 1.58e+23, \"tooltipData\": {\"Model\": \"SparseOPT-175B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.58e+23\", \"Organization\": \"Institute of Science and Technology Austria (ISTA),Neural Magic\", \"Publication date\": \"2023-01-02\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2020.407305936073, \"y\": 3.14e+23, \"tooltipData\": {\"Model\": \"GPT-3 175B (davinci)\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.14e+23\", \"Organization\": \"OpenAI\", \"Publication date\": \"2020-05-28\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.1463470319634, \"y\": 5.5e+23, \"tooltipData\": {\"Model\": \"LLaMA-65B\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"5.50e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2023-02-24\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2020.0739726027398, \"y\": 1.12e+23, \"tooltipData\": {\"Model\": \"Meena\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"1.12e+23\", \"Organization\": \"Google Brain\", \"Publication date\": \"2020-01-28\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.10799086758, \"y\": 3.55e+23, \"tooltipData\": {\"Model\": \"LaMDA\", \"Domain\": \"Language\", \"Training compute <br>(FLOP)\": \"3.55e+23\", \"Organization\": \"Google\", \"Publication date\": \"2022-02-10\"}, \"fillColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"label\": \"Language\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.9303652968038, \"y\": 5.0000000001e+25, \"tooltipData\": {\"Model\": \"Gemini Ultra\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"5.00e+25\", \"Organization\": \"Google DeepMind\", \"Publication date\": \"2023-12-06\"}, \"fillColor\": \"rgb(252.0, 101.0, 56.0)\", \"strokeColor\": \"rgb(252.0, 101.0, 56.0)\", \"label\": \"Multimodal\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.2050228310502, \"y\": 2.1e+25, \"tooltipData\": {\"Model\": \"GPT-4\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"2.10e+25\", \"Organization\": \"OpenAI\", \"Publication date\": \"2023-03-15\"}, \"fillColor\": \"rgb(252.0, 101.0, 56.0)\", \"strokeColor\": \"rgb(252.0, 101.0, 56.0)\", \"label\": \"Multimodal\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.3267123287671, \"y\": 2.7e+23, \"tooltipData\": {\"Model\": \"Flamingo\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"2.70e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2022-04-29\"}, \"fillColor\": \"rgb(252.0, 101.0, 56.0)\", \"strokeColor\": \"rgb(252.0, 101.0, 56.0)\", \"label\": \"Multimodal\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.8212328767124, \"y\": 1.09200000000001e+24, \"tooltipData\": {\"Model\": \"ChatGLM3\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"1.09e+24\", \"Organization\": \"Zhipu AI\", \"Publication date\": \"2023-10-27\"}, \"fillColor\": \"rgb(252.0, 101.0, 56.0)\", \"strokeColor\": \"rgb(252.0, 101.0, 56.0)\", \"label\": \"Multimodal\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.6408675799087, \"y\": 1.1593580544e+23, \"tooltipData\": {\"Model\": \"IDEFICS\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"1.16e+23\", \"Organization\": \"Hugging Face\", \"Publication date\": \"2023-08-22\"}, \"fillColor\": \"rgb(252.0, 101.0, 56.0)\", \"strokeColor\": \"rgb(252.0, 101.0, 56.0)\", \"label\": \"Multimodal\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.8744292237443, \"y\": 3.24e+23, \"tooltipData\": {\"Model\": \"Galactica\", \"Domain\": \"Multimodal\", \"Training compute <br>(FLOP)\": \"3.24e+23\", \"Organization\": \"Meta AI\", \"Publication date\": \"2022-11-16\"}, \"fillColor\": \"rgb(252.0, 101.0, 56.0)\", \"strokeColor\": \"rgb(252.0, 101.0, 56.0)\", \"label\": \"Multimodal\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.8470319634703, \"y\": 2.7e+23, \"tooltipData\": {\"Model\": \"Whisper v3\", \"Domain\": \"Speech\", \"Training compute <br>(FLOP)\": \"2.70e+23\", \"Organization\": \"OpenAI\", \"Publication date\": \"2023-11-06\"}, \"fillColor\": \"rgb(176.0, 135.0, 244.0)\", \"strokeColor\": \"rgb(176.0, 135.0, 244.0)\", \"label\": \"Speech\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.9276255707764, \"y\": 1.1e+23, \"tooltipData\": {\"Model\": \"Whisper v2\", \"Domain\": \"Speech\", \"Training compute <br>(FLOP)\": \"1.10e+23\", \"Organization\": \"OpenAI\", \"Publication date\": \"2022-12-05\"}, \"fillColor\": \"rgb(176.0, 135.0, 244.0)\", \"strokeColor\": \"rgb(176.0, 135.0, 244.0)\", \"label\": \"Speech\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2017.7965753424658, \"y\": 3.41e+23, \"tooltipData\": {\"Model\": \"AlphaGo Zero\", \"Domain\": \"Games\", \"Training compute <br>(FLOP)\": \"3.41e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2017-10-18\"}, \"fillColor\": \"rgb(31.0, 110.0, 229.0)\", \"strokeColor\": \"rgb(31.0, 110.0, 229.0)\", \"label\": \"Games\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2017.0, \"y\": 1.5e+23, \"tooltipData\": {\"Model\": \"AlphaGo Master\", \"Domain\": \"Games\", \"Training compute <br>(FLOP)\": \"1.50e+23\", \"Organization\": \"DeepMind\", \"Publication date\": \"2017-01-01\"}, \"fillColor\": \"rgb(31.0, 110.0, 229.0)\", \"strokeColor\": \"rgb(31.0, 110.0, 229.0)\", \"label\": \"Games\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2022.4742009132422, \"y\": 3.962895376192635e+23, \"tooltipData\": {\"Model\": \"Parti\", \"Domain\": \"Drawing\", \"Training compute <br>(FLOP)\": \"3.96e+23\", \"Organization\": \"Google Research\", \"Publication date\": \"2022-06-22\"}, \"fillColor\": \"rgb(255.0, 178.0, 60.0)\", \"strokeColor\": \"rgb(255.0, 178.0, 60.0)\", \"label\": \"Drawing\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.513698630137, \"y\": 6.0001e+23, \"tooltipData\": {\"Model\": \"xTrimoPGLM -100B\", \"Domain\": \"Biology\", \"Training compute <br>(FLOP)\": \"6.00e+23\", \"Organization\": \"Tsinghua University,BioMap Research\", \"Publication date\": \"2023-07-06\"}, \"fillColor\": \"rgb(167.0, 82.0, 55.0)\", \"strokeColor\": \"rgb(167.0, 82.0, 55.0)\", \"label\": \"Biology\", \"labelIsLegendLabel\": true, \"size\": 8}, {\"x\": 2023.10799086758, \"y\": 4.0001e+23, \"tooltipData\": {\"Model\": \"ViT-22B\", \"Domain\": \"Vision\", \"Training compute <br>(FLOP)\": \"4.00e+23\", \"Organization\": \"Google\", \"Publication date\": \"2023-02-10\"}, \"fillColor\": \"rgb(224.0, 61.0, 144.0)\", \"strokeColor\": \"rgb(224.0, 61.0, 144.0)\", \"label\": \"Vision\", \"labelIsLegendLabel\": true, \"size\": 8}], \"marker\": \"M 0.0,-0.5 C 0.13260155,-0.5 0.25978993539242673,-0.44731684579412084 0.3535533905932738,-0.3535533905932738 C 0.44731684579412084,-0.25978993539242673 0.5,-0.13260155 0.5,0.0 C 0.5,0.13260155 0.44731684579412084,0.25978993539242673 0.3535533905932738,0.3535533905932738 C 0.25978993539242673,0.44731684579412084 0.13260155,0.5 0.0,0.5 C -0.13260155,0.5 -0.25978993539242673,0.44731684579412084 -0.3535533905932738,0.3535533905932738 C -0.44731684579412084,0.25978993539242673 -0.5,0.13260155 -0.5,0.0 C -0.5,-0.13260155 -0.44731684579412084,-0.25978993539242673 -0.3535533905932738,-0.3535533905932738 C -0.25978993539242673,-0.44731684579412084 -0.13260155,-0.5 0.0,-0.5 Z 0.0,-0.5\", \"isFilled\": true}, {\"type\": \"annotation\", \"text\": \"AlphaGo Zero\", \"x\": 2018.0965753424657, \"y\": 5.797e+23, \"ha\": \"center\", \"va\": \"bottom\", \"targetX\": 2017.7965753424658, \"targetY\": 3.41e+23, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2018.1265753424657, 5.2173e+23], 3, 0.5, [2017.8765753424657, 3.5805000000000005e+23]]}, {\"type\": \"annotation\", \"text\": \"GPT-3\", \"x\": 2020.707305936073, \"y\": 5.338e+23, \"ha\": \"center\", \"va\": \"bottom\", \"targetX\": 2020.407305936073, \"targetY\": 3.14e+23, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2020.737305936073, 4.8042e+23], 3, 0.5, [2020.487305936073, 3.297e+23]]}, {\"type\": \"annotation\", \"text\": \"GPT-4\", \"x\": 2023.1050228310503, \"y\": 1.05e+25, \"ha\": \"center\", \"va\": \"bottom\", \"targetX\": 2023.2050228310502, \"targetY\": 2.1e+25, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2023.0750228310503, 1.365e+25], 3, 0.5, [2023.1450228310503, 1.89e+25]]}, {\"type\": \"annotation\", \"text\": \"Gemini Ultra\", \"x\": 2023.6303652968038, \"y\": 3.50000000007e+25, \"ha\": \"right\", \"va\": \"center\", \"targetX\": 2023.9303652968038, \"targetY\": 5.0000000001e+25, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2023.6603652968038, 3.50000000007e+25], -3, 0.5, [2023.8703652968038, 4.5000000000900005e+25]]}, {\"type\": \"annotation\", \"text\": \"PaLM\", \"x\": 2022.1582191780824, \"y\": 4.80168e+24, \"ha\": \"center\", \"va\": \"bottom\", \"targetX\": 2022.2582191780823, \"targetY\": 2.5272e+24, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2022.1882191780824, 4.321512e+24], 3, 0.5, [2022.2582191780823, 2.90628e+24]]}, {\"type\": \"annotation\", \"text\": \"AlphaGo Master\", \"x\": 2017.3, \"y\": 1.2000000000000002e+23, \"ha\": \"left\", \"va\": \"center\", \"targetX\": 2017.0, \"targetY\": 1.5e+23, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2017.27, 1.2000000000000002e+23], 3, 0.5, [2017.08, 1.3800000000000001e+23]]}, {\"type\": \"annotation\", \"text\": \"Meena\", \"x\": 2020.2739726027398, \"y\": 1.568e+23, \"ha\": \"center\", \"va\": \"bottom\", \"targetX\": 2020.0739726027398, \"targetY\": 1.12e+23, \"hasArrow\": true, \"arrowType\": \"bezier\", \"arrowControlPoints\": [[2020.3039726027398, 1.4112e+23], 3, 0.5, [2020.1539726027397, 1.176e+23]]}], \"hud\": [], \"originalDataAspectRatio\": 0.7451612903225805, \"additionalLegendItems\": [], \"tooltipKeyWidth\": 120, \"tooltipMinWidth\": 250} Figure 4: Large-scale AI models by year, separated by domain. Game-playing models such as AlphaGo Master were the first to exceed the 10\u00b2\u00b3 FLOP threshold, and it took two years for language models to catch up. After 2021, the number of LLMs rapidly increased, while there were no more game-playing models at this scale of compute. Although language models have been the dominant category of large AI models from 2020 onward, the first two large-scale models under our definition were both game-playing models. The first models trained on over 10 23 FLOP, AlphaGo Master and AlphaGo Zero, were developed by DeepMind and published in 2017. These models were trained with an unprecedented amount of compute, over an order of magnitude more than any preceding model. There are no game-playing models in the dataset after these. Figure 4 shows how the next model trained at a similar scale, Google\u2019s GPT-inspired Meena, was not published until 2020. GPT and GPT-2 were below 10 23 FLOP, but GPT-3 arrived later the same year, followed by other GPT-inspired language models such as Jurassic-1. Computer vision is a prominent area of research, but only one non-generative vision model (ViT-22B, current SOTA for ObjectNet) is near today\u2019s frontier of compute. The largest models in our database, GPT-4 and Gemini Ultra, 2 are both multimodal models that accept non-text inputs \u2014 since multimodal models are more useful than pure language models, this may become a trend among large models. Many of these models have been used in commercial products, such as GPT-4 in ChatGPT, Gemini, and ERNIE in Ernie Bot. Diffusion models have also been used in many image- and video-generation products, like DALL-E and Midjourney. Several of these may be near the compute frontier, but their developers have typically not published training details. Most large-scale models are developed by US companies 43 out of 81 large-scale AI models were developed by organizations based in the United States, followed by 19 in China and 6 in the UK. 10 models were developed in other countries outside the US, China, and the UK, and three were developed by collaborations involving researchers and organizations from multiple countries. DeepMind is responsible for every large-scale model developed exclusively in the UK; since merging with the AI teams at Google, it has produced two more as multinational collaborations. {\"title\": \"Large-scale models by country\", \"xAxis\": {\"label\": \"Year\", \"lim\": [2017.0, 2024.237], \"scaleType\": \"linear\", \"ticks\": [2017.0, 2018.0, 2019.0, 2020.0, 2021.0, 2022.0, 2023.0, 2024.0, 2025.0], \"tickLabels\": [\"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"], \"nice\": false}, \"yAxis\": {\"label\": \"Cumulative number of models\", \"lim\": [0.0, 84.7], \"scaleType\": \"linear\", \"ticks\": [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0], \"tickLabels\": [\"0\", \"10\", \"20\", \"30\", \"40\", \"50\", \"60\", \"70\", \"80\", \"90\"]}, \"showLegend\": true, \"legendPosition\": \"header\", \"showFrame\": true, \"addDataPadding\": false, \"multiTooltip\": {\"datapointXs\": [2017.0, 2017.0833333333333, 2017.1666666666665, 2017.2499999999998, 2017.333333333333, 2017.4166666666663, 2017.4999999999995, 2017.5833333333328, 2017.666666666666, 2017.7499999999993, 2017.8333333333326, 2017.9166666666658, 2017.999999999999, 2018.0833333333323, 2018.1666666666656, 2018.2499999999989, 2018.3333333333321, 2018.4166666666654, 2018.4999999999986, 2018.583333333332, 2018.6666666666652, 2018.7499999999984, 2018.8333333333317, 2018.916666666665, 2018.9999999999982, 2019.0833333333314, 2019.1666666666647, 2019.249999999998, 2019.3333333333312, 2019.4166666666645, 2019.4999999999977, 2019.583333333331, 2019.6666666666642, 2019.7499999999975, 2019.8333333333308, 2019.916666666664, 2019.9999999999973, 2020.0833333333305, 2020.1666666666638, 2020.249999999997, 2020.3333333333303, 2020.4166666666636, 2020.4999999999968, 2020.58333333333, 2020.6666666666633, 2020.7499999999966, 2020.8333333333298, 2020.916666666663, 2020.9999999999964, 2021.0833333333296, 2021.1666666666629, 2021.2499999999961, 2021.3333333333294, 2021.4166666666626, 2021.499999999996, 2021.5833333333292, 2021.6666666666624, 2021.7499999999957, 2021.833333333329, 2021.9166666666622, 2021.9999999999955, 2022.0833333333287, 2022.166666666662, 2022.2499999999952, 2022.3333333333285, 2022.4166666666617, 2022.499999999995, 2022.5833333333283, 2022.6666666666615, 2022.7499999999948, 2022.833333333328, 2022.9166666666613, 2022.9999999999945, 2023.0833333333278, 2023.166666666661, 2023.2499999999943, 2023.3333333333276, 2023.4166666666608, 2023.499999999994, 2023.5833333333273, 2023.6666666666606, 2023.7499999999939, 2023.8333333333271, 2023.9166666666604, 2023.9999999999936, 2024.083333333327, 2024.1666666666601, 2024.2499999999934], \"datapointHeaders\": [\"Jan 2017\", \"Feb 2017\", \"Mar 2017\", \"Apr 2017\", \"May 2017\", \"Jun 2017\", \"Jul 2017\", \"Aug 2017\", \"Sep 2017\", \"Oct 2017\", \"Nov 2017\", \"Dec 2017\", \"Jan 2018\", \"Feb 2018\", \"Mar 2018\", \"Apr 2018\", \"May 2018\", \"Jun 2018\", \"Jul 2018\", \"Aug 2018\", \"Sep 2018\", \"Oct 2018\", \"Nov 2018\", \"Dec 2018\", \"Jan 2019\", \"Feb 2019\", \"Mar 2019\", \"Apr 2019\", \"May 2019\", \"Jun 2019\", \"Jul 2019\", \"Aug 2019\", \"Sep 2019\", \"Oct 2019\", \"Nov 2019\", \"Dec 2019\", \"Jan 2020\", \"Feb 2020\", \"Mar 2020\", \"Apr 2020\", \"May 2020\", \"Jun 2020\", \"Jul 2020\", \"Aug 2020\", \"Sep 2020\", \"Oct 2020\", \"Nov 2020\", \"Dec 2020\", \"Jan 2021\", \"Feb 2021\", \"Mar 2021\", \"Apr 2021\", \"May 2021\", \"Jun 2021\", \"Jul 2021\", \"Aug 2021\", \"Sep 2021\", \"Oct 2021\", \"Nov 2021\", \"Dec 2021\", \"Jan 2022\", \"Feb 2022\", \"Mar 2022\", \"Apr 2022\", \"May 2022\", \"Jun 2022\", \"Jul 2022\", \"Aug 2022\", \"Sep 2022\", \"Oct 2022\", \"Nov 2022\", \"Dec 2022\", \"Jan 2023\", \"Feb 2023\", \"Mar 2023\", \"Apr 2023\", \"May 2023\", \"Jun 2023\", \"Jul 2023\", \"Aug 2023\", \"Sep 2023\", \"Oct 2023\", \"Nov 2023\", \"Dec 2023\", \"Jan 2024\", \"Feb 2024\", \"Mar 2024\", \"Apr 2024\"], \"columnNames\": [\"Country\", \"Large-scale Models\"], \"rows\": {\"United States\": {\"values\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 4, 4, 6, 6, 7, 9, 13, 13, 14, 14, 15, 18, 19, 19, 22, 24, 24, 25, 27, 31, 34, 34, 34, 38, 38, 39, 42, 43], \"color\": \"#00a5a6\"}, \"China\": {\"values\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 9, 14, 17, 18, 19, 19, 19], \"color\": \"#fc6538\"}, \"Other\": {\"values\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, 8, 9, 9, 10, 10], \"color\": \"#ffb23c\"}, \"United Kingdom\": {\"values\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], \"color\": \"#b087f4\"}, \"Multinational\": {\"values\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3], \"color\": \"#1f6ee5\"}}, \"includeTotal\": true}, \"tooltipPosition\": \"bottom-left\", \"tooltipType\": \"vertical-multitooltip\", \"objects\": [{\"type\": \"polygon\", \"label\": \"United States\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"alpha\": 1.0, \"zOrder\": 1, \"points\": [{\"x\": 2017.0, \"y\": 0.0}, {\"x\": 2017.0, \"y\": 0.0}, {\"x\": 2017.796, \"y\": 0.0}, {\"x\": 2020.073, \"y\": 0.0}, {\"x\": 2020.407, \"y\": 0.0}, {\"x\": 2021.61, \"y\": 0.0}, {\"x\": 2021.691, \"y\": 0.0}, {\"x\": 2021.777, \"y\": 0.0}, {\"x\": 2021.78, \"y\": 0.0}, {\"x\": 2021.935, \"y\": 0.0}, {\"x\": 2021.949, \"y\": 0.0}, {\"x\": 2021.976, \"y\": 0.0}, {\"x\": 2022.086, \"y\": 0.0}, {\"x\": 2022.107, \"y\": 0.0}, {\"x\": 2022.127, \"y\": 0.0}, {\"x\": 2022.243, \"y\": 0.0}, {\"x\": 2022.258, \"y\": 0.0}, {\"x\": 2022.326, \"y\": 0.0}, {\"x\": 2022.336, \"y\": 0.0}, {\"x\": 2022.357, \"y\": 0.0}, {\"x\": 2022.438, \"y\": 0.0}, {\"x\": 2022.471, \"y\": 0.0}, {\"x\": 2022.474, \"y\": 0.0}, {\"x\": 2022.476, \"y\": 0.0}, {\"x\": 2022.493, \"y\": 0.0}, {\"x\": 2022.586, \"y\": 0.0}, {\"x\": 2022.591, \"y\": 0.0}, {\"x\": 2022.607, \"y\": 0.0}, {\"x\": 2022.621, \"y\": 0.0}, {\"x\": 2022.802, \"y\": 0.0}, {\"x\": 2022.852, \"y\": 0.0}, {\"x\": 2022.874, \"y\": 0.0}, {\"x\": 2022.907, \"y\": 0.0}, {\"x\": 2022.927, \"y\": 0.0}, {\"x\": 2023.002, \"y\": 0.0}, {\"x\": 2023.107, \"y\": 0.0}, {\"x\": 2023.146, \"y\": 0.0}, {\"x\": 2023.154, \"y\": 0.0}, {\"x\": 2023.205, \"y\": 0.0}, {\"x\": 2023.218, \"y\": 0.0}, {\"x\": 2023.221, \"y\": 0.0}, {\"x\": 2023.246, \"y\": 0.0}, {\"x\": 2023.357, \"y\": 0.0}, {\"x\": 2023.474, \"y\": 0.0}, {\"x\": 2023.476, \"y\": 0.0}, {\"x\": 2023.513, \"y\": 0.0}, {\"x\": 2023.527, \"y\": 0.0}, {\"x\": 2023.546, \"y\": 0.0}, {\"x\": 2023.618, \"y\": 0.0}, {\"x\": 2023.64, \"y\": 0.0}, {\"x\": 2023.68, \"y\": 0.0}, {\"x\": 2023.74, \"y\": 0.0}, {\"x\": 2023.774, \"y\": 0.0}, {\"x\": 2023.821, \"y\": 0.0}, {\"x\": 2023.829, \"y\": 0.0}, {\"x\": 2023.832, \"y\": 0.0}, {\"x\": 2023.833, \"y\": 0.0}, {\"x\": 2023.836, \"y\": 0.0}, {\"x\": 2023.847, \"y\": 0.0}, {\"x\": 2023.871, \"y\": 0.0}, {\"x\": 2023.89, \"y\": 0.0}, {\"x\": 2023.912, \"y\": 0.0}, {\"x\": 2023.93, \"y\": 0.0}, {\"x\": 2023.935, \"y\": 0.0}, {\"x\": 2023.952, \"y\": 0.0}, {\"x\": 2024.01, \"y\": 0.0}, {\"x\": 2024.076, \"y\": 0.0}, {\"x\": 2024.138, \"y\": 0.0}, {\"x\": 2024.151, \"y\": 0.0}, {\"x\": 2024.16, \"y\": 0.0}, {\"x\": 2024.237, \"y\": 0.0}, {\"x\": 2024.237, \"y\": 41.66666666666667}, {\"x\": 2024.237, \"y\": 41.66666666666667}, {\"x\": 2024.16, \"y\": 40.66666666666667}, {\"x\": 2024.151, \"y\": 39.66666666666667}, {\"x\": 2024.138, \"y\": 39.00000000000001}, {\"x\": 2024.076, \"y\": 38.33333333333334}, {\"x\": 2024.01, \"y\": 38.00000000000001}, {\"x\": 2023.952, \"y\": 38.00000000000001}, {\"x\": 2023.935, \"y\": 38.00000000000001}, {\"x\": 2023.93, \"y\": 37.66666666666667}, {\"x\": 2023.912, \"y\": 37.00000000000001}, {\"x\": 2023.89, \"y\": 36.00000000000001}, {\"x\": 2023.871, \"y\": 35.00000000000001}, {\"x\": 2023.847, \"y\": 34.33333333333334}, {\"x\": 2023.836, \"y\": 34.00000000000001}, {\"x\": 2023.833, \"y\": 34.00000000000001}, {\"x\": 2023.832, \"y\": 34.00000000000001}, {\"x\": 2023.829, \"y\": 34.00000000000001}, {\"x\": 2023.821, \"y\": 34.00000000000001}, {\"x\": 2023.774, \"y\": 34.00000000000001}, {\"x\": 2023.74, \"y\": 34.00000000000001}, {\"x\": 2023.68, \"y\": 33.66666666666667}, {\"x\": 2023.64, \"y\": 32.66666666666667}, {\"x\": 2023.618, \"y\": 30.66666666666667}, {\"x\": 2023.546, \"y\": 28.66666666666667}, {\"x\": 2023.527, \"y\": 27.33333333333334}, {\"x\": 2023.513, \"y\": 26.66666666666667}, {\"x\": 2023.476, \"y\": 26.000000000000004}, {\"x\": 2023.474, \"y\": 25.000000000000004}, {\"x\": 2023.357, \"y\": 24.000000000000004}, {\"x\": 2023.246, \"y\": 23.333333333333336}, {\"x\": 2023.221, \"y\": 23.000000000000004}, {\"x\": 2023.218, \"y\": 22.66666666666667}, {\"x\": 2023.205, \"y\": 22.000000000000004}, {\"x\": 2023.154, \"y\": 21.000000000000004}, {\"x\": 2023.146, \"y\": 20.000000000000004}, {\"x\": 2023.107, \"y\": 19.333333333333336}, {\"x\": 2023.002, \"y\": 18.666666666666668}, {\"x\": 2022.927, \"y\": 18.0}, {\"x\": 2022.907, \"y\": 17.0}, {\"x\": 2022.874, \"y\": 15.999999999999998}, {\"x\": 2022.852, \"y\": 14.999999999999998}, {\"x\": 2022.802, \"y\": 14.333333333333332}, {\"x\": 2022.621, \"y\": 13.999999999999998}, {\"x\": 2022.607, \"y\": 13.999999999999998}, {\"x\": 2022.591, \"y\": 13.666666666666664}, {\"x\": 2022.586, \"y\": 12.999999999999998}, {\"x\": 2022.493, \"y\": 12.333333333333332}, {\"x\": 2022.476, \"y\": 11.666666666666666}, {\"x\": 2022.474, \"y\": 11.0}, {\"x\": 2022.471, \"y\": 10.0}, {\"x\": 2022.438, \"y\": 9.0}, {\"x\": 2022.357, \"y\": 8.0}, {\"x\": 2022.336, \"y\": 7.333333333333334}, {\"x\": 2022.326, \"y\": 6.666666666666667}, {\"x\": 2022.258, \"y\": 6.333333333333334}, {\"x\": 2022.243, \"y\": 5.666666666666667}, {\"x\": 2022.127, \"y\": 5.0}, {\"x\": 2022.107, \"y\": 4.333333333333333}, {\"x\": 2022.086, \"y\": 4.0}, {\"x\": 2021.976, \"y\": 3.666666666666667}, {\"x\": 2021.949, \"y\": 3.3333333333333335}, {\"x\": 2021.935, \"y\": 3.0}, {\"x\": 2021.78, \"y\": 2.6666666666666665}, {\"x\": 2021.777, \"y\": 2.333333333333333}, {\"x\": 2021.691, \"y\": 1.9999999999999998}, {\"x\": 2021.61, \"y\": 1.6666666666666665}, {\"x\": 2020.407, \"y\": 1.0}, {\"x\": 2020.073, \"y\": 0.3333333333333333}, {\"x\": 2017.796, \"y\": 0.0}, {\"x\": 2017.0, \"y\": 0.0}, {\"x\": 2017.0, \"y\": 0.0}], \"closed\": true, \"strokeWidth\": 0, \"strokeColor\": \"rgb(0.0, 165.0, 166.0)\", \"strokeAlpha\": 1.0}, {\"type\": \"polygon\", \"label\": \"China\", \"color\": \"rgb(252.0, 101.0, 56.0)\", \"alpha\": 1.0, \"zOrder\": 1, \"points\": [{\"x\": 2017.0, \"y\": 0.0}, {\"x\": 2017.0, \"y\": 0.0}, {\"x\": 2017.796, \"y\": 0.0}, {\"x\": 2020.073, \"y\": 0.3333333333333333}, {\"x\": 2020.407, \"y\": 1.0}, {\"x\": 2021.61, \"y\": 1.6666666666666665}, {\"x\": 2021.691, \"y\": 1.9999999999999998}, {\"x\": 2021.777, \"y\": 2.333333333333333}, {\"x\": 2021.78, \"y\": 2.6666666666666665}, {\"x\": 2021.935, \"y\": 3.0}, {\"x\": 2021.949, \"y\": 3.3333333333333335}, {\"x\": 2021.976, \"y\": 3.666666666666667}, {\"x\": 2022.086, \"y\": 4.0}, {\"x\": 2022.107, \"y\": 4.333333333333333}, {\"x\": 2022.127, \"y\": 5.0}, {\"x\": 2022.243, \"y\": 5.666666666666667}, {\"x\": 2022.258, \"y\": 6.333333333333334}, {\"x\": 2022.326, \"y\": 6.666666666666667}, {\"x\": 2022.336, \"y\": 7.333333333333334}, {\"x\": 2022.357, \"y\": 8.0}, {\"x\": 2022.438, \"y\": 9.0}, {\"x\": 2022.471, \"y\": 10.0}, {\"x\": 2022.474, \"y\": 11.0}, {\"x\": 2022.476, \"y\": 11.666666666666666}, {\"x\": 2022.493, \"y\": 12.333333333333332}, {\"x\": 2022.586, \"y\": 12.999999999999998}, {\"x\": 2022.591, \"y\": 13.666666666666664}, {\"x\": 2022.607, \"y\": 13.999999999999998}, {\"x\": 2022.621, \"y\": 13.999999999999998}, {\"x\": 2022.802, \"y\": 14.333333333333332}, {\"x\": 2022.852, \"y\": 14.999999999999998}, {\"x\": 2022.874, \"y\": 15.999999999999998}, {\"x\": 2022.907, \"y\": 17.0}, {\"x\": 2022.927, \"y\": 18.0}, {\"x\": 2023.002, \"y\": 18.666666666666668}, {\"x\": 2023.107, \"y\": 19.333333333333336}, {\"x\": 2023.146, \"y\": 20.000000000000004}, {\"x\": 2023.154, \"y\": 21.000000000000004}, {\"x\": 2023.205, \"y\": 22.000000000000004}, {\"x\": 2023.218, \"y\": 22.66666666666667}, {\"x\": 2023.221, \"y\": 23.000000000000004}, {\"x\": 2023.246, \"y\": 23.333333333333336}, {\"x\": 2023.357, \"y\": 24.000000000000004}, {\"x\": 2023.474, \"y\": 25.000000000000004}, {\"x\": 2023.476, \"y\": 26.000000000000004}, {\"x\": 2023.513, \"y\": 26.66666666666667}, {\"x\": 2023.527, \"y\": 27.33333333333334}, {\"x\": 2023.546, \"y\": 28.66666666666667}, {\"x\": 2023.618, \"y\": 30.66666666666667}, {\"x\": 2023.64, \"y\": 32.66666666666667}, {\"x\": 2023.68, \"y\": 33.66666666666667}, {\"x\": 2023.74, \"y\": 34.00000000000001}, {\"x\": 2023.774, \"y\": 34.00000000000001}, {\"x\": 2023.821, \"y\": 34.00000000000001}, {\"x\": 2023.829, \"y\": 34.00000000000001}, {\"x\": 2023.832, \"y\": 34.00000000000001}, {\"x\": 2023.833, \"y\": 34.00000000000001}, {\"x\": 2023.836, \"y\": 34.00000000000001}, {\"x\": 2023.847, \"y\": 34.33333333333334}, {\"x\": 2023.871, \"y\": 35.00000000000001}, {\"x\": 2023.89, \"y\": 36.00000000000001}, {\"x\": 2023.912, \"y\": 37.00000000000001}, {\"x\": 2023.93, \"y\": 37.66666666666667}, {\"x\": 2023.935, \"y\": 38.00000000000001}, {\"x\": 2023.952, \"y\": 38.00000000000001}, {\"x\": 2024.01, \"y\": 38.00000000000001}, {\"x\": 2024.076, \"y\": 38.33333333333334}, {\"x\": 2024.138, \"y\": 39.00000000000001}, {\"x\": 2024.151, \"y\": 39.66666666666667}, {\"x\": 2024.16, \"y\": 40.66666666666667}, {\"x\": 2024.237, \"y\": 41.66666666666667}, {\"x\": 2024.237, \"y\": 60.66666666666667}, {\"x\": 2024.237, \"y\": 60.66666666666667}, {\"x\": 2024.16, \"y\": 59.66666666666667}, {\"x\": 2024.151, \"y\": 58.66666666666667}, {\"x\": 2024.138, \"y\": 58.0}, {\"x\": 2024.076, \"y\": 57.00000000000001}, {\"x\": 2024.01, \"y\": 56.33333333333334}, {\"x\": 2023.952, \"y\": 55.66666666666667}, {\"x\": 2023.935, \"y\": 55.33333333333334}, {\"x\": 2023.93, \"y\": 54.333333333333336}, {\"x\": 2023.912, \"y\": 53.33333333333334}, {\"x\": 2023.89, \"y\": 52.00000000000001}, {\"x\": 2023.871, \"y\": 51.00000000000001}, {\"x\": 2023.847, \"y\": 49.66666666666668}, {\"x\": 2023.836, \"y\": 48.33333333333334}, {\"x\": 2023.833, \"y\": 47.00000000000001}, {\"x\": 2023.832, \"y\": 46.00000000000001}, {\"x\": 2023.829, \"y\": 45.00000000000001}, {\"x\": 2023.821, \"y\": 44.00000000000001}, {\"x\": 2023.774, \"y\": 42.66666666666667}, {\"x\": 2023.74, \"y\": 41.00000000000001}, {\"x\": 2023.68, \"y\": 39.333333333333336}, {\"x\": 2023.64, \"y\": 37.66666666666667}, {\"x\": 2023.618, \"y\": 35.66666666666667}, {\"x\": 2023.546, \"y\": 33.66666666666667}, {\"x\": 2023.527, \"y\": 32.00000000000001}, {\"x\": 2023.513, \"y\": 31.000000000000004}, {\"x\": 2023.476, \"y\": 30.000000000000004}, {\"x\": 2023.474, \"y\": 29.000000000000004}, {\"x\": 2023.357, \"y\": 28.000000000000004}, {\"x\": 2023.246, \"y\": 27.333333333333336}, {\"x\": 2023.221, \"y\": 26.66666666666667}, {\"x\": 2023.218, \"y\": 26.000000000000004}, {\"x\": 2023.205, \"y\": 25.000000000000004}, {\"x\": 2023.154, \"y\": 24.000000000000004}, {\"x\": 2023.146, \"y\": 23.000000000000004}, {\"x\": 2023.107, \"y\": 22.333333333333336}, {\"x\": 2023.002, \"y\": 21.666666666666668}, {\"x\": 2022.927, \"y\": 21.0}, {\"x\": 2022.907, \"y\": 20.0}, {\"x\": 2022.874, \"y\": 19.0}, {\"x\": 2022.852, \"y\": 18.0}, {\"x\": 2022.802, \"y\": 17.333333333333332}, {\"x\": 2022.621, \"y\": 17.0}, {\"x\": 2022.607, \"y\": 16.666666666666664}, {\"x\": 2022.591, \"y\": 15.999999999999996}, {\"x\": 2022.586, \"y\": 14.999999999999998}, {\"x\": 2022.493, \"y\": 14.333333333333332}, {\"x\": 2022.476, \"y\": 13.666666666666666}, {\"x\": 2022.474, \"y\": 13.0}, {\"x\": 2022.471, \"y\": 12.0}, {\"x\": 2022.438, \"y\": 11.0}, {\"x\": 2022.357, \"y\": 10.0}, {\"x\": 2022.336, \"y\": 9.333333333333334}, {\"x\": 2022.326, \"y\": 8.666666666666666}, {\"x\": 2022.258, \"y\": 8.333333333333334}, {\"x\": 2022.243, \"y\": 7.666666666666667}, {\"x\": 2022.127, \"y\": 7.0}, {\"x\": 2022.107, \"y\": 6.333333333333333}, {\"x\": 2022.086, \"y\": 5.666666666666666}, {\"x\": 2021.976, \"y\": 5.0}, {\"x\": 2021.949, \"y\": 4.333333333333334}, {\"x\": 2021.935, \"y\": 3.6666666666666665}, {\"x\": 2021.78, \"y\": 3.0}, {\"x\": 2021.777, \"y\": 2.333333333333333}, {\"x\": 2021.691, \"y\": 1.9999999999999998}, {\"x\": 2021.61, \"y\": 1.6666666666666665}, {\"x\": 2020.407, \"y\": 1.0}, {\"x\": 2020.073, \"y\": 0.3333333333333333}, {\"x\": 2017.796, \"y\": 0.0}, {\"x\": 2017.0, \"y\": 0.0}, {\"x\": 2017.0, \"y\": 0.0}], \"closed\": true, \"strokeWidth\": 0, \"strokeColor\": \"rgb(252.0, 101.0, 56.0)\", \"strokeAlpha\": 1.0}, {\"type\": \"polygon\", \"label\": \"Other\", \"color\": \"rgb(255.0, 178.0, 60.0)\", \"alpha\": 1.0, \"zOrder\": 1, \"points\": [{\"x\": 2017.0, \"y\": 0.0}, {\"x\": 2017.0, \"y\": 0.0}, {\"x\": 2017.796, \"y\": 0.0}, {\"x\": 2020.073, \"y\": 0.3333333333333333}, {\"x\": 2020.407, \"y\": 1.0}, {\"x\": 2021.61, \"y\": 1.6666666666666665}, {\"x\": 2021.691, \"y\": 1.9999999999999998}, {\"x\": 2021.777, \"y\": 2.333333333333333}, {\"x\": 2021.78, \"y\": 3.0}, {\"x\": 2021.935, \"y\": 3.6666666666666665}, {\"x\": 2021.949, \"y\": 4.333333333333334}, {\"x\": 2021.976, \"y\": 5.0}, {\"x\": 2022.086, \"y\": 5.666666666666666}, {\"x\": 2022.107, \"y\": 6.333333333333333}, {\"x\": 2022.127, \"y\": 7.0}, {\"x\": 2022.243, \"y\": 7.666666666666667}, {\"x\": 2022.258, \"y\": 8.333333333333334}, {\"x\": 2022.326, \"y\": 8.666666666666666}, {\"x\": 2022.336, \"y\": 9.333333333333334}, {\"x\": 2022.357, \"y\": 10.0}, {\"x\": 2022.438, \"y\": 11.0}, {\"x\": 2022.471, \"y\": 12.0}, {\"x\": 2022.474, \"y\": 13.0}, {\"x\": 2022.476, \"y\": 13.666666666666666}, {\"x\": 2022.493, \"y\": 14.333333333333332}, {\"x\": 2022.586, \"y\": 14.999999999999998}, {\"x\": 2022.591, \"y\": 15.999999999999996}, {\"x\": 2022.607, \"y\": 16.666666666666664}, {\"x\": 2022.621, \"y\": 17.0}, {\"x\": 2022.802, \"y\": 17.333333333333332}, {\"x\": 2022.852, \"y\": 18.0}, {\"x\": 2022.874, \"y\": 19.0}, {\"x\": 2022.907, \"y\": 20.0}, {\"x\": 2022.927, \"y\": 21.0}, {\"x\": 2023.002, \"y\": 21.666666666666668}, {\"x\": 2023.107, \"y\": 22.333333333333336}, {\"x\": 2023.146, \"y\": 23.000000000000004}, {\"x\": 2023.154, \"y\": 24.000000000000004}, {\"x\": 2023.205, \"y\": 25.000000000000004}, {\"x\": 2023.218, \"y\": 26.000000000000004}, {\"x\": 2023.221, \"y\": 26.66666666666667}, {\"x\": 2023.246, \"y\": 27.333333333333336}, {\"x\": 2023.357, \"y\": 28.000000000000004}, {\"x\": 2023.474, \"y\": 29.000000000000004}, {\"x\": 2023.476, \"y\": 30.000000000000004}, {\"x\": 2023.513, \"y\": 31.000000000000004}, {\"x\": 2023.527, \"y\": 32.00000000000001}, {\"x\": 2023.546, \"y\": 33.66666666666667}, {\"x\": 2023.618, \"y\": 35.66666666666667}, {\"x\": 2023.64, \"y\": 37.66666666666667}, {\"x\": 2023.68, \"y\": 39.333333333333336}, {\"x\": 2023.74, \"y\": 41.00000000000001}, {\"x\": 2023.774, \"y\": 42.66666666666667}, {\"x\": 2023.821, \"y\": 44.00000000000001}, {\"x\": 2023.829, \"y\": 45.00000000000001}, {\"x\": 2023.832, \"y\": 46.00000000000001}, {\"x\": 2023.833, \"y\": 47.00000000000001}, {\"x\": 2023.836, \"y\": 48.33333333333334}, {\"x\": 2023.847, \"y\": 49.66666666666668}, {\"x\": 2023.871, \"y\": 51.00000000000001}, {\"x\": 2023.89, \"y\": 52.00000000000001}, {\"x\": 2023.912, \"y\": 53.33333333333334}, {\"x\": 2023.93, \"y\": 54.333333333333336}, {\"x\": 2023.935, \"y\": 55.33333333333334}, {\"x\": 2023.952, \"y\": 55.66666666666667}, {\"x\": 2024.01, \"y\": 56.33333333333334}, {\"x\": 2024.076, \"y\": 57.00000000000001}, {\"x\": 2024.138, \"y\": 58.0}, {\"x\": 2024.151, \"y\": 58.66666666666667}, {\"x\": 2024.16, \"y\": 59.66666666666667}, {\"x\": 2024.237, \"y\": 60.66666666666667}, {\"x\": 2024.237, \"y\": 70.66666666666667}, {\"x\": 2024.237, \"y\": 70.66666666666667}, {\"x\": 2024.16, \"y\": 69.33333333333334}, {\"x\": 2024.151, \"y\": 68.0}, {\"x\": 2024.138, \"y\": 67.0}, {\"x\": 2024.076, \"y\": 66.0}, {\"x\": 2024.01, \"y\": 65.00000000000001}, {\"x\": 2023.952, \"y\": 64.0}, {\"x\": 2023.935, \"y\": 63.33333333333334}, {\"x\": 2023.93, \"y\": 62.333333333333336}, {\"x\": 2023.912, \"y\": 61.33333333333334}, {\"x\": 2023.89, \"y\": 60.00000000000001}, {\"x\": 2023.871, \"y\": 59.00000000000001}, {\"x\": 2023.847, \"y\": 57.66666666666668}, {\"x\": 2023.836, \"y\": 56.33333333333334}, {\"x\": 2023.833, \"y\": 55.00000000000001}, {\"x\": 2023.832, \"y\": 54.00000000000001}, {\"x\": 2023.829, \"y\": 53.00000000000001}, {\"x\": 2023.821, \"y\": 52.00000000000001}, {\"x\": 2023.774, \"y\": 50.333333333333336}, {\"x\": 2023.74, \"y\": 48.00000000000001}, {\"x\": 2023.68, \"y\": 45.66666666666667}, {\"x\": 2023.64, \"y\": 43.66666666666667}, {\"x\": 2023.618, \"y\": 41.66666666666667}, {\"x\": 2023.546, \"y\": 39.66666666666667}, {\"x\": 2023.527, \"y\": 38.00000000000001}, {\"x\": 2023.513, \"y\": 37.0}, {\"x\": 2023.476, \"y\": 36.0}, {\"x\": 2023.474, \"y\": 35.0}, {\"x\": 2023.357, \"y\": 34.0}, {\"x\": 2023.246, \"y\": 33.0}, {\"x\": 2023.221, \"y\": 32.00000000000001}, {\"x\": 2023.218, \"y\": 30.66666666666667}, {\"x\": 2023.205, \"y\": 29.333333333333336}, {\"x\": 2023.154, \"y\": 28.000000000000004}, {\"x\": 2023.146, \"y\": 27.000000000000004}, {\"x\": 2023.107, \"y\": 26.333333333333336}, {\"x\": 2023.002, \"y\": 25.666666666666668}, {\"x\": 2022.927, \"y\": 25.0}, {\"x\": 2022.907, \"y\": 24.0}, {\"x\": 2022.874, \"y\": 23.0}, {\"x\": 2022.852, \"y\": 22.0}, {\"x\": 2022.802, \"y\": 21.0}, {\"x\": 2022.621, \"y\": 20.333333333333332}, {\"x\": 2022.607, \"y\": 19.666666666666664}, {\"x\": 2022.591, \"y\": 18.999999999999996}, {\"x\": 2022.586, \"y\": 18.0}, {\"x\": 2022.493, \"y\": 17.0}, {\"x\": 2022.476, \"y\": 16.0}, {\"x\": 2022.474, \"y\": 15.0}, {\"x\": 2022.471, \"y\": 14.0}, {\"x\": 2022.438, \"y\": 13.0}, {\"x\": 2022.357, \"y\": 12.0}, {\"x\": 2022.336, \"y\": 11.333333333333334}, {\"x\": 2022.326, \"y\": 10.666666666666666}, {\"x\": 2022.258, \"y\": 10.333333333333334}, {\"x\": 2022.243, \"y\": 9.666666666666666}, {\"x\": 2022.127, \"y\": 9.0}, {\"x\": 2022.107, \"y\": 8.333333333333332}, {\"x\": 2022.086, \"y\": 7.666666666666666}, {\"x\": 2021.976, \"y\": 7.0}, {\"x\": 2021.949, \"y\": 6.333333333333334}, {\"x\": 2021.935, \"y\": 5.666666666666666}, {\"x\": 2021.78, \"y\": 5.0}, {\"x\": 2021.777, \"y\": 3.9999999999999996}, {\"x\": 2021.691, \"y\": 3.0}, {\"x\": 2021.61, \"y\": 1.9999999999999998}, {\"x\": 2020.407, \"y\": 1.0}, {\"x\": 2020.073, \"y\": 0.3333333333333333}, {\"x\": 2017.796, \"y\": 0.0}, {\"x\": 2017.0, \"y\": 0.0}, {\"x\": 2017.0, \"y\": 0.0}], \"closed\": true, \"strokeWidth\": 0, \"strokeColor\": \"rgb(255.0, 178.0, 60.0)\", \"strokeAlpha\": 1.0}, {\"type\": \"polygon\", \"label\": \"United Kingdom\", \"color\": \"rgb(176.0, 135.0, 244.0)\", \"alpha\": 1.0, \"zOrder\": 1, \"points\": [{\"x\": 2017.0, \"y\": 1.0}, {\"x\": 2017.0, \"y\": 0.0}, {\"x\": 2017.796, \"y\": 0.0}, {\"x\": 2020.073, \"y\": 0.3333333333333333}, {\"x\": 2020.407, \"y\": 1.0}, {\"x\": 2021.61, \"y\": 1.9999999999999998}, {\"x\": 2021.691, \"y\": 3.0}, {\"x\": 2021.777, \"y\": 3.9999999999999996}, {\"x\": 2021.78, \"y\": 5.0}, {\"x\": 2021.935, \"y\": 5.666666666666666}, {\"x\": 2021.949, \"y\": 6.333333333333334}, {\"x\": 2021.976, \"y\": 7.0}, {\"x\": 2022.086, \"y\": 7.666666666666666}, {\"x\": 2022.107, \"y\": 8.333333333333332}, {\"x\": 2022.127, \"y\": 9.0}, {\"x\": 2022.243, \"y\": 9.666666666666666}, {\"x\": 2022.258, \"y\": 10.333333333333334}, {\"x\": 2022.326, \"y\": 10.666666666666666}, {\"x\": 2022.336, \"y\": 11.333333333333334}, {\"x\": 2022.357, \"y\": 12.0}, {\"x\": 2022.438, \"y\": 13.0}, {\"x\": 2022.471, \"y\": 14.0}, {\"x\": 2022.474, \"y\": 15.0}, {\"x\": 2022.476, \"y\": 16.0}, {\"x\": 2022.493, \"y\": 17.0}, {\"x\": 2022.586, \"y\": 18.0}, {\"x\": 2022.591, \"y\": 18.999999999999996}, {\"x\": 2022.607, \"y\": 19.666666666666664}, {\"x\": 2022.621, \"y\": 20.333333333333332}, {\"x\": 2022.802, \"y\": 21.0}, {\"x\": 2022.852, \"y\": 22.0}, {\"x\": 2022.874, \"y\": 23.0}, {\"x\": 2022.907, \"y\": 24.0}, {\"x\": 2022.927, \"y\": 25.0}, {\"x\": 2023.002, \"y\": 25.666666666666668}, {\"x\": 2023.107, \"y\": 26.333333333333336}, {\"x\": 2023.146, \"y\": 27.000000000000004}, {\"x\": 2023.154, \"y\": 28.000000000000004}, {\"x\": 2023.205, \"y\": 29.333333333333336}, {\"x\": 2023.218, \"y\": 30.66666666666667}, {\"x\": 2023.221, \"y\": 32.00000000000001}, {\"x\": 2023.246, \"y\": 33.0}, {\"x\": 2023.357, \"y\": 34.0}, {\"x\": 2023.474, \"y\": 35.0}, {\"x\": 2023.476, \"y\": 36.0}, {\"x\": 2023.513, \"y\": 37.0}, {\"x\": 2023.527, \"y\": 38.00000000000001}, {\"x\": 2023.546, \"y\": 39.66666666666667}, {\"x\": 2023.618, \"y\": 41.66666666666667}, {\"x\": 2023.64, \"y\": 43.66666666666667}, {\"x\": 2023.68, \"y\": 45.66666666666667}, {\"x\": 2023.74, \"y\": 48.00000000000001}, {\"x\": 2023.774, \"y\": 50.333333333333336}, {\"x\": 2023.821, \"y\": 52.00000000000001}, {\"x\": 2023.829, \"y\": 53.00000000000001}, {\"x\": 2023.832, \"y\": 54.00000000000001}, {\"x\": 2023.833, \"y\": 55.00000000000001}, {\"x\": 2023.836, \"y\": 56.33333333333334}, {\"x\": 2023.847, \"y\": 57.66666666666668}, {\"x\": 2023.871, \"y\": 59.00000000000001}, {\"x\": 2023.89, \"y\": 60.00000000000001}, {\"x\": 2023.912, \"y\": 61.33333333333334}, {\"x\": 2023.93, \"y\": 62.333333333333336}, {\"x\": 2023.935, \"y\": 63.33333333333334}, {\"x\": 2023.952, \"y\": 64.0}, {\"x\": 2024.01, \"y\": 65.00000000000001}, {\"x\": 2024.076, \"y\": 66.0}, {\"x\": 2024.138, \"y\": 67.0}, {\"x\": 2024.151, \"y\": 68.0}, {\"x\": 2024.16, \"y\": 69.33333333333334}, {\"x\": 2024.237, \"y\": 70.66666666666667}, {\"x\": 2024.237, \"y\": 77.66666666666667}, {\"x\": 2024.237, \"y\": 77.66666666666667}, {\"x\": 2024.16, \"y\": 76.33333333333334}, {\"x\": 2024.151, \"y\": 75.0}, {\"x\": 2024.138, \"y\": 74.0}, {\"x\": 2024.076, \"y\": 73.0}, {\"x\": 2024.01, \"y\": 72.00000000000001}, {\"x\": 2023.952, \"y\": 71.0}, {\"x\": 2023.935, \"y\": 70.33333333333334}, {\"x\": 2023.93, \"y\": 69.33333333333333}, {\"x\": 2023.912, \"y\": 68.33333333333334}, {\"x\": 2023.89, \"y\": 67.0}, {\"x\": 2023.871, \"y\": 66.0}, {\"x\": 2023.847, \"y\": 64.66666666666667}, {\"x\": 2023.836, \"y\": 63.33333333333334}, {\"x\": 2023.833, \"y\": 62.00000000000001}, {\"x\": 2023.832, \"y\": 61.00000000000001}, {\"x\": 2023.829, \"y\": 60.00000000000001}, {\"x\": 2023.821, \"y\": 59.00000000000001}, {\"x\": 2023.774, \"y\": 57.333333333333336}, {\"x\": 2023.74, \"y\": 55.00000000000001}, {\"x\": 2023.68, \"y\": 52.66666666666667}, {\"x\": 2023.64, \"y\": 50.66666666666667}, {\"x\": 2023.618, \"y\": 48.66666666666667}, {\"x\": 2023.546, \"y\": 46.66666666666667}, {\"x\": 2023.527, \"y\": 45.00000000000001}, {\"x\": 2023.513, \"y\": 44.0}, {\"x\": 2023.476, \"y\": 43.0}, {\"x\": 2023.474, \"y\": 42.0}, {\"x\": 2023.357, \"y\": 41.0}, {\"x\": 2023.246, \"y\": 40.0}, {\"x\": 2023.221, \"y\": 39.00000000000001}, {\"x\": 2023.218, \"y\": 37.66666666666667}, {\"x\": 2023.205, \"y\": 36.333333333333336}, {\"x\": 2023.154, \"y\": 35.0}, {\"x\": 2023.146, \"y\": 34.0}, {\"x\": 2023.107, \"y\": 33.333333333333336}, {\"x\": 2023.002, \"y\": 32.666666666666664}, {\"x\": 2022.927, \"y\": 32.0}, {\"x\": 2022.907, \"y\": 31.0}, {\"x\": 2022.874, \"y\": 30.0}, {\"x\": 2022.852, \"y\": 29.0}, {\"x\": 2022.802, \"y\": 28.0}, {\"x\": 2022.621, \"y\": 27.33333333333333}, {\"x\": 2022.607, \"y\": 26.666666666666664}, {\"x\": 2022.591, \"y\": 25.999999999999993}, {\"x\": 2022.586, \"y\": 25.0}, {\"x\": 2022.493, \"y\": 24.0}, {\"x\": 2022.476, \"y\": 23.0}, {\"x\": 2022.474, \"y\": 22.0}, {\"x\": 2022.471, \"y\": 21.0}, {\"x\": 2022.438, \"y\": 20.0}, {\"x\": 2022.357, \"y\": 19.0}, {\"x\": 2022.336, \"y\": 18.0}, {\"x\": 2022.326, \"y\": 17.0}, {\"x\": 2022.258, \"y\": 16.0}, {\"x\": 2022.243, \"y\": 14.999999999999998}, {\"x\": 2022.127, \"y\": 14.0}, {\"x\": 2022.107, \"y\": 12.999999999999998}, {\"x\": 2022.086, \"y\": 12.0}, {\"x\": 2021.976, \"y\": 11.0}, {\"x\": 2021.949, \"y\": 10.0}, {\"x\": 2021.935, \"y\": 9.0}, {\"x\": 2021.78, \"y\": 8.0}, {\"x\": 2021.777, \"y\": 7.0}, {\"x\": 2021.691, \"y\": 6.0}, {\"x\": 2021.61, \"y\": 5.0}, {\"x\": 2020.407, \"y\": 4.0}, {\"x\": 2020.073, \"y\": 3.0}, {\"x\": 2017.796, \"y\": 2.0}, {\"x\": 2017.0, \"y\": 1.0}, {\"x\": 2017.0, \"y\": 1.0}], \"closed\": true, \"strokeWidth\": 0, \"strokeColor\": \"rgb(176.0, 135.0, 244.0)\", \"strokeAlpha\": 1.0}, {\"type\": \"polygon\", \"label\": \"Multinational\", \"color\": \"rgb(31.0, 110.0, 229.0)\", \"alpha\": 1.0, \"zOrder\": 1, \"points\": [{\"x\": 2017.0, \"y\": 1.0}, {\"x\": 2017.0, \"y\": 1.0}, {\"x\": 2017.796, \"y\": 2.0}, {\"x\": 2020.073, \"y\": 3.0}, {\"x\": 2020.407, \"y\": 4.0}, {\"x\": 2021.61, \"y\": 5.0}, {\"x\": 2021.691, \"y\": 6.0}, {\"x\": 2021.777, \"y\": 7.0}, {\"x\": 2021.78, \"y\": 8.0}, {\"x\": 2021.935, \"y\": 9.0}, {\"x\": 2021.949, \"y\": 10.0}, {\"x\": 2021.976, \"y\": 11.0}, {\"x\": 2022.086, \"y\": 12.0}, {\"x\": 2022.107, \"y\": 12.999999999999998}, {\"x\": 2022.127, \"y\": 14.0}, {\"x\": 2022.243, \"y\": 14.999999999999998}, {\"x\": 2022.258, \"y\": 16.0}, {\"x\": 2022.326, \"y\": 17.0}, {\"x\": 2022.336, \"y\": 18.0}, {\"x\": 2022.357, \"y\": 19.0}, {\"x\": 2022.438, \"y\": 20.0}, {\"x\": 2022.471, \"y\": 21.0}, {\"x\": 2022.474, \"y\": 22.0}, {\"x\": 2022.476, \"y\": 23.0}, {\"x\": 2022.493, \"y\": 24.0}, {\"x\": 2022.586, \"y\": 25.0}, {\"x\": 2022.591, \"y\": 25.999999999999993}, {\"x\": 2022.607, \"y\": 26.666666666666664}, {\"x\": 2022.621, \"y\": 27.33333333333333}, {\"x\": 2022.802, \"y\": 28.0}, {\"x\": 2022.852, \"y\": 29.0}, {\"x\": 2022.874, \"y\": 30.0}, {\"x\": 2022.907, \"y\": 31.0}, {\"x\": 2022.927, \"y\": 32.0}, {\"x\": 2023.002, \"y\": 32.666666666666664}, {\"x\": 2023.107, \"y\": 33.333333333333336}, {\"x\": 2023.146, \"y\": 34.0}, {\"x\": 2023.154, \"y\": 35.0}, {\"x\": 2023.205, \"y\": 36.333333333333336}, {\"x\": 2023.218, \"y\": 37.66666666666667}, {\"x\": 2023.221, \"y\": 39.00000000000001}, {\"x\": 2023.246, \"y\": 40.0}, {\"x\": 2023.357, \"y\": 41.0}, {\"x\": 2023.474, \"y\": 42.0}, {\"x\": 2023.476, \"y\": 43.0}, {\"x\": 2023.513, \"y\": 44.0}, {\"x\": 2023.527, \"y\": 45.00000000000001}, {\"x\": 2023.546, \"y\": 46.66666666666667}, {\"x\": 2023.618, \"y\": 48.66666666666667}, {\"x\": 2023.64, \"y\": 50.66666666666667}, {\"x\": 2023.68, \"y\": 52.66666666666667}, {\"x\": 2023.74, \"y\": 55.00000000000001}, {\"x\": 2023.774, \"y\": 57.333333333333336}, {\"x\": 2023.821, \"y\": 59.00000000000001}, {\"x\": 2023.829, \"y\": 60.00000000000001}, {\"x\": 2023.832, \"y\": 61.00000000000001}, {\"x\": 2023.833, \"y\": 62.00000000000001}, {\"x\": 2023.836, \"y\": 63.33333333333334}, {\"x\": 2023.847, \"y\": 64.66666666666667}, {\"x\": 2023.871, \"y\": 66.0}, {\"x\": 2023.89, \"y\": 67.0}, {\"x\": 2023.912, \"y\": 68.33333333333334}, {\"x\": 2023.93, \"y\": 69.33333333333333}, {\"x\": 2023.935, \"y\": 70.33333333333334}, {\"x\": 2023.952, \"y\": 71.0}, {\"x\": 2024.01, \"y\": 72.00000000000001}, {\"x\": 2024.076, \"y\": 73.0}, {\"x\": 2024.138, \"y\": 74.0}, {\"x\": 2024.151, \"y\": 75.0}, {\"x\": 2024.16, \"y\": 76.33333333333334}, {\"x\": 2024.237, \"y\": 77.66666666666667}, {\"x\": 2024.237, \"y\": 80.66666666666667}, {\"x\": 2024.237, \"y\": 80.66666666666667}, {\"x\": 2024.16, \"y\": 79.33333333333334}, {\"x\": 2024.151, \"y\": 78.0}, {\"x\": 2024.138, \"y\": 77.0}, {\"x\": 2024.076, \"y\": 76.0}, {\"x\": 2024.01, \"y\": 75.00000000000001}, {\"x\": 2023.952, \"y\": 74.0}, {\"x\": 2023.935, \"y\": 73.00000000000001}, {\"x\": 2023.93, \"y\": 71.66666666666666}, {\"x\": 2023.912, \"y\": 70.33333333333334}, {\"x\": 2023.89, \"y\": 69.0}, {\"x\": 2023.871, \"y\": 68.0}, {\"x\": 2023.847, \"y\": 66.66666666666667}, {\"x\": 2023.836, \"y\": 65.33333333333334}, {\"x\": 2023.833, \"y\": 64.0}, {\"x\": 2023.832, \"y\": 63.00000000000001}, {\"x\": 2023.829, \"y\": 62.00000000000001}, {\"x\": 2023.821, \"y\": 61.00000000000001}, {\"x\": 2023.774, \"y\": 59.333333333333336}, {\"x\": 2023.74, \"y\": 57.00000000000001}, {\"x\": 2023.68, \"y\": 54.66666666666667}, {\"x\": 2023.64, \"y\": 52.66666666666667}, {\"x\": 2023.618, \"y\": 50.66666666666667}, {\"x\": 2023.546, \"y\": 48.66666666666667}, {\"x\": 2023.527, \"y\": 47.00000000000001}, {\"x\": 2023.513, \"y\": 46.0}, {\"x\": 2023.476, \"y\": 45.0}, {\"x\": 2023.474, \"y\": 44.0}, {\"x\": 2023.357, \"y\": 43.0}, {\"x\": 2023.246, \"y\": 42.0}, {\"x\": 2023.221, \"y\": 41.00000000000001}, {\"x\": 2023.218, \"y\": 39.66666666666667}, {\"x\": 2023.205, \"y\": 38.333333333333336}, {\"x\": 2023.154, \"y\": 37.0}, {\"x\": 2023.146, \"y\": 36.0}, {\"x\": 2023.107, \"y\": 35.0}, {\"x\": 2023.002, \"y\": 34.0}, {\"x\": 2022.927, \"y\": 33.0}, {\"x\": 2022.907, \"y\": 32.0}, {\"x\": 2022.874, \"y\": 31.0}, {\"x\": 2022.852, \"y\": 30.0}, {\"x\": 2022.802, \"y\": 29.0}, {\"x\": 2022.621, \"y\": 27.999999999999996}, {\"x\": 2022.607, \"y\": 26.999999999999996}, {\"x\": 2022.591, \"y\": 25.999999999999993}, {\"x\": 2022.586, \"y\": 25.0}, {\"x\": 2022.493, \"y\": 24.0}, {\"x\": 2022.476, \"y\": 23.0}, {\"x\": 2022.474, \"y\": 22.0}, {\"x\": 2022.471, \"y\": 21.0}, {\"x\": 2022.438, \"y\": 20.0}, {\"x\": 2022.357, \"y\": 19.0}, {\"x\": 2022.336, \"y\": 18.0}, {\"x\": 2022.326, \"y\": 17.0}, {\"x\": 2022.258, \"y\": 16.0}, {\"x\": 2022.243, \"y\": 14.999999999999998}, {\"x\": 2022.127, \"y\": 14.0}, {\"x\": 2022.107, \"y\": 12.999999999999998}, {\"x\": 2022.086, \"y\": 12.0}, {\"x\": 2021.976, \"y\": 11.0}, {\"x\": 2021.949, \"y\": 10.0}, {\"x\": 2021.935, \"y\": 9.0}, {\"x\": 2021.78, \"y\": 8.0}, {\"x\": 2021.777, \"y\": 7.0}, {\"x\": 2021.691, \"y\": 6.0}, {\"x\": 2021.61, \"y\": 5.0}, {\"x\": 2020.407, \"y\": 4.0}, {\"x\": 2020.073, \"y\": 3.0}, {\"x\": 2017.796, \"y\": 2.0}, {\"x\": 2017.0, \"y\": 1.0}, {\"x\": 2017.0, \"y\": 1.0}], \"closed\": true, \"strokeWidth\": 0, \"strokeColor\": \"rgb(31.0, 110.0, 229.0)\", \"strokeAlpha\": 1.0}], \"hud\": [], \"originalDataAspectRatio\": 0.7451612903225805, \"additionalLegendItems\": []} Figure 5: Number of large-scale AI models developed by selected countries, over time. Multinational refers to models developed by collaborators in multiple countries. DeepMind is considered to be within the United Kingdom prior to April 2023; following its merger with Google Brain, models developed by Google DeepMind are considered multinational. Other refers to models developed within any single country not listed in the legend. 3 The leading organizations in number of confirmed large-scale models are Google, Meta, DeepMind 4 , Hugging Face, and OpenAI. Other developers include corporations, universities, and governments. 5 Findings are broadly similar when including unconfirmed models, although organizations such as Anthropic and Alibaba move further up the ranking. The vast majority of large-scale models are developed by industry (71) rather than academia (2), with a few industry-academia collaborations (6), and a couple developed by government institutions (2). Slightly over half (53) have been documented in an academic format, although only 17 of these have been published in a peer-reviewed journal or conference, with the remaining 36 published as arXiv preprints or similar. {\"title\": \"Large-scale models by organization\", \"xAxis\": {\"label\": \"Number of models\", \"lim\": [0.0, 19.0], \"scaleType\": \"linear\", \"ticks\": [0, 5, 10, 15, 20], \"tickLabels\": [\"0\", \"5\", \"10\", \"15\", \"20\"], \"nice\": false}, \"yAxis\": {\"lim\": [-0.36, 10.65], \"scaleType\": \"linear\", \"ticks\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"tickLabels\": [\"Hugging Face\", \"BAAI\", \"Tsinghua\", \"Mistral AI\", \"NVIDIA\", \"DeepMind\", \"Alibaba\", \"Anthropic\", \"OpenAI\", \"Meta\", \"Google\"], \"hideTicks\": true, \"keepAllLabels\": true, \"keepLabelSizes\": true}, \"showLegend\": true, \"legendPosition\": \"header\", \"showFrame\": false, \"addDataPadding\": false, \"showYGrid\": false, \"showYBasis\": true, \"tooltipType\": \"group-multitooltip\", \"objects\": [{\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Hugging Face\", \"points\": [{\"x\": 0, \"y\": -0.35}, {\"x\": 4, \"y\": -0.35}, {\"x\": 4, \"y\": 0.35}, {\"x\": 0, \"y\": 0.35}, {\"x\": 0, \"y\": -0.35}], \"closed\": true, \"tooltipData\": 4}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"BAAI\", \"points\": [{\"x\": 0, \"y\": 0.65}, {\"x\": 0, \"y\": 0.65}, {\"x\": 0, \"y\": 1.35}, {\"x\": 0, \"y\": 1.35}, {\"x\": 0, \"y\": 0.65}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Tsinghua\", \"points\": [{\"x\": 0, \"y\": 1.65}, {\"x\": 2, \"y\": 1.65}, {\"x\": 2, \"y\": 2.3499999999999996}, {\"x\": 0, \"y\": 2.3499999999999996}, {\"x\": 0, \"y\": 1.65}], \"closed\": true, \"tooltipData\": 2}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Mistral AI\", \"points\": [{\"x\": 0, \"y\": 2.65}, {\"x\": 1, \"y\": 2.65}, {\"x\": 1, \"y\": 3.3499999999999996}, {\"x\": 0, \"y\": 3.3499999999999996}, {\"x\": 0, \"y\": 2.65}], \"closed\": true, \"tooltipData\": 1}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"NVIDIA\", \"points\": [{\"x\": 0, \"y\": 3.65}, {\"x\": 4, \"y\": 3.65}, {\"x\": 4, \"y\": 4.35}, {\"x\": 0, \"y\": 4.35}, {\"x\": 0, \"y\": 3.65}], \"closed\": true, \"tooltipData\": 4}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"DeepMind\", \"points\": [{\"x\": 0, \"y\": 4.65}, {\"x\": 6, \"y\": 4.65}, {\"x\": 6, \"y\": 5.3500000000000005}, {\"x\": 0, \"y\": 5.3500000000000005}, {\"x\": 0, \"y\": 4.65}], \"closed\": true, \"tooltipData\": 6}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Alibaba\", \"points\": [{\"x\": 0, \"y\": 5.65}, {\"x\": 3, \"y\": 5.65}, {\"x\": 3, \"y\": 6.3500000000000005}, {\"x\": 0, \"y\": 6.3500000000000005}, {\"x\": 0, \"y\": 5.65}], \"closed\": true, \"tooltipData\": 3}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Anthropic\", \"points\": [{\"x\": 0, \"y\": 6.65}, {\"x\": 1, \"y\": 6.65}, {\"x\": 1, \"y\": 7.3500000000000005}, {\"x\": 0, \"y\": 7.3500000000000005}, {\"x\": 0, \"y\": 6.65}], \"closed\": true, \"tooltipData\": 1}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"OpenAI\", \"points\": [{\"x\": 0, \"y\": 7.65}, {\"x\": 5, \"y\": 7.65}, {\"x\": 5, \"y\": 8.35}, {\"x\": 0, \"y\": 8.35}, {\"x\": 0, \"y\": 7.65}], \"closed\": true, \"tooltipData\": 5}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Meta\", \"points\": [{\"x\": 0, \"y\": 8.65}, {\"x\": 12, \"y\": 8.65}, {\"x\": 12, \"y\": 9.35}, {\"x\": 0, \"y\": 9.35}, {\"x\": 0, \"y\": 8.65}], \"closed\": true, \"tooltipData\": 12}, {\"type\": \"polygon\", \"label\": \"Large-scale\", \"color\": \"rgb(0.0, 165.0, 166.0)\", \"zOrder\": 1, \"tooltipGroup\": \"Google\", \"points\": [{\"x\": 0, \"y\": 9.65}, {\"x\": 14, \"y\": 9.65}, {\"x\": 14, \"y\": 10.35}, {\"x\": 0, \"y\": 10.35}, {\"x\": 0, \"y\": 9.65}], \"closed\": true, \"tooltipData\": 14}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Hugging Face\", \"points\": [{\"x\": 4, \"y\": -0.35}, {\"x\": 4, \"y\": -0.35}, {\"x\": 4, \"y\": 0.35}, {\"x\": 4, \"y\": 0.35}, {\"x\": 4, \"y\": -0.35}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"BAAI\", \"points\": [{\"x\": 0, \"y\": 0.65}, {\"x\": 4, \"y\": 0.65}, {\"x\": 4, \"y\": 1.35}, {\"x\": 0, \"y\": 1.35}, {\"x\": 0, \"y\": 0.65}], \"closed\": true, \"tooltipData\": 4}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Tsinghua\", \"points\": [{\"x\": 2, \"y\": 1.65}, {\"x\": 4, \"y\": 1.65}, {\"x\": 4, \"y\": 2.3499999999999996}, {\"x\": 2, \"y\": 2.3499999999999996}, {\"x\": 2, \"y\": 1.65}], \"closed\": true, \"tooltipData\": 2}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Mistral AI\", \"points\": [{\"x\": 1, \"y\": 2.65}, {\"x\": 4, \"y\": 2.65}, {\"x\": 4, \"y\": 3.3499999999999996}, {\"x\": 1, \"y\": 3.3499999999999996}, {\"x\": 1, \"y\": 2.65}], \"closed\": true, \"tooltipData\": 3}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"NVIDIA\", \"points\": [{\"x\": 4, \"y\": 3.65}, {\"x\": 5, \"y\": 3.65}, {\"x\": 5, \"y\": 4.35}, {\"x\": 4, \"y\": 4.35}, {\"x\": 4, \"y\": 3.65}], \"closed\": true, \"tooltipData\": 1}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"DeepMind\", \"points\": [{\"x\": 6, \"y\": 4.65}, {\"x\": 6, \"y\": 4.65}, {\"x\": 6, \"y\": 5.3500000000000005}, {\"x\": 6, \"y\": 5.3500000000000005}, {\"x\": 6, \"y\": 4.65}], \"closed\": true, \"tooltipData\": 0}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Alibaba\", \"points\": [{\"x\": 3, \"y\": 5.65}, {\"x\": 6, \"y\": 5.65}, {\"x\": 6, \"y\": 6.3500000000000005}, {\"x\": 3, \"y\": 6.3500000000000005}, {\"x\": 3, \"y\": 5.65}], \"closed\": true, \"tooltipData\": 3}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Anthropic\", \"points\": [{\"x\": 1, \"y\": 6.65}, {\"x\": 9, \"y\": 6.65}, {\"x\": 9, \"y\": 7.3500000000000005}, {\"x\": 1, \"y\": 7.3500000000000005}, {\"x\": 1, \"y\": 6.65}], \"closed\": true, \"tooltipData\": 8}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"OpenAI\", \"points\": [{\"x\": 5, \"y\": 7.65}, {\"x\": 10, \"y\": 7.65}, {\"x\": 10, \"y\": 8.35}, {\"x\": 5, \"y\": 8.35}, {\"x\": 5, \"y\": 7.65}], \"closed\": true, \"tooltipData\": 5}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Meta\", \"points\": [{\"x\": 12, \"y\": 8.65}, {\"x\": 14, \"y\": 8.65}, {\"x\": 14, \"y\": 9.35}, {\"x\": 12, \"y\": 9.35}, {\"x\": 12, \"y\": 8.65}], \"closed\": true, \"tooltipData\": 2}, {\"type\": \"polygon\", \"label\": \"Unconfirmed large-scale\", \"color\": \"rgb(155.0, 191.0, 193.0)\", \"alpha\": 0.4, \"zOrder\": 1, \"tooltipGroup\": \"Google\", \"points\": [{\"x\": 14, \"y\": 9.65}, {\"x\": 18, \"y\": 9.65}, {\"x\": 18, \"y\": 10.35}, {\"x\": 14, \"y\": 10.35}, {\"x\": 14, \"y\": 9.65}], \"closed\": true, \"tooltipData\": 4}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"4\", \"x\": 4.1, \"y\": 0, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"4\", \"x\": 4.1, \"y\": 1, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"2\", \"x\": 1.0, \"y\": 2, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"2\", \"x\": 3.0, \"y\": 2, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"4\", \"x\": 4.1, \"y\": 2, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"1\", \"x\": 0.5, \"y\": 3, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"3\", \"x\": 2.5, \"y\": 3, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"4\", \"x\": 4.1, \"y\": 3, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"4\", \"x\": 2.0, \"y\": 4, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"1\", \"x\": 4.5, \"y\": 4, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"5\", \"x\": 5.1, \"y\": 4, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"6\", \"x\": 6.1, \"y\": 5, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"3\", \"x\": 1.5, \"y\": 6, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"3\", \"x\": 4.5, \"y\": 6, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"6\", \"x\": 6.1, \"y\": 6, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"1\", \"x\": 0.5, \"y\": 7, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"8\", \"x\": 5.0, \"y\": 7, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"9\", \"x\": 9.1, \"y\": 7, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"5\", \"x\": 2.5, \"y\": 8, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"5\", \"x\": 7.5, \"y\": 8, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"10\", \"x\": 10.1, \"y\": 8, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"12\", \"x\": 6.0, \"y\": 9, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"2\", \"x\": 13.0, \"y\": 9, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"14\", \"x\": 14.1, \"y\": 9, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"14\", \"x\": 7.0, \"y\": 10, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"4\", \"x\": 16.0, \"y\": 10, \"ha\": \"center\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"normal\"}, {\"type\": \"text\", \"color\": \"#102930\", \"zOrder\": 3, \"text\": \"18\", \"x\": 18.1, \"y\": 10, \"ha\": \"left\", \"va\": \"central\", \"rotation\": 0.0, \"rotationMode\": \"default\", \"weight\": \"semibold\"}], \"hud\": [], \"originalDataAspectRatio\": 0.7451612903225805, \"additionalLegendItems\": []} Figure 6: Number of large-scale AI models by organization, for organizations involved in at least 4 such models. Some models are developed by multiple organizations and therefore appear in each of their tallies. Downloadable models are common, but have lower training compute 38 out of 81 large-scale models are downloadable 6 , with their model weights available to the public. The downloadable model trained with the most compute to date is Falcon-180B, which was trained using 3.8 * 10 24 FLOP, or roughly \u2155 as much compute as GPT-4. Most (86%) of these downloadable models were trained on between 10 23 and 10 24 FLOP. Meta\u2019s OPT-175B , released in May 2022, was the first downloadable model to reach the 10 23 level. To date, Meta has now released a dozen downloadable models at this scale and has committed to continuing this approach in the future, suggesting that open-access ML will continue to grow with the investments of one of the largest tech companies. Chinese labs are well-represented among large-scale downloadable models \u2014 12 were developed by organizations based in China. Alibaba\u2019s Qwen is one notable example, achieving state-of-the-art performance on several Chinese benchmarks. All large-scale downloadable models from China are large language models trained on datasets containing trillions of tokens of English and Chinese text. Most of them primarily function as chatbots, but several are also trained for code generation. The organizations developing these large models include established companies from the Chinese technology industry like Alibaba, Baidu, Huawei, and research institutions such as Tsinghua University. There are also several new AI startups, including DeepSeek and Baichuan. Methods for finding large-scale models We used a variety of resources to find candidate models that were plausibly trained using more than 10 23 FLOP. We then estimated how much compute was used to train these models when it is not directly reported by the developers. The models were added to our database , along with estimates of their training compute, number of parameters, and amount of training data whenever this information was available. Models trained with over 10 23 FLOP were tallied and the results presented in this article. Benchmarks and Repositories Benchmark leaderboards are an important tool for identifying frontier models because researchers evaluate their models against benchmarks as part of standard practice and models trained on the most compute tend to perform the best . We have searched a variety of benchmarks in depth, selected to provide coverage of many different domains of machine learning. Papers With Code (PWC) maintains a collection of machine learning research papers that have been published along with their repositories, allowing other researchers to replicate their work. It also has a collection of benchmarks , each with a leaderboard tracking the state-of-the-art level of benchmark performance over time. For every benchmark with over 100 model evaluations submitted, we examined the top-performing model. Additionally, we examined every model in the PWC dataset that had at least 1 billion parameters, estimating their training compute based on their associated papers, and added them to our database. The Center for Research on Foundation Models (CRFM) at Stanford University also works on tracking large models, especially foundation models used in downstream applications after fine-tuning, and has some resources showing a number of large-scale systems. Their Holistic Evaluation of Language Models 7 project has a live leaderboard ranking many large models on a variety of metrics and scenarios. CRFM also graphically tracks the ecosystem of foundation models, child models, and datasets, and provides a table of information on these components. We\u2019ve checked their sources for information relevant for estimating compute and collected candidate models from the models table. Hugging Face is a company that maintains a large online repository of open-source model weights. We used their API to search for models with over 100 downloads and at least 1 billion parameters. Hugging Face also operates Chatbot Arena , a crowdsourced comparison platform where users chat with two LMs at once and choose the one whose responses they like better, which adjusts the models\u2019 Elo ratings accordingly. Chatbot Arena\u2019s leaderboard then ranks all the competing models by Elo rating. We\u2019ve collected all of the models from the leaderboard with training runs likely to have used at least 10 23 FLOP. Non-English news and websites As the fields of AI and ML grow, development of large models is spreading around the world, and systems developed in regions where languages other than English are predominant may not receive enough coverage in English-language media to come to our attention unless we seek them out. Considering the most commonly spoken languages around the world and in areas with active technology industries, we selected the following languages and did searches using the most popular search engines for each language: Chinese, Hindi, Spanish, French, Arabic, Bengali, Portuguese, Russian, Urdu, German, Japanese, Turkish, Korean, Persian, and Hebrew. 8 We looked at the top 100 results for each of the 3-6 most relevant AI keywords in each language, filtered to 2020-present, encompassing the time period when models began to regularly exceed 10 23 FLOP. One of the most fruitful search avenues has been Chinese-language media, which has extensively covered recent AI developments. We found a large number of previously unfamiliar large-scale models through Baidu searches for terms such as \u201cchatbot\u201d, \u201ctrillion parameter\u201d, and \u201clarge AI model\u201d. For each keyword, we translated the term into Chinese using Google Translate, searched for the translated term on Baidu, and recorded any AI models mentioned in the first 100 search results. These were then researched to determine whether they were trained using at least 10 23 floating-point operations. We also looked at two leaderboards, SuperCLUE (Chinese Language Understanding Evaluation) and OpenCompass , that rank many LLMs by their performance on Chinese-language benchmarks. Most of these LLMs were developed in China. These leaderboards were checked for models previously missing from Epoch AI\u2019s database. Other sources Several other sources were used to identify models for the database, though these were not checked exhaustively. We reviewed blog posts and press releases from the major frontier labs (Anthropic, Google, DeepMind, Microsoft, OpenAI) from the past few years to identify research and products involving large models. Epoch AI staff also follow machine learning research and announcements on various platforms and newsletters. We collect new model releases and then add them to the database. We periodically conduct literature searches of machine learning topics in order to find highly influential research. large-scale systems are frequently (but not always) highly cited, which is to be expected because well-funded research has more potential to achieve breakthroughs and attention within the field. However, during this focused effort to find large-scale models, no models were identified in scholarly searches or bibliographies that were not already identified by the other methods above. Unconfirmed large-scale models A challenge in this work is that many models do not report enough details to estimate training compute. Our dataset focuses on models with known training compute, but this precludes many notable models such as Claude 3 or Sora. To mitigate this, we collected a separate table of unconfirmed large-scale models, where compute is unknown but available information suggests their training compute could exceed 10 23 FLOP. This data is also downloadable in the Appendix . We selected models in our database published in 2022 or later, with unknown training compute. We excluded models with fewer than 3 billion parameters or 10 billion training data points. These thresholds were based on parameter counts and dataset sizes in models with confirmed compute. We also excluded models fine-tuned from other models. We then manually inspected candidates to rule out models likely to fall below 10 23 FLOP, for example based on training hardware or model capabilities. Although some of these models may nevertheless have been trained with less than 10 23 FLOP, this data provides more context about large-scale models, for example in Figures 3 and 6. Outcomes and limitations There are limits to our search methods, and some models within the scope of our search may not be possible to find by these or other methods. Commercial products are often proprietary and their technical details not divulged, especially in the cybersecurity and B2B service industries. The cybersecurity suite Darktrace and anti-malware software McAfee are typical examples of this, as is the custom chatbot service Dante. Sometimes, there are publicly announced AI products which may or may not have important models behind them. Microsoft Azure offers chatbots which may have been developed within Microsoft or may be using GPT-4. Salesforce\u2019s competing service EinsteinGPT is similar and may be using an OpenAI model on the backend or may involve a custom, fine-tuned model. ByteDance\u2019s Tako chatbot (used in TikTok) is almost certainly not based on GPT-4, but it is unclear how much compute or data was used when creating it. The image generation service Craiyon may involve a large model, possibly based on Stable Diffusion. The chat and writing services YouChat and rytr, on the other hand, definitively do not contain bespoke models, instead relying on the GPT API. In other cases, labs may develop large models but forgo announcing them in order to maintain secrecy. We don\u2019t know how many such models have flown under the radar. Determining the number or identity of organizations capable of creating these models would be a useful endeavor, but was beyond the scope of this process. Conclusion Large-scale AI models are of particular interest for policymakers and researchers. We have collected a dataset covering 81 large-scale models, trained with large amounts of compute. Analyzing this dataset confirms several key insights. First, language modeling is by far the most common domain. This trend seems likely to continue, with language prioritized in development of the next generation of frontier models, even as they become multimodal. Second, large-scale models are primarily developed by US companies, but a quarter come from Chinese labs, and this proportion has grown in recent years. Finally, almost half of highly large-scale models are downloadable, a trend suggesting that new cutting-edge capabilities become widely available soon after they are developed in proprietary models. Ultimately, we can never find every machine learning model in existence, so our search continues. A key challenge in this work has been the large number of models without known training compute. 9 We urge machine learning developers to report your compute so that the policy and research communities can maintain a well-informed view of the frontier of the field. To keep our database up-to-date, 15 foreign languages have been incorporated into an automated pipeline that searches 5 languages per month for AI news articles published since the previous search, to find any model within three months of its release in any language. As machine learning companies, talent, and infrastructure spread around the world, these search results will reveal more and more large models. We\u2019ll continue tracking new leading models on previously searched benchmarks, models leading on benchmarks newly exceeding 100 evaluated systems, and models with over 1 billion parameters submitted to Papers With Code and Hugging Face. Suggestions for large ML systems not featured in our database can be submitted using this form . If this data has been useful to you, we\u2019d love to hear from you at data@epoch.ai . Appendix Dataset Large-scale models : These models were trained with over 10 23 FLOP, based on our estimates of training compute. Unconfirmed and known large-scale models : This dataset includes both models that were trained with over 10 23 FLOP as well as models which have unknown training compute but may have been trained on over 10 23 FLOP. Further details are described in Unconfirmed large-scale models . Show: Confirmed large-scale models Confirmed and unconfirmed large-scale models The data can be downloaded using the dropdown menu at the top of the embedded table. We also offer a downloadable large-scale model dataset , updated daily, on our database portal and at this CSV link . Growth of the compute frontier {\"title\": \"Model count by training compute at different years\", \"xAxis\": {\"label\": \"Training compute (FLOP)\", \"lim\": [22.0, 26.0], \"scaleType\": \"linear\", \"ticks\": [22.0, 23.0, 24.0, 25.0, 26.0], \"tickLabels\": [\"$10^{22}$\", \"$10^{23}$\", \"$10^{24}$\", \"$10^{25}$\", \"$10^{26}$\"], \"nice\": false}, \"yAxis\": {\"label\": \"Number of models\", \"lim\": [-5.0, 210.0], \"scaleType\": \"linear\", \"ticks\": [0, 50, 100, 150, 200], \"tickLabels\": [\"0\", \"50\", \"100\", \"150\", \"200\"]}, \"showLegend\": true, \"legendPosition\": \"header\", \"showFrame\": true, \"addDataPadding\": false, \"objects\": [{\"type\": \"line\", \"label\": \"2020\", \"color\": \"#034752\", \"zOrder\": 2, \"strokeWidth\": 1.5, \"lineStyle\": \"-\", \"points\": [{\"x\": 0.0, \"y\": 197}, {\"x\": 0.12999999999999998, \"y\": 197}, {\"x\": 0.25999999999999995, \"y\": 197}, {\"x\": 0.39, \"y\": 197}, {\"x\": 0.52, \"y\": 197}, {\"x\": 0.65, \"y\": 197}, {\"x\": 0.78, \"y\": 197}, {\"x\": 0.91, \"y\": 197}, {\"x\": 1.04, \"y\": 197}, {\"x\": 1.17, \"y\": 197}, {\"x\": 1.3, \"y\": 197}, {\"x\": 1.4300000000000002, \"y\": 197}, {\"x\": 1.56, \"y\": 197}, {\"x\": 1.69, \"y\": 196}, {\"x\": 1.82, \"y\": 196}, {\"x\": 1.9500000000000002, \"y\": 196}, {\"x\": 2.08, \"y\": 196}, {\"x\": 2.21, \"y\": 196}, {\"x\": 2.34, \"y\": 196}, {\"x\": 2.47, \"y\": 196}, {\"x\": 2.6, \"y\": 196}, {\"x\": 2.73, \"y\": 196}, {\"x\": 2.8600000000000003, \"y\": 196}, {\"x\": 2.99, \"y\": 196}, {\"x\": 3.12, \"y\": 196}, {\"x\": 3.25, \"y\": 196}, {\"x\": 3.38, \"y\": 196}, {\"x\": 3.5100000000000002, \"y\": 196}, {\"x\": 3.64, \"y\": 196}, {\"x\": 3.77, \"y\": 196}, {\"x\": 3.9000000000000004, \"y\": 196}, {\"x\": 4.03, \"y\": 195}, {\"x\": 4.16, \"y\": 195}, {\"x\": 4.29, \"y\": 195}, {\"x\": 4.42, \"y\": 195}, {\"x\": 4.55, \"y\": 195}, {\"x\": 4.68, \"y\": 195}, {\"x\": 4.8100000000000005, \"y\": 195}, {\"x\": 4.94, \"y\": 195}, {\"x\": 5.07, \"y\": 195}, {\"x\": 5.2, \"y\": 195}, {\"x\": 5.33, \"y\": 195}, {\"x\": 5.46, \"y\": 195}, {\"x\": 5.59, \"y\": 195}, {\"x\": 5.720000000000001, \"y\": 195}, {\"x\": 5.8500000000000005, \"y\": 194}, {\"x\": 5.98, \"y\": 194}, {\"x\": 6.11, \"y\": 194}, {\"x\": 6.24, \"y\": 194}, {\"x\": 6.37, \"y\": 194}, {\"x\": 6.5, \"y\": 194}, {\"x\": 6.63, \"y\": 194}, {\"x\": 6.76, \"y\": 194}, {\"x\": 6.890000000000001, \"y\": 194}, {\"x\": 7.0200000000000005, \"y\": 194}, {\"x\": 7.15, \"y\": 194}, {\"x\": 7.28, \"y\": 194}, {\"x\": 7.41, \"y\": 194}, {\"x\": 7.54, \"y\": 194}, {\"x\": 7.67, \"y\": 194}, {\"x\": 7.800000000000001, \"y\": 194}, {\"x\": 7.930000000000001, \"y\": 194}, {\"x\": 8.06, \"y\": 194}, {\"x\": 8.19, \"y\": 192}, {\"x\": 8.32, \"y\": 192}, {\"x\": 8.450000000000001, \"y\": 191}, {\"x\": 8.58, \"y\": 191}, {\"x\": 8.71, \"y\": 190}, {\"x\": 8.84, \"y\": 189}, {\"x\": 8.97, \"y\": 188}, {\"x\": 9.1, \"y\": 188}, {\"x\": 9.23, \"y\": 187}, {\"x\": 9.36, \"y\": 187}, {\"x\": 9.49, \"y\": 187}, {\"x\": 9.620000000000001, \"y\": 187}, {\"x\": 9.75, \"y\": 187}, {\"x\": 9.88, \"y\": 187}, {\"x\": 10.01, \"y\": 187}, {\"x\": 10.14, \"y\": 186}, {\"x\": 10.27, \"y\": 186}, {\"x\": 10.4, \"y\": 186}, {\"x\": 10.530000000000001, \"y\": 184}, {\"x\": 10.66, \"y\": 183}, {\"x\": 10.790000000000001, \"y\": 183}, {\"x\": 10.92, \"y\": 182}, {\"x\": 11.05, \"y\": 182}, {\"x\": 11.18, \"y\": 182}, {\"x\": 11.31, \"y\": 182}, {\"x\": 11.440000000000001, \"y\": 181}, {\"x\": 11.57, \"y\": 181}, {\"x\": 11.700000000000001, \"y\": 181}, {\"x\": 11.83, \"y\": 181}, {\"x\": 11.96, \"y\": 181}, {\"x\": 12.09, \"y\": 181}, {\"x\": 12.22, \"y\": 181}, {\"x\": 12.35, \"y\": 181}, {\"x\": 12.48, \"y\": 180}, {\"x\": 12.610000000000001, \"y\": 180}, {\"x\": 12.74, \"y\": 180}, {\"x\": 12.870000000000001, \"y\": 180}, {\"x\": 13.0, \"y\": 179}, {\"x\": 13.13, \"y\": 178}, {\"x\": 13.26, \"y\": 178}, {\"x\": 13.39, \"y\": 175}, {\"x\": 13.52, \"y\": 175}, {\"x\": 13.65, \"y\": 175}, {\"x\": 13.780000000000001, \"y\": 175}, {\"x\": 13.91, \"y\": 174}, {\"x\": 14.040000000000001, \"y\": 173}, {\"x\": 14.17, \"y\": 172}, {\"x\": 14.3, \"y\": 172}, {\"x\": 14.43, \"y\": 171}, {\"x\": 14.56, \"y\": 170}, {\"x\": 14.690000000000001, \"y\": 169}, {\"x\": 14.82, \"y\": 168}, {\"x\": 14.950000000000001, \"y\": 168}, {\"x\": 15.08, \"y\": 166}, {\"x\": 15.21, \"y\": 164}, {\"x\": 15.34, \"y\": 162}, {\"x\": 15.47, \"y\": 160}, {\"x\": 15.600000000000001, \"y\": 153}, {\"x\": 15.73, \"y\": 151}, {\"x\": 15.860000000000001, \"y\": 149}, {\"x\": 15.99, \"y\": 147}, {\"x\": 16.12, \"y\": 143}, {\"x\": 16.25, \"y\": 142}, {\"x\": 16.38, \"y\": 139}, {\"x\": 16.51, \"y\": 137}, {\"x\": 16.64, \"y\": 135}, {\"x\": 16.77, \"y\": 134}, {\"x\": 16.900000000000002, \"y\": 126}, {\"x\": 17.03, \"y\": 121}, {\"x\": 17.16, \"y\": 118}, {\"x\": 17.29, \"y\": 117}, {\"x\": 17.42, \"y\": 116}, {\"x\": 17.55, \"y\": 113}, {\"x\": 17.68, \"y\": 106}, {\"x\": 17.810000000000002, \"y\": 103}, {\"x\": 17.94, \"y\": 100}, {\"x\": 18.07, \"y\": 98}, {\"x\": 18.2, \"y\": 93}, {\"x\": 18.330000000000002, \"y\": 92}, {\"x\": 18.46, \"y\": 88}, {\"x\": 18.59, \"y\": 86}, {\"x\": 18.72, \"y\": 81}, {\"x\": 18.85, \"y\": 80}, {\"x\": 18.98, \"y\": 74}, {\"x\": 19.11, \"y\": 70}, {\"x\": 19.240000000000002, \"y\": 69}, {\"x\": 19.37, \"y\": 65}, {\"x\": 19.5, \"y\": 60}, {\"x\": 19.63, \"y\": 56}, {\"x\": 19.76, \"y\": 51}, {\"x\": 19.89, \"y\": 47}, {\"x\": 20.02, \"y\": 44}, {\"x\": 20.150000000000002, \"y\": 43}, {\"x\": 20.28, \"y\": 39}, {\"x\": 20.41, \"y\": 39}, {\"x\": 20.54, \"y\": 37}, {\"x\": 20.67, \"y\": 33}, {\"x\": 20.8, \"y\": 30}, {\"x\": 20.93, \"y\": 26}, {\"x\": 21.060000000000002, \"y\": 24}, {\"x\": 21.19, \"y\": 22}, {\"x\": 21.32, \"y\": 21}, {\"x\": 21.45, \"y\": 19}, {\"x\": 21.580000000000002, \"y\": 17}, {\"x\": 21.71, \"y\": 15}, {\"x\": 21.84, \"y\": 13}, {\"x\": 21.970000000000002, \"y\": 10}, {\"x\": 22.1, \"y\": 10}, {\"x\": 22.23, \"y\": 8}, {\"x\": 22.36, \"y\": 8}, {\"x\": 22.490000000000002, \"y\": 8}, {\"x\": 22.62, \"y\": 6}, {\"x\": 22.75, \"y\": 6}, {\"x\": 22.880000000000003, \"y\": 3}, {\"x\": 23.01, \"y\": 3}, {\"x\": 23.14, \"y\": 2}, {\"x\": 23.27, \"y\": 1}, {\"x\": 23.400000000000002, \"y\": 1}, {\"x\": 23.53, \"y\": 1}]}, {\"type\": \"line\", \"label\": \"2021\", \"color\": \"#02767c\", \"zOrder\": 2, \"strokeWidth\": 1.5, \"lineStyle\": \"-\", \"points\": [{\"x\": 0.0, \"y\": 261}, {\"x\": 0.12999999999999998, \"y\": 261}, {\"x\": 0.25999999999999995, \"y\": 261}, {\"x\": 0.39, \"y\": 261}, {\"x\": 0.52, \"y\": 261}, {\"x\": 0.65, \"y\": 261}, {\"x\": 0.78, \"y\": 261}, {\"x\": 0.91, \"y\": 261}, {\"x\": 1.04, \"y\": 261}, {\"x\": 1.17, \"y\": 261}, {\"x\": 1.3, \"y\": 261}, {\"x\": 1.4300000000000002, \"y\": 261}, {\"x\": 1.56, \"y\": 261}, {\"x\": 1.69, \"y\": 260}, {\"x\": 1.82, \"y\": 260}, {\"x\": 1.9500000000000002, \"y\": 260}, {\"x\": 2.08, \"y\": 260}, {\"x\": 2.21, \"y\": 260}, {\"x\": 2.34, \"y\": 260}, {\"x\": 2.47, \"y\": 260}, {\"x\": 2.6, \"y\": 260}, {\"x\": 2.73, \"y\": 260}, {\"x\": 2.8600000000000003, \"y\": 260}, {\"x\": 2.99, \"y\": 260}, {\"x\": 3.12, \"y\": 260}, {\"x\": 3.25, \"y\": 260}, {\"x\": 3.38, \"y\": 260}, {\"x\": 3.5100000000000002, \"y\": 260}, {\"x\": 3.64, \"y\": 260}, {\"x\": 3.77, \"y\": 260}, {\"x\": 3.9000000000000004, \"y\": 260}, {\"x\": 4.03, \"y\": 259}, {\"x\": 4.16, \"y\": 259}, {\"x\": 4.29, \"y\": 259}, {\"x\": 4.42, \"y\": 259}, {\"x\": 4.55, \"y\": 259}, {\"x\": 4.68, \"y\": 259}, {\"x\": 4.8100000000000005, \"y\": 259}, {\"x\": 4.94, \"y\": 259}, {\"x\": 5.07, \"y\": 259}, {\"x\": 5.2, \"y\": 259}, {\"x\": 5.33, \"y\": 259}, {\"x\": 5.46, \"y\": 259}, {\"x\": 5.59, \"y\": 259}, {\"x\": 5.720000000000001, \"y\": 259}, {\"x\": 5.8500000000000005, \"y\": 258}, {\"x\": 5.98, \"y\": 258}, {\"x\": 6.11, \"y\": 258}, {\"x\": 6.24, \"y\": 258}, {\"x\": 6.37, \"y\": 258}, {\"x\": 6.5, \"y\": 258}, {\"x\": 6.63, \"y\": 258}, {\"x\": 6.76, \"y\": 258}, {\"x\": 6.890000000000001, \"y\": 258}, {\"x\": 7.0200000000000005, \"y\": 258}, {\"x\": 7.15, \"y\": 258}, {\"x\": 7.28, \"y\": 258}, {\"x\": 7.41, \"y\": 258}, {\"x\": 7.54, \"y\": 258}, {\"x\": 7.67, \"y\": 258}, {\"x\": 7.800000000000001, \"y\": 258}, {\"x\": 7.930000000000001, \"y\": 258}, {\"x\": 8.06, \"y\": 258}, {\"x\": 8.19, \"y\": 256}, {\"x\": 8.32, \"y\": 256}, {\"x\": 8.450000000000001, \"y\": 255}, {\"x\": 8.58, \"y\": 255}, {\"x\": 8.71, \"y\": 254}, {\"x\": 8.84, \"y\": 253}, {\"x\": 8.97, \"y\": 252}, {\"x\": 9.1, \"y\": 252}, {\"x\": 9.23, \"y\": 251}, {\"x\": 9.36, \"y\": 251}, {\"x\": 9.49, \"y\": 251}, {\"x\": 9.620000000000001, \"y\": 251}, {\"x\": 9.75, \"y\": 251}, {\"x\": 9.88, \"y\": 251}, {\"x\": 10.01, \"y\": 251}, {\"x\": 10.14, \"y\": 250}, {\"x\": 10.27, \"y\": 250}, {\"x\": 10.4, \"y\": 250}, {\"x\": 10.530000000000001, \"y\": 248}, {\"x\": 10.66, \"y\": 247}, {\"x\": 10.790000000000001, \"y\": 247}, {\"x\": 10.92, \"y\": 246}, {\"x\": 11.05, \"y\": 246}, {\"x\": 11.18, \"y\": 246}, {\"x\": 11.31, \"y\": 246}, {\"x\": 11.440000000000001, \"y\": 245}, {\"x\": 11.57, \"y\": 245}, {\"x\": 11.700000000000001, \"y\": 245}, {\"x\": 11.83, \"y\": 245}, {\"x\": 11.96, \"y\": 245}, {\"x\": 12.09, \"y\": 245}, {\"x\": 12.22, \"y\": 245}, {\"x\": 12.35, \"y\": 245}, {\"x\": 12.48, \"y\": 244}, {\"x\": 12.610000000000001, \"y\": 244}, {\"x\": 12.74, \"y\": 244}, {\"x\": 12.870000000000001, \"y\": 244}, {\"x\": 13.0, \"y\": 243}, {\"x\": 13.13, \"y\": 242}, {\"x\": 13.26, \"y\": 242}, {\"x\": 13.39, \"y\": 239}, {\"x\": 13.52, \"y\": 239}, {\"x\": 13.65, \"y\": 239}, {\"x\": 13.780000000000001, \"y\": 239}, {\"x\": 13.91, \"y\": 238}, {\"x\": 14.040000000000001, \"y\": 237}, {\"x\": 14.17, \"y\": 236}, {\"x\": 14.3, \"y\": 236}, {\"x\": 14.43, \"y\": 235}, {\"x\": 14.56, \"y\": 233}, {\"x\": 14.690000000000001, \"y\": 232}, {\"x\": 14.82, \"y\": 231}, {\"x\": 14.950000000000001, \"y\": 231}, {\"x\": 15.08, \"y\": 229}, {\"x\": 15.21, \"y\": 227}, {\"x\": 15.34, \"y\": 225}, {\"x\": 15.47, \"y\": 223}, {\"x\": 15.600000000000001, \"y\": 216}, {\"x\": 15.73, \"y\": 214}, {\"x\": 15.860000000000001, \"y\": 212}, {\"x\": 15.99, \"y\": 210}, {\"x\": 16.12, \"y\": 206}, {\"x\": 16.25, \"y\": 203}, {\"x\": 16.38, \"y\": 200}, {\"x\": 16.51, \"y\": 198}, {\"x\": 16.64, \"y\": 196}, {\"x\": 16.77, \"y\": 195}, {\"x\": 16.900000000000002, \"y\": 187}, {\"x\": 17.03, \"y\": 181}, {\"x\": 17.16, \"y\": 177}, {\"x\": 17.29, \"y\": 176}, {\"x\": 17.42, \"y\": 175}, {\"x\": 17.55, \"y\": 172}, {\"x\": 17.68, \"y\": 165}, {\"x\": 17.810000000000002, \"y\": 160}, {\"x\": 17.94, \"y\": 156}, {\"x\": 18.07, \"y\": 154}, {\"x\": 18.2, \"y\": 148}, {\"x\": 18.330000000000002, \"y\": 146}, {\"x\": 18.46, \"y\": 142}, {\"x\": 18.59, \"y\": 137}, {\"x\": 18.72, \"y\": 131}, {\"x\": 18.85, \"y\": 129}, {\"x\": 18.98, \"y\": 122}, {\"x\": 19.11, \"y\": 115}, {\"x\": 19.240000000000002, \"y\": 114}, {\"x\": 19.37, \"y\": 110}, {\"x\": 19.5, \"y\": 101}, {\"x\": 19.63, \"y\": 96}, {\"x\": 19.76, \"y\": 90}, {\"x\": 19.89, \"y\": 85}, {\"x\": 20.02, \"y\": 82}, {\"x\": 20.150000000000002, \"y\": 77}, {\"x\": 20.28, \"y\": 72}, {\"x\": 20.41, \"y\": 72}, {\"x\": 20.54, \"y\": 69}, {\"x\": 20.67, \"y\": 62}, {\"x\": 20.8, \"y\": 58}, {\"x\": 20.93, \"y\": 53}, {\"x\": 21.060000000000002, \"y\": 51}, {\"x\": 21.19, \"y\": 48}, {\"x\": 21.32, \"y\": 43}, {\"x\": 21.45, \"y\": 40}, {\"x\": 21.580000000000002, \"y\": 36}, {\"x\": 21.71, \"y\": 33}, {\"x\": 21.84, \"y\": 29}, {\"x\": 21.970000000000002, \"y\": 22}, {\"x\": 22.1, \"y\": 20}, {\"x\": 22.23, \"y\": 16}, {\"x\": 22.36, \"y\": 15}, {\"x\": 22.490000000000002, \"y\": 15}, {\"x\": 22.62, \"y\": 11}, {\"x\": 22.75, \"y\": 9}, {\"x\": 22.880000000000003, \"y\": 6}, {\"x\": 23.01, \"y\": 4}, {\"x\": 23.14, \"y\": 3}, {\"x\": 23.27, \"y\": 2}, {\"x\": 23.400000000000002, \"y\": 2}, {\"x\": 23.53, \"y\": 1}]}, {\"type\": \"line\", \"label\": \"2022\", \"color\": \"#00a5a6\", \"zOrder\": 2, \"strokeWidth\": 1.5, \"lineStyle\": \"-\", \"points\": [{\"x\": 0.0, \"y\": 358}, {\"x\": 0.12999999999999998, \"y\": 358}, {\"x\": 0.25999999999999995, \"y\": 358}, {\"x\": 0.39, \"y\": 358}, {\"x\": 0.52, \"y\": 358}, {\"x\": 0.65, \"y\": 358}, {\"x\": 0.78, \"y\": 358}, {\"x\": 0.91, \"y\": 358}, {\"x\": 1.04, \"y\": 358}, {\"x\": 1.17, \"y\": 358}, {\"x\": 1.3, \"y\": 358}, {\"x\": 1.4300000000000002, \"y\": 358}, {\"x\": 1.56, \"y\": 358}, {\"x\": 1.69, \"y\": 357}, {\"x\": 1.82, \"y\": 357}, {\"x\": 1.9500000000000002, \"y\": 357}, {\"x\": 2.08, \"y\": 357}, {\"x\": 2.21, \"y\": 357}, {\"x\": 2.34, \"y\": 357}, {\"x\": 2.47, \"y\": 357}, {\"x\": 2.6, \"y\": 357}, {\"x\": 2.73, \"y\": 357}, {\"x\": 2.8600000000000003, \"y\": 357}, {\"x\": 2.99, \"y\": 357}, {\"x\": 3.12, \"y\": 357}, {\"x\": 3.25, \"y\": 357}, {\"x\": 3.38, \"y\": 357}, {\"x\": 3.5100000000000002, \"y\": 357}, {\"x\": 3.64, \"y\": 357}, {\"x\": 3.77, \"y\": 357}, {\"x\": 3.9000000000000004, \"y\": 357}, {\"x\": 4.03, \"y\": 356}, {\"x\": 4.16, \"y\": 356}, {\"x\": 4.29, \"y\": 356}, {\"x\": 4.42, \"y\": 356}, {\"x\": 4.55, \"y\": 356}, {\"x\": 4.68, \"y\": 356}, {\"x\": 4.8100000000000005, \"y\": 356}, {\"x\": 4.94, \"y\": 356}, {\"x\": 5.07, \"y\": 356}, {\"x\": 5.2, \"y\": 356}, {\"x\": 5.33, \"y\": 356}, {\"x\": 5.46, \"y\": 356}, {\"x\": 5.59, \"y\": 356}, {\"x\": 5.720000000000001, \"y\": 356}, {\"x\": 5.8500000000000005, \"y\": 355}, {\"x\": 5.98, \"y\": 355}, {\"x\": 6.11, \"y\": 355}, {\"x\": 6.24, \"y\": 355}, {\"x\": 6.37, \"y\": 355}, {\"x\": 6.5, \"y\": 355}, {\"x\": 6.63, \"y\": 355}, {\"x\": 6.76, \"y\": 355}, {\"x\": 6.890000000000001, \"y\": 355}, {\"x\": 7.0200000000000005, \"y\": 355}, {\"x\": 7.15, \"y\": 355}, {\"x\": 7.28, \"y\": 355}, {\"x\": 7.41, \"y\": 355}, {\"x\": 7.54, \"y\": 355}, {\"x\": 7.67, \"y\": 355}, {\"x\": 7.800000000000001, \"y\": 355}, {\"x\": 7.930000000000001, \"y\": 355}, {\"x\": 8.06, \"y\": 355}, {\"x\": 8.19, \"y\": 353}, {\"x\": 8.32, \"y\": 353}, {\"x\": 8.450000000000001, \"y\": 352}, {\"x\": 8.58, \"y\": 352}, {\"x\": 8.71, \"y\": 351}, {\"x\": 8.84, \"y\": 350}, {\"x\": 8.97, \"y\": 349}, {\"x\": 9.1, \"y\": 349}, {\"x\": 9.23, \"y\": 348}, {\"x\": 9.36, \"y\": 348}, {\"x\": 9.49, \"y\": 348}, {\"x\": 9.620000000000001, \"y\": 348}, {\"x\": 9.75, \"y\": 348}, {\"x\": 9.88, \"y\": 348}, {\"x\": 10.01, \"y\": 348}, {\"x\": 10.14, \"y\": 347}, {\"x\": 10.27, \"y\": 347}, {\"x\": 10.4, \"y\": 347}, {\"x\": 10.530000000000001, \"y\": 345}, {\"x\": 10.66, \"y\": 344}, {\"x\": 10.790000000000001, \"y\": 344}, {\"x\": 10.92, \"y\": 343}, {\"x\": 11.05, \"y\": 343}, {\"x\": 11.18, \"y\": 343}, {\"x\": 11.31, \"y\": 343}, {\"x\": 11.440000000000001, \"y\": 342}, {\"x\": 11.57, \"y\": 342}, {\"x\": 11.700000000000001, \"y\": 342}, {\"x\": 11.83, \"y\": 342}, {\"x\": 11.96, \"y\": 342}, {\"x\": 12.09, \"y\": 342}, {\"x\": 12.22, \"y\": 342}, {\"x\": 12.35, \"y\": 342}, {\"x\": 12.48, \"y\": 341}, {\"x\": 12.610000000000001, \"y\": 341}, {\"x\": 12.74, \"y\": 341}, {\"x\": 12.870000000000001, \"y\": 341}, {\"x\": 13.0, \"y\": 340}, {\"x\": 13.13, \"y\": 339}, {\"x\": 13.26, \"y\": 339}, {\"x\": 13.39, \"y\": 336}, {\"x\": 13.52, \"y\": 336}, {\"x\": 13.65, \"y\": 336}, {\"x\": 13.780000000000001, \"y\": 336}, {\"x\": 13.91, \"y\": 335}, {\"x\": 14.040000000000001, \"y\": 334}, {\"x\": 14.17, \"y\": 333}, {\"x\": 14.3, \"y\": 333}, {\"x\": 14.43, \"y\": 331}, {\"x\": 14.56, \"y\": 329}, {\"x\": 14.690000000000001, \"y\": 328}, {\"x\": 14.82, \"y\": 327}, {\"x\": 14.950000000000001, \"y\": 327}, {\"x\": 15.08, \"y\": 325}, {\"x\": 15.21, \"y\": 323}, {\"x\": 15.34, \"y\": 321}, {\"x\": 15.47, \"y\": 319}, {\"x\": 15.600000000000001, \"y\": 311}, {\"x\": 15.73, \"y\": 309}, {\"x\": 15.860000000000001, \"y\": 307}, {\"x\": 15.99, \"y\": 305}, {\"x\": 16.12, \"y\": 301}, {\"x\": 16.25, \"y\": 298}, {\"x\": 16.38, \"y\": 293}, {\"x\": 16.51, \"y\": 290}, {\"x\": 16.64, \"y\": 288}, {\"x\": 16.77, \"y\": 287}, {\"x\": 16.900000000000002, \"y\": 279}, {\"x\": 17.03, \"y\": 273}, {\"x\": 17.16, \"y\": 269}, {\"x\": 17.29, \"y\": 268}, {\"x\": 17.42, \"y\": 267}, {\"x\": 17.55, \"y\": 264}, {\"x\": 17.68, \"y\": 255}, {\"x\": 17.810000000000002, \"y\": 250}, {\"x\": 17.94, \"y\": 246}, {\"x\": 18.07, \"y\": 243}, {\"x\": 18.2, \"y\": 236}, {\"x\": 18.330000000000002, \"y\": 234}, {\"x\": 18.46, \"y\": 230}, {\"x\": 18.59, \"y\": 221}, {\"x\": 18.72, \"y\": 215}, {\"x\": 18.85, \"y\": 211}, {\"x\": 18.98, \"y\": 200}, {\"x\": 19.11, \"y\": 190}, {\"x\": 19.240000000000002, \"y\": 189}, {\"x\": 19.37, \"y\": 184}, {\"x\": 19.5, \"y\": 172}, {\"x\": 19.63, \"y\": 167}, {\"x\": 19.76, \"y\": 161}, {\"x\": 19.89, \"y\": 154}, {\"x\": 20.02, \"y\": 149}, {\"x\": 20.150000000000002, \"y\": 141}, {\"x\": 20.28, \"y\": 135}, {\"x\": 20.41, \"y\": 134}, {\"x\": 20.54, \"y\": 128}, {\"x\": 20.67, \"y\": 120}, {\"x\": 20.8, \"y\": 113}, {\"x\": 20.93, \"y\": 105}, {\"x\": 21.060000000000002, \"y\": 100}, {\"x\": 21.19, \"y\": 96}, {\"x\": 21.32, \"y\": 89}, {\"x\": 21.45, \"y\": 86}, {\"x\": 21.580000000000002, \"y\": 79}, {\"x\": 21.71, \"y\": 73}, {\"x\": 21.84, \"y\": 66}, {\"x\": 21.970000000000002, \"y\": 58}, {\"x\": 22.1, \"y\": 55}, {\"x\": 22.23, \"y\": 47}, {\"x\": 22.36, \"y\": 42}, {\"x\": 22.490000000000002, \"y\": 40}, {\"x\": 22.62, \"y\": 31}, {\"x\": 22.75, \"y\": 25}, {\"x\": 22.880000000000003, \"y\": 20}, {\"x\": 23.01, \"y\": 15}, {\"x\": 23.14, \"y\": 14}, {\"x\": 23.27, \"y\": 11}, {\"x\": 23.400000000000002, \"y\": 11}, {\"x\": 23.53, \"y\": 9}, {\"x\": 23.66, \"y\": 4}, {\"x\": 23.79, \"y\": 3}, {\"x\": 23.92, \"y\": 2}, {\"x\": 24.05, \"y\": 1}]}, {\"type\": \"line\", \"label\": \"2023\", \"color\": \"#11df8c\", \"zOrder\": 2, \"strokeWidth\": 1.5, \"lineStyle\": \"-\", \"points\": [{\"x\": 0.0, \"y\": 457}, {\"x\": 0.12999999999999998, \"y\": 457}, {\"x\": 0.25999999999999995, \"y\": 457}, {\"x\": 0.39, \"y\": 457}, {\"x\": 0.52, \"y\": 457}, {\"x\": 0.65, \"y\": 457}, {\"x\": 0.78, \"y\": 457}, {\"x\": 0.91, \"y\": 457}, {\"x\": 1.04, \"y\": 457}, {\"x\": 1.17, \"y\": 457}, {\"x\": 1.3, \"y\": 457}, {\"x\": 1.4300000000000002, \"y\": 457}, {\"x\": 1.56, \"y\": 457}, {\"x\": 1.69, \"y\": 456}, {\"x\": 1.82, \"y\": 456}, {\"x\": 1.9500000000000002, \"y\": 456}, {\"x\": 2.08, \"y\": 456}, {\"x\": 2.21, \"y\": 456}, {\"x\": 2.34, \"y\": 456}, {\"x\": 2.47, \"y\": 456}, {\"x\": 2.6, \"y\": 456}, {\"x\": 2.73, \"y\": 456}, {\"x\": 2.8600000000000003, \"y\": 456}, {\"x\": 2.99, \"y\": 456}, {\"x\": 3.12, \"y\": 456}, {\"x\": 3.25, \"y\": 456}, {\"x\": 3.38, \"y\": 456}, {\"x\": 3.5100000000000002, \"y\": 456}, {\"x\": 3.64, \"y\": 456}, {\"x\": 3.77, \"y\": 456}, {\"x\": 3.9000000000000004, \"y\": 456}, {\"x\": 4.03, \"y\": 455}, {\"x\": 4.16, \"y\": 455}, {\"x\": 4.29, \"y\": 455}, {\"x\": 4.42, \"y\": 455}, {\"x\": 4.55, \"y\": 455}, {\"x\": 4.68, \"y\": 455}, {\"x\": 4.8100000000000005, \"y\": 455}, {\"x\": 4.94, \"y\": 455}, {\"x\": 5.07, \"y\": 455}, {\"x\": 5.2, \"y\": 455}, {\"x\": 5.33, \"y\": 455}, {\"x\": 5.46, \"y\": 455}, {\"x\": 5.59, \"y\": 455}, {\"x\": 5.720000000000001, \"y\": 455}, {\"x\": 5.8500000000000005, \"y\": 454}, {\"x\": 5.98, \"y\": 454}, {\"x\": 6.11, \"y\": 454}, {\"x\": 6.24, \"y\": 454}, {\"x\": 6.37, \"y\": 454}, {\"x\": 6.5, \"y\": 454}, {\"x\": 6.63, \"y\": 454}, {\"x\": 6.76, \"y\": 454}, {\"x\": 6.890000000000001, \"y\": 454}, {\"x\": 7.0200000000000005, \"y\": 454}, {\"x\": 7.15, \"y\": 454}, {\"x\": 7.28, \"y\": 454}, {\"x\": 7.41, \"y\": 454}, {\"x\": 7.54, \"y\": 454}, {\"x\": 7.67, \"y\": 454}, {\"x\": 7.800000000000001, \"y\": 454}, {\"x\": 7.930000000000001, \"y\": 454}, {\"x\": 8.06, \"y\": 454}, {\"x\": 8.19, \"y\": 452}, {\"x\": 8.32, \"y\": 452}, {\"x\": 8.450000000000001, \"y\": 451}, {\"x\": 8.58, \"y\": 451}, {\"x\": 8.71, \"y\": 450}, {\"x\": 8.84, \"y\": 449}, {\"x\": 8.97, \"y\": 448}, {\"x\": 9.1, \"y\": 448}, {\"x\": 9.23, \"y\": 447}, {\"x\": 9.36, \"y\": 447}, {\"x\": 9.49, \"y\": 447}, {\"x\": 9.620000000000001, \"y\": 447}, {\"x\": 9.75, \"y\": 447}, {\"x\": 9.88, \"y\": 447}, {\"x\": 10.01, \"y\": 447}, {\"x\": 10.14, \"y\": 446}, {\"x\": 10.27, \"y\": 446}, {\"x\": 10.4, \"y\": 446}, {\"x\": 10.530000000000001, \"y\": 444}, {\"x\": 10.66, \"y\": 443}, {\"x\": 10.790000000000001, \"y\": 443}, {\"x\": 10.92, \"y\": 442}, {\"x\": 11.05, \"y\": 442}, {\"x\": 11.18, \"y\": 442}, {\"x\": 11.31, \"y\": 442}, {\"x\": 11.440000000000001, \"y\": 441}, {\"x\": 11.57, \"y\": 441}, {\"x\": 11.700000000000001, \"y\": 441}, {\"x\": 11.83, \"y\": 441}, {\"x\": 11.96, \"y\": 441}, {\"x\": 12.09, \"y\": 441}, {\"x\": 12.22, \"y\": 441}, {\"x\": 12.35, \"y\": 441}, {\"x\": 12.48, \"y\": 440}, {\"x\": 12.610000000000001, \"y\": 440}, {\"x\": 12.74, \"y\": 440}, {\"x\": 12.870000000000001, \"y\": 440}, {\"x\": 13.0, \"y\": 439}, {\"x\": 13.13, \"y\": 438}, {\"x\": 13.26, \"y\": 438}, {\"x\": 13.39, \"y\": 435}, {\"x\": 13.52, \"y\": 435}, {\"x\": 13.65, \"y\": 435}, {\"x\": 13.780000000000001, \"y\": 435}, {\"x\": 13.91, \"y\": 434}, {\"x\": 14.040000000000001, \"y\": 433}, {\"x\": 14.17, \"y\": 432}, {\"x\": 14.3, \"y\": 432}, {\"x\": 14.43, \"y\": 430}, {\"x\": 14.56, \"y\": 428}, {\"x\": 14.690000000000001, \"y\": 427}, {\"x\": 14.82, \"y\": 426}, {\"x\": 14.950000000000001, \"y\": 426}, {\"x\": 15.08, \"y\": 424}, {\"x\": 15.21, \"y\": 422}, {\"x\": 15.34, \"y\": 420}, {\"x\": 15.47, \"y\": 418}, {\"x\": 15.600000000000001, \"y\": 410}, {\"x\": 15.73, \"y\": 408}, {\"x\": 15.860000000000001, \"y\": 406}, {\"x\": 15.99, \"y\": 404}, {\"x\": 16.12, \"y\": 400}, {\"x\": 16.25, \"y\": 397}, {\"x\": 16.38, \"y\": 392}, {\"x\": 16.51, \"y\": 389}, {\"x\": 16.64, \"y\": 387}, {\"x\": 16.77, \"y\": 386}, {\"x\": 16.900000000000002, \"y\": 378}, {\"x\": 17.03, \"y\": 372}, {\"x\": 17.16, \"y\": 367}, {\"x\": 17.29, \"y\": 366}, {\"x\": 17.42, \"y\": 365}, {\"x\": 17.55, \"y\": 361}, {\"x\": 17.68, \"y\": 351}, {\"x\": 17.810000000000002, \"y\": 346}, {\"x\": 17.94, \"y\": 342}, {\"x\": 18.07, \"y\": 339}, {\"x\": 18.2, \"y\": 332}, {\"x\": 18.330000000000002, \"y\": 330}, {\"x\": 18.46, \"y\": 325}, {\"x\": 18.59, \"y\": 315}, {\"x\": 18.72, \"y\": 309}, {\"x\": 18.85, \"y\": 304}, {\"x\": 18.98, \"y\": 292}, {\"x\": 19.11, \"y\": 280}, {\"x\": 19.240000000000002, \"y\": 279}, {\"x\": 19.37, \"y\": 274}, {\"x\": 19.5, \"y\": 259}, {\"x\": 19.63, \"y\": 254}, {\"x\": 19.76, \"y\": 247}, {\"x\": 19.89, \"y\": 237}, {\"x\": 20.02, \"y\": 230}, {\"x\": 20.150000000000002, \"y\": 219}, {\"x\": 20.28, \"y\": 212}, {\"x\": 20.41, \"y\": 208}, {\"x\": 20.54, \"y\": 202}, {\"x\": 20.67, \"y\": 192}, {\"x\": 20.8, \"y\": 183}, {\"x\": 20.93, \"y\": 174}, {\"x\": 21.060000000000002, \"y\": 168}, {\"x\": 21.19, \"y\": 160}, {\"x\": 21.32, \"y\": 153}, {\"x\": 21.45, \"y\": 146}, {\"x\": 21.580000000000002, \"y\": 136}, {\"x\": 21.71, \"y\": 128}, {\"x\": 21.84, \"y\": 119}, {\"x\": 21.970000000000002, \"y\": 108}, {\"x\": 22.1, \"y\": 102}, {\"x\": 22.23, \"y\": 90}, {\"x\": 22.36, \"y\": 83}, {\"x\": 22.490000000000002, \"y\": 79}, {\"x\": 22.62, \"y\": 69}, {\"x\": 22.75, \"y\": 57}, {\"x\": 22.880000000000003, \"y\": 49}, {\"x\": 23.01, \"y\": 42}, {\"x\": 23.14, \"y\": 38}, {\"x\": 23.27, \"y\": 34}, {\"x\": 23.400000000000002, \"y\": 29}, {\"x\": 23.53, \"y\": 23}, {\"x\": 23.66, \"y\": 13}, {\"x\": 23.79, \"y\": 8}, {\"x\": 23.92, \"y\": 7}, {\"x\": 24.05, \"y\": 6}, {\"x\": 24.18, \"y\": 5}, {\"x\": 24.310000000000002, \"y\": 5}, {\"x\": 24.44, \"y\": 1}, {\"x\": 24.57, \"y\": 1}, {\"x\": 24.7, \"y\": 1}, {\"x\": 24.830000000000002, \"y\": 1}, {\"x\": 24.96, \"y\": 1}, {\"x\": 25.09, \"y\": 1}, {\"x\": 25.220000000000002, \"y\": 1}]}, {\"type\": \"line\", \"label\": \"2024\", \"color\": \"#93e75e\", \"zOrder\": 2, \"strokeWidth\": 1.5, \"lineStyle\": \"-\", \"points\": [{\"x\": 0.0, \"y\": 570}, {\"x\": 0.12999999999999998, \"y\": 570}, {\"x\": 0.25999999999999995, \"y\": 570}, {\"x\": 0.39, \"y\": 570}, {\"x\": 0.52, \"y\": 570}, {\"x\": 0.65, \"y\": 570}, {\"x\": 0.78, \"y\": 570}, {\"x\": 0.91, \"y\": 570}, {\"x\": 1.04, \"y\": 570}, {\"x\": 1.17, \"y\": 570}, {\"x\": 1.3, \"y\": 570}, {\"x\": 1.4300000000000002, \"y\": 570}, {\"x\": 1.56, \"y\": 570}, {\"x\": 1.69, \"y\": 569}, {\"x\": 1.82, \"y\": 569}, {\"x\": 1.9500000000000002, \"y\": 569}, {\"x\": 2.08, \"y\": 569}, {\"x\": 2.21, \"y\": 569}, {\"x\": 2.34, \"y\": 569}, {\"x\": 2.47, \"y\": 569}, {\"x\": 2.6, \"y\": 569}, {\"x\": 2.73, \"y\": 569}, {\"x\": 2.8600000000000003, \"y\": 569}, {\"x\": 2.99, \"y\": 569}, {\"x\": 3.12, \"y\": 569}, {\"x\": 3.25, \"y\": 569}, {\"x\": 3.38, \"y\": 569}, {\"x\": 3.5100000000000002, \"y\": 569}, {\"x\": 3.64, \"y\": 569}, {\"x\": 3.77, \"y\": 569}, {\"x\": 3.9000000000000004, \"y\": 569}, {\"x\": 4.03, \"y\": 568}, {\"x\": 4.16, \"y\": 568}, {\"x\": 4.29, \"y\": 568}, {\"x\": 4.42, \"y\": 568}, {\"x\": 4.55, \"y\": 568}, {\"x\": 4.68, \"y\": 568}, {\"x\": 4.8100000000000005, \"y\": 568}, {\"x\": 4.94, \"y\": 568}, {\"x\": 5.07, \"y\": 568}, {\"x\": 5.2, \"y\": 568}, {\"x\": 5.33, \"y\": 568}, {\"x\": 5.46, \"y\": 568}, {\"x\": 5.59, \"y\": 568}, {\"x\": 5.720000000000001, \"y\": 568}, {\"x\": 5.8500000000000005, \"y\": 567}, {\"x\": 5.98, \"y\": 567}, {\"x\": 6.11, \"y\": 567}, {\"x\": 6.24, \"y\": 567}, {\"x\": 6.37, \"y\": 567}, {\"x\": 6.5, \"y\": 567}, {\"x\": 6.63, \"y\": 567}, {\"x\": 6.76, \"y\": 567}, {\"x\": 6.890000000000001, \"y\": 567}, {\"x\": 7.0200000000000005, \"y\": 567}, {\"x\": 7.15, \"y\": 567}, {\"x\": 7.28, \"y\": 567}, {\"x\": 7.41, \"y\": 567}, {\"x\": 7.54, \"y\": 567}, {\"x\": 7.67, \"y\": 567}, {\"x\": 7.800000000000001, \"y\": 567}, {\"x\": 7.930000000000001, \"y\": 567}, {\"x\": 8.06, \"y\": 567}, {\"x\": 8.19, \"y\": 565}, {\"x\": 8.32, \"y\": 565}, {\"x\": 8.450000000000001, \"y\": 564}, {\"x\": 8.58, \"y\": 564}, {\"x\": 8.71, \"y\": 563}, {\"x\": 8.84, \"y\": 562}, {\"x\": 8.97, \"y\": 561}, {\"x\": 9.1, \"y\": 561}, {\"x\": 9.23, \"y\": 560}, {\"x\": 9.36, \"y\": 560}, {\"x\": 9.49, \"y\": 560}, {\"x\": 9.620000000000001, \"y\": 560}, {\"x\": 9.75, \"y\": 560}, {\"x\": 9.88, \"y\": 560}, {\"x\": 10.01, \"y\": 560}, {\"x\": 10.14, \"y\": 559}, {\"x\": 10.27, \"y\": 559}, {\"x\": 10.4, \"y\": 559}, {\"x\": 10.530000000000001, \"y\": 557}, {\"x\": 10.66, \"y\": 556}, {\"x\": 10.790000000000001, \"y\": 556}, {\"x\": 10.92, \"y\": 555}, {\"x\": 11.05, \"y\": 555}, {\"x\": 11.18, \"y\": 555}, {\"x\": 11.31, \"y\": 555}, {\"x\": 11.440000000000001, \"y\": 554}, {\"x\": 11.57, \"y\": 554}, {\"x\": 11.700000000000001, \"y\": 554}, {\"x\": 11.83, \"y\": 554}, {\"x\": 11.96, \"y\": 554}, {\"x\": 12.09, \"y\": 554}, {\"x\": 12.22, \"y\": 554}, {\"x\": 12.35, \"y\": 554}, {\"x\": 12.48, \"y\": 553}, {\"x\": 12.610000000000001, \"y\": 553}, {\"x\": 12.74, \"y\": 553}, {\"x\": 12.870000000000001, \"y\": 553}, {\"x\": 13.0, \"y\": 552}, {\"x\": 13.13, \"y\": 551}, {\"x\": 13.26, \"y\": 551}, {\"x\": 13.39, \"y\": 548}, {\"x\": 13.52, \"y\": 548}, {\"x\": 13.65, \"y\": 548}, {\"x\": 13.780000000000001, \"y\": 548}, {\"x\": 13.91, \"y\": 547}, {\"x\": 14.040000000000001, \"y\": 546}, {\"x\": 14.17, \"y\": 545}, {\"x\": 14.3, \"y\": 545}, {\"x\": 14.43, \"y\": 543}, {\"x\": 14.56, \"y\": 541}, {\"x\": 14.690000000000001, \"y\": 540}, {\"x\": 14.82, \"y\": 539}, {\"x\": 14.950000000000001, \"y\": 539}, {\"x\": 15.08, \"y\": 537}, {\"x\": 15.21, \"y\": 535}, {\"x\": 15.34, \"y\": 533}, {\"x\": 15.47, \"y\": 531}, {\"x\": 15.600000000000001, \"y\": 523}, {\"x\": 15.73, \"y\": 521}, {\"x\": 15.860000000000001, \"y\": 519}, {\"x\": 15.99, \"y\": 517}, {\"x\": 16.12, \"y\": 513}, {\"x\": 16.25, \"y\": 510}, {\"x\": 16.38, \"y\": 505}, {\"x\": 16.51, \"y\": 502}, {\"x\": 16.64, \"y\": 500}, {\"x\": 16.77, \"y\": 498}, {\"x\": 16.900000000000002, \"y\": 490}, {\"x\": 17.03, \"y\": 484}, {\"x\": 17.16, \"y\": 479}, {\"x\": 17.29, \"y\": 478}, {\"x\": 17.42, \"y\": 477}, {\"x\": 17.55, \"y\": 473}, {\"x\": 17.68, \"y\": 463}, {\"x\": 17.810000000000002, \"y\": 458}, {\"x\": 17.94, \"y\": 454}, {\"x\": 18.07, \"y\": 451}, {\"x\": 18.2, \"y\": 444}, {\"x\": 18.330000000000002, \"y\": 442}, {\"x\": 18.46, \"y\": 437}, {\"x\": 18.59, \"y\": 427}, {\"x\": 18.72, \"y\": 420}, {\"x\": 18.85, \"y\": 415}, {\"x\": 18.98, \"y\": 402}, {\"x\": 19.11, \"y\": 390}, {\"x\": 19.240000000000002, \"y\": 389}, {\"x\": 19.37, \"y\": 384}, {\"x\": 19.5, \"y\": 368}, {\"x\": 19.63, \"y\": 363}, {\"x\": 19.76, \"y\": 355}, {\"x\": 19.89, \"y\": 344}, {\"x\": 20.02, \"y\": 337}, {\"x\": 20.150000000000002, \"y\": 325}, {\"x\": 20.28, \"y\": 316}, {\"x\": 20.41, \"y\": 310}, {\"x\": 20.54, \"y\": 302}, {\"x\": 20.67, \"y\": 292}, {\"x\": 20.8, \"y\": 283}, {\"x\": 20.93, \"y\": 272}, {\"x\": 21.060000000000002, \"y\": 266}, {\"x\": 21.19, \"y\": 257}, {\"x\": 21.32, \"y\": 249}, {\"x\": 21.45, \"y\": 240}, {\"x\": 21.580000000000002, \"y\": 227}, {\"x\": 21.71, \"y\": 216}, {\"x\": 21.84, \"y\": 204}, {\"x\": 21.970000000000002, \"y\": 188}, {\"x\": 22.1, \"y\": 176}, {\"x\": 22.195899652409235, \"y\": 165}, {\"x\": 22.23, \"y\": 162}, {\"x\": 22.36, \"y\": 147}, {\"x\": 22.490000000000002, \"y\": 139}, {\"x\": 22.56442071922308, \"y\": 131}, {\"x\": 22.62, \"y\": 122}, {\"x\": 22.75, \"y\": 106}, {\"x\": 22.880000000000003, \"y\": 92}, {\"x\": 23.01, \"y\": 80}, {\"x\": 23.041392685158225, \"y\": 77}, {\"x\": 23.14, \"y\": 72}, {\"x\": 23.27, \"y\": 63}, {\"x\": 23.400000000000002, \"y\": 53}, {\"x\": 23.496929648073216, \"y\": 46}, {\"x\": 23.53, \"y\": 44}, {\"x\": 23.66, \"y\": 31}, {\"x\": 23.79, \"y\": 23}, {\"x\": 23.90848501887865, \"y\": 20}, {\"x\": 23.92, \"y\": 20}, {\"x\": 24.05, \"y\": 16}, {\"x\": 24.18, \"y\": 12}, {\"x\": 24.310000000000002, \"y\": 12}, {\"x\": 24.44, \"y\": 7}, {\"x\": 24.57, \"y\": 7}, {\"x\": 24.57518784492766, \"y\": 6}, {\"x\": 24.7, \"y\": 5}, {\"x\": 24.830000000000002, \"y\": 5}, {\"x\": 24.96, \"y\": 4}, {\"x\": 25.09, \"y\": 3}, {\"x\": 25.220000000000002, \"y\": 3}, {\"x\": 25.32221929473392, \"y\": 1}, {\"x\": 25.35, \"y\": 1}, {\"x\": 25.48, \"y\": 1}, {\"x\": 25.61, \"y\": 1}, {\"x\": 25.698970004344705, \"y\": 0}]}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"Gemini\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"center\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 25.698970004344705, \"targetY\": 0, \"relDx\": -0.02, \"relDy\": 0.04, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"GPT-4\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"center\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 25.32221929473392, \"targetY\": 1, \"relDx\": -0.01, \"relDy\": 0.04, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"Falcon 180B\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"center\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 24.57518784492766, \"targetY\": 6, \"relDx\": -0.01, \"relDy\": 0.04, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"Llama 2 70B\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"center\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 23.90848501887865, \"targetY\": 20, \"relDx\": -0.01, \"relDy\": 0.04, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"GPT-3\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"center\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 23.496929648073216, \"targetY\": 46, \"relDx\": 0.015, \"relDy\": 0, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"Turing-NLG\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"center\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 22.195899652409235, \"targetY\": 165, \"relDx\": 0.015, \"relDy\": 0, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"AlphaZero\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"center\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 22.56442071922308, \"targetY\": 131, \"relDx\": 0.015, \"relDy\": 0, \"targetCoords\": \"data\"}, {\"type\": \"annotation\", \"color\": \"black\", \"zOrder\": 3, \"text\": \"Whisper v2\", \"x\": 5, \"y\": 0, \"ha\": \"left\", \"va\": \"center\", \"textCoords\": \"offset points\", \"weight\": \"normal\", \"targetX\": 23.041392685158225, \"targetY\": 77, \"relDx\": 0.015, \"relDy\": 0, \"targetCoords\": \"data\"}, {\"type\": \"scatter\", \"alpha\": 1, \"zOrder\": 10, \"points\": [{\"x\": 25.698970004344705, \"y\": 0.0, \"size\": 8}, {\"x\": 25.32221929473392, \"y\": 1.0, \"size\": 8}, {\"x\": 24.57518784492766, \"y\": 6.0, \"size\": 8}, {\"x\": 23.90848501887865, \"y\": 20.0, \"size\": 8}, {\"x\": 23.496929648073216, \"y\": 46.0, \"size\": 8}, {\"x\": 22.195899652409235, \"y\": 165.0, \"size\": 8}, {\"x\": 22.56442071922308, \"y\": 131.0, \"size\": 8}, {\"x\": 23.041392685158225, \"y\": 77.0, \"size\": 8}], \"fillColor\": \"rgb(147.0, 231.0, 94.0)\", \"strokeColor\": \"rgb(147.0, 231.0, 94.0)\", \"fillAlpha\": 1.0, \"strokeAlpha\": 1.0, \"marker\": \"M 0.0,-0.5 C 0.13260155,-0.5 0.25978993539242673,-0.44731684579412084 0.3535533905932738,-0.3535533905932738 C 0.44731684579412084,-0.25978993539242673 0.5,-0.13260155 0.5,0.0 C 0.5,0.13260155 0.44731684579412084,0.25978993539242673 0.3535533905932738,0.3535533905932738 C 0.25978993539242673,0.44731684579412084 0.13260155,0.5 0.0,0.5 C -0.13260155,0.5 -0.25978993539242673,0.44731684579412084 -0.3535533905932738,0.3535533905932738 C -0.44731684579412084,0.25978993539242673 -0.5,0.13260155 -0.5,0.0 C -0.5,-0.13260155 -0.44731684579412084,-0.25978993539242673 -0.3535533905932738,-0.3535533905932738 C -0.25978993539242673,-0.44731684579412084 -0.13260155,-0.5 0.0,-0.5 Z 0.0,-0.5\", \"isFilled\": true}], \"hud\": [], \"originalDataAspectRatio\": 0.5961290322580646, \"additionalLegendItems\": []} Figure 7: number of models (y-axis) above different compute thresholds (x-axis) in different years (legend). In any year, the figure illustrates how many models had been published after being trained on a given amount of compute. The annual counts are spaced one year apart, with numbers as of March 31 in each year. The compute frontier of large models advanced rapidly after 2021. Most existing large-scale models were published in the last two years: 21 during 2022, and 42 in 2023. A similar pattern holds across all compute thresholds between 10 22 and 10 24 FLOP. At any given time, there were a small number of models at the leading edge of training compute. However, once the first model was trained at a given compute scale, subsequent models followed at an increasing rate. The frontier has grown rapidly: in 2020 a handful of models were above 10 22 FLOP, but by 2024 there were hundreds, as illustrated in Figure 7. Credits This article was written by Robi Rahman, David Owen, and Josh You. The frontier model search was conducted by Robi Rahman, Josh You, David Owen, and Ben Cottier. The training compute of models identified during the search was estimated by Josh You, Bartosz Podkanowicz, and Robi Rahman. We thank Anson Ho, Jaime Sevilla, Robert Sandler, Tamay Besiroglu, Ben Cottier, and Markus Anderljung for review and feedback on the drafts, and Edu Roldan and Robert Sandler for formatting the article for online publication. Notes Based on December 2023 cloud compute cost rates to train a model with 10 23 FLOPs at 30-50% model FLOPs utilization. For detailed training cost estimates, see our forthcoming study in the 2024 Stanford AI Index Report. \u21a9 Gemini 1.5 Pro and Claude 3 are two recently announced models that are also multimodal. They are likely among the largest models trained to date, though we do not have compute estimates for them yet. \u21a9 Countries included in the category Other include: the United Arab Emirates (2 models), France (2), Israel (1), South Korea (1), Russia (1), Germany (1), Japan (1), and Finland (1). \u21a9 Google\u2019s AI division merged with DeepMind in 2023, forming an AI lab called Google DeepMind. Models created by Google DeepMind are counted as \u201cGoogle\u201d in this graph, while DeepMind models from before this merger are counted separately. \u21a9 TII, or Technology Innovation Institute, is a government research institute in the United Arab Emirates. \u21a9 Includes fully open-source models (weights, code, data), and models with weights released under permissive or open-source licenses. \u21a9 paper , repository \u21a9 The most popular search engine is Google in every language except the following: Baidu is the top search engine for Chinese, Yandex for Russian, and Naver for Korean. \u21a9 Some models that were in fact trained on >10 23 FLOP may be missing from our list of large-scale models because we don\u2019t have enough information on how much compute was used to train them, not because we don\u2019t know about them. See the appendix for a list of models we suspect may have been trained on over 10 23 FLOP, but for which we do not have compute counts or compute estimates. \u21a9 About the authors Robi Rahman manages Epoch's database of AI models, machine learning hardware, and supercomputers. His research covers inputs and algorithms for AI development. Before joining Epoch AI, Robi was a contributor to the Stanford AI Index Report and received a master's degree in data science from Harvard University. David Owen is a researcher with a background in computer vision and machine learning. He is interested in analysing and predicting model capabilities, and using empirical data to explore AI deployment in the real world. Before joining Epoch AI, David worked in an industrial research lab developing AI models for surgical video. Josh You is a data analyst who collects and analyzes data on AI systems. Before Epoch AI, he worked as a software engineer and a content writer, and graduated from Carleton College with a degree in Computer Science and Mathematics. Tags Training Data Related work announcement \u00b7 1 min read Announcing Epoch AI\u2019s Updated Parameter, Compute and Data Trends Database Our database now spans over 700 ML systems, tracking parameters, datasets, and training compute details for notable machine learning models. Oct 23, 2023 \u00b7 By The Epoch AI Team report \u00b7 66 min read Trends in the Dollar Training Cost of Machine Learning Systems How much does it cost to train AI models? Looking at 124 ML systems from between 2009 and 2022, we find the cost has grown by approximately 0.5OOM/year. Jan 31, 2023 \u00b7 By Ben Cottier report \u00b7 23 min read Biological Sequence Models in the Context of the AI Directives Our expanded database now includes biological sequence models, highlighting potential regulatory gaps and the growth of training compute in these models. Jan 17, 2024 \u00b7 Updated Apr 09, 2024 \u00b7 By Nicole Maug, Aidan O'Gara and Tamay Besiroglu Epoch AI\u2019s work is free to use, distribute, and reproduce provided the source and authors are credited under the Creative Commons Attribution license . Cite this work as Robi Rahman, David Owen and Josh You (2024), \"Tracking Large-Scale AI Models\". Published online at epoch.ai. Retrieved from: 'https://epoch.ai/blog/tracking-large-scale-ai-models' [online resource] BibTeX citation @misc{epoch2024trackinglargescaleaimodels, title={Tracking Large-Scale AI Models}, author={Robi Rahman and David Owen and Josh You}, year={2024}, url={https://epoch.ai/blog/tracking-large-scale-ai-models}, note={Accessed: } } Not implemented yet We will redirect to a new page listing all the posts by this author. Excited about our work? Talk to us Support our research Sign up for our newsletter to read weekly commentary on AI news and developments and the latest updates on our research. Subscribe to our newsletter Publications & Commentary Publications Newsletter Podcast Data & Resources Data on AI AI Trends & Statistics Data Insights Projects FrontierMath GATE Playground Distributed Training Model Counts Company About Us Our Team Careers Consultations Our Funding Donate Latest Contact @ 2025 Epoch AI Privacy Notice Cookie Policy We value your privacy Our website uses cookies to enhance your browsing experience and analyze site traffic. By clicking \u2018Accept All,\u2019 you consent to our use of cookies as described in our Privacy Policy and Cookie Policy . If you wish to withdraw your consent, you can contact us at ops@epoch.ai . Reject all Accept all Help us make our website better! Please tell us about you. I am a: Government Representative Policy Advisor Software Engineer ML/AI Engineer AI Researcher Data Scientist Journalist Student Entrepreneur AI Enthusiast Other No thanks Submit Feedback Leave feedback Have a question? Noticed something wrong? Let us know. Please enter your feedback If you would like a reply, please include your name and email address. Cancel Submit Thank you for your feedback! Your comment will be reviewed. We may not be able to respond to every submission. Close There\u2019s been an error in submitting your feedback. Please try again later. Back",
  "_binary": false
}
```

### 3. 3
**Status**: success

```json
{
  "status": "success",
  "url": "https://epoch.ai/data",
  "title": "Data on the Trajectory of AI | Epoch AI Database | Epoch AI",
  "extracted_text": "Data on the Trajectory of AI | Epoch AI Database | Epoch AI Publications & Commentary Publications Newsletter Podcast Data & Resources Datasets Overview AI Benchmarking AI Models ML Hardware GPU Clusters Resources AI Trends & Statistics Data Insights Projects FrontierMath GATE Playground Distributed Training Model Counts About About Us Our Team Careers Consultations Our Funding Donate Latest Contact Search epoch.ai Search Enter a query to search for results Placeholder Data on AI Epoch AI collects key data on machine learning models from 1950 to the present to analyze historical and contemporary progress in AI. Our database is a valuable resource for policymakers, researchers, and stakeholders to foster responsible AI development and deployment. Explore our data Data on AI Models Our public database, the largest of its kind, tracks over 3000 machine learning models from 1950 to today. Explore data and graphs showing the trajectory of AI. Updated September 20, 2025 AI Benchmarking Hub Track the performance of leading AI models on challenging benchmarks, with insights into compute and accessibility. Updated September 20, 2025 Machine Learning Hardware Key data on 160+ AI accelerators, such as GPUs and TPUs, used for developing and deploying ML models in the deep learning era. Updated September 20, 2025 GPU Clusters Our database of over 500 GPU clusters and supercomputers tracks large hardware facilities, including those used for AI training and inference. Updated September 20, 2025 Use our work Epoch AI\u2019s data is free to use, distribute, and reproduce provided the source and authors are credited under the Creative Commons Attribution license . Citations can be found on the respective pages for each dataset. Download our data Data on AI Models ZIP, Updated September 20, 2025 AI Benchmarking Hub ZIP, Updated September 20, 2025 Machine Learning Hardware ZIP, Updated September 20, 2025 GPU Clusters ZIP, Updated September 20, 2025 Our methodology We identify and track contemporary and historic advances in AI, collating key details across several areas. This research includes who developed models, when, and for what tasks, how much compute was used for training, how many parameters models have, how much data was used for training, what hardware was used for training, and more. Our research Epoch AI is a multidisciplinary research institute investigating the trajectory and impact of artificial intelligence. We publish datasets, data visualizations, research reports, and predictive models to analyze the forces shaping AI development. Our goal is to foster scientific dialogue and bring empirical rigor to predictions about the future of AI. Learn more about our research . Press & citations Our World in Data The brief history of artificial intelligence December 6, 2022 Time 4 Charts That Show Why AI Progress Is Unlikely to Slow Down November 6, 2023 Google An important next step on our AI journey February 6, 2023 The Economist The bigger-is-better approach to AI is running out of road June 21, 2023 The New York Times The 2024 Good Tech Awards December 30, 2024 Fortune The cost of training AI could soon become too much to bear April 4, 2024 Collaborate with us We\u2019re proud to partner with select stakeholders on projects aligned with our mission. Contact us Sign up for our newsletter to read weekly commentary on AI news and developments and the latest updates on our research. Subscribe to our newsletter Publications & Commentary Publications Newsletter Podcast Data & Resources Data on AI AI Trends & Statistics Data Insights Projects FrontierMath GATE Playground Distributed Training Model Counts Company About Us Our Team Careers Consultations Our Funding Donate Latest Contact @ 2025 Epoch AI Privacy Notice Cookie Policy We value your privacy Our website uses cookies to enhance your browsing experience and analyze site traffic. By clicking \u2018Accept All,\u2019 you consent to our use of cookies as described in our Privacy Policy and Cookie Policy . If you wish to withdraw your consent, you can contact us at ops@epoch.ai . Reject all Accept all Help us make our website better! Please tell us about you. I am a: Government Representative Policy Advisor Software Engineer ML/AI Engineer AI Researcher Data Scientist Journalist Student Entrepreneur AI Enthusiast Other No thanks Submit Feedback Leave feedback Have a question? Noticed something wrong? Let us know. Please enter your feedback If you would like a reply, please include your name and email address. Cancel Submit Thank you for your feedback! Your comment will be reviewed. We may not be able to respond to every submission. Close There\u2019s been an error in submitting your feedback. Please try again later. Back",
  "_binary": false
}
```

### 4. 4
**Status**: success

```json
{
  "status": "success",
  "url": "https://mne.tools/stable/auto_tutorials/epochs/20_visualize_epochs.html",
  "title": "Visualizing epoched data \u2014 MNE 1.10.1 documentation",
  "extracted_text": "Visualizing epoched data \u2014 MNE 1.10.1 documentation Skip to main content Ctrl + K Install Documentation API Reference Get help Development Choose version Discord (office hours) Mastodon Q&A Forum Code Repository Sponsor us on GitHub Donate via OpenCollective Install Documentation API Reference Get help Development Choose version Discord (office hours) Mastodon Q&A Forum Code Repository Sponsor us on GitHub Donate via OpenCollective Section Navigation Tutorials Introductory tutorials Overview of MEG/EEG analysis with MNE-Python Modifying data in-place Parsing events from raw data The Info data structure Working with sensor locations Configuring MNE-Python Getting started with mne.Report Reading data for different recording systems Importing data from MEG devices Importing data from EEG devices Importing data from fNIRS devices Working with CTF data: the Brainstorm auditory dataset Importing Data from Eyetracking devices Working with continuous data The Raw data structure: continuous data Working with events Annotating continuous data Built-in plotting methods for Raw objects Preprocessing Overview of artifact detection Handling bad channels Rejecting bad data spans and breaks Background information on filtering Filtering and resampling data Repairing artifacts with regression Repairing artifacts with ICA Background on projectors and projections Repairing artifacts with SSP Setting the EEG reference Extracting and visualizing subject head movement Signal-space separation (SSS) and Maxwell filtering Preprocessing functional near-infrared spectroscopy (fNIRS) data Preprocessing optically pumped magnetometer (OPM) MEG data Working with eye tracker data in MNE-Python Segmenting continuous data into epochs The Epochs data structure: discontinuous data Regression-based baseline correction Visualizing epoched data Working with Epoch metadata Auto-generating Epochs metadata Exporting Epochs to Pandas DataFrames Divide continuous data into equally-spaced epochs Estimating evoked responses The Evoked data structure: evoked/averaged data Visualizing Evoked data EEG analysis - Event-Related Potentials (ERPs) Plotting whitened data Time-frequency analysis The Spectrum and EpochsSpectrum classes: frequency-domain data Frequency and time-frequency sensor analysis Frequency-tagging: Basic analysis of an SSVEP/vSSR dataset Forward models and source spaces FreeSurfer MRI reconstruction Source alignment and coordinate frames Using an automated approach to coregistration Head model and forward computation EEG forward operator with a template MRI How MNE uses FreeSurfer\u2019s outputs Fixing BEM and head surfaces Computing a covariance matrix Source localization and inverses The SourceEstimate data structure Source localization with equivalent current dipole (ECD) fit Source localization with MNE, dSPM, sLORETA, and eLORETA The role of dipole orientations in distributed source localization Computing various MNE solutions Source reconstruction using an LCMV beamformer Visualize source time courses (stcs) EEG source localization given electrode locations on an MRI Brainstorm Elekta phantom dataset tutorial Brainstorm CTF phantom dataset tutorial 4D Neuroimaging/BTi phantom dataset tutorial KIT phantom dataset tutorial Statistical analysis of sensor data Statistical inference Visualising statistical significance thresholds on EEG data Non-parametric 1 sample cluster statistic on single trial power Non-parametric between conditions cluster statistic on single trial power Mass-univariate twoway repeated measures ANOVA on single trial power Spatiotemporal permutation F-test on full sensor data Statistical analysis of source estimates Permutation t-test on source data with spatio-temporal clustering 2 samples permutation test on source data with spatio-temporal clustering Repeated measures ANOVA on source data with spatio-temporal clustering Machine learning models of neural activity Spectro-temporal receptive field (STRF) estimation on continuous data Decoding (MVPA) Clinical applications Working with sEEG data Working with ECoG data Sleep stage classification from polysomnography (PSG) data Simulation Creating MNE-Python data structures from scratch Corrupt known signal with point spread DICS for power mapping Visualization tutorials Make figures more publication ready Using the event system to link figures Examples Input/Output Getting averaging info from .fif files Getting impedances from raw files How to use data in neural ensemble (NEO) format Reading/Writing a noise covariance matrix Reading XDF EEG data Data Simulation Compare simulated and estimated source activity Generate simulated evoked data Generate simulated raw data Simulate raw data using subject anatomy Generate simulated source data Preprocessing Using contralateral referencing for EEG Cortical Signal Suppression (CSS) for removal of cortical signals Define target events based on time lag, plot evoked response Identify EEG Electrodes Bridged by too much Gel Transform EEG data using current source density (CSD) Show EOG artifact timing Reduce EOG artifacts through regression Automated epochs metadata generation with variable time windows Principal Component Analysis - Optimal Basis Sets (PCA-OBS) removing cardiac artefact Find MEG reference channel artifacts Visualise NIRS artifact correction methods Compare the different ICA algorithms in MNE Interpolate bad channels for MEG/EEG channels Interpolate EEG data to any montage Maxwell filter data with movement compensation Annotate movement artifacts and reestimate dev_head_t Annotate muscle artifacts Removing muscle ICA components Plot sensor denoising using oversampled temporal projection Shifting time-scale in evoked data Remap MEG channel types XDAWN Denoising Visualization How to convert 3D electrode positions to a 2D image Plotting with mne.viz.Brain Visualize channel over epochs as an image Plotting EEG sensors on the scalp Plotting topographic arrowmaps of evoked data Plotting topographic maps of evoked data Whitening evoked data with a noise covariance Plotting eye-tracking heatmaps in MNE-Python Plotting sensor layouts of MEG systems Plot the MNE brain and helmet Plotting sensor layouts of EEG systems Plot a cortical parcellation Plot single trial activity, grouped by ROI and sorted by RT Sensitivity map of SSP projections Compare evoked responses for different conditions Plot custom topographies for MEG sensors Cross-hemisphere comparison Time-Frequency Examples Compute a cross-spectral density (CSD) matrix Compute Power Spectral Density of inverse solution from single epochs Compute power and phase lock in label of the source space Compute source power spectral density (PSD) in a label Compute source power spectral density (PSD) of VectorView and OPM data Compute induced power in the source space with dSPM Temporal whitening with AR model Compute and visualize ERDS maps Explore event-related dynamics for specific frequency bands Time-frequency on simulated data (Multitaper vs. Morlet vs. Stockwell vs. Hilbert) Statistics Examples Permutation F-test on sensor data with 1D cluster level FDR correction on T-test on sensor data Regression on continuous data (rER[P/F]) Permutation T-test on sensor data Analysing continuous features with binning and regression in sensor space Machine Learning (Decoding, Encoding, and MVPA) Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP) Decoding in time-frequency space using Common Spatial Patterns (CSP) Representational Similarity Analysis Decoding source space data Continuous Target Decoding with SPoC Decoding sensor space data with generalization across time and conditions Analysis of evoked response using ICA and PCA reduction techniques XDAWN Decoding From EEG data Compute effect-matched-spatial filtering (EMS) Linear classifier on sensor data with plot patterns and filters Receptive Field Estimation and Prediction Compute spatial filters with Spatio-Spectral Decomposition (SSD) Connectivity Analysis Examples Forward modeling Display sensitivity maps for EEG and MEG sensors Generate a left cerebellum volume source space Use source space morphing Inverse problem and source analysis Compute MNE-dSPM inverse solution on single epochs Compute sLORETA inverse solution on raw data Compute MNE-dSPM inverse solution on evoked data in volume source space Source localization with a custom inverse solver Compute source level time-frequency timecourses using a DICS beamformer Compute source power using DICS beamformer Compute evoked ERS source power using DICS, LCMV beamformer, and dSPM Compute a sparse inverse solution using the Gamma-MAP empirical Bayesian method Extracting time course from source_estimate object Generate a functional label from source estimates Extracting the time series of activations in a label Compute sparse inverse solution with mixed norm: MxNE and irMxNE Compute MNE inverse solution on evoked data with a mixed source space Compute source power estimate by projecting the covariance with MNE Morph surface source estimate Morph volumetric source estimate Computing source timecourses with an XFit-like multi-dipole model Compute iterative reweighted TF-MxNE with multiscale time-frequency dictionary Visualize source leakage among labels using a circular graph Plot point-spread functions (PSFs) and cross-talk functions (CTFs) Compute cross-talk functions for LCMV beamformers Plot point-spread functions (PSFs) for a volume Compute Rap-Music on evoked data Reading an inverse operator Reading an STC file Compute spatial resolution metrics in source space Compute spatial resolution metrics to compare MEG with EEG+MEG Estimate data SNR using an inverse Computing source space SNR Compute MxNE with time-frequency sparse prior Compute Trap-Music on evoked data Plotting the full vector-valued MNE solution Examples on open datasets Brainstorm raw (median nerve) dataset HF-SEF dataset Kernel OPM phantom data Single trial linear regression analysis with the LIMO dataset Optically pumped magnetometer (OPM) data From raw data to dSPM on SPM Faces dataset Glossary Implementation details Design philosophy Example datasets Command-line tools Migrating from other analysis software The typical M/EEG workflow How to cite MNE-Python Papers citing MNE-Python Note Go to the end to download the full example code. Visualizing epoched data # This tutorial shows how to plot epoched data as time series, how to plot the spectral density of epoched data, how to plot epochs as an imagemap, and how to plot the sensor locations and projectors stored in Epochs objects. We\u2019ll start by importing the modules we need, loading the continuous (raw) sample data, and cropping it to save memory: # Authors: The MNE-Python contributors. # License: BSD-3-Clause # Copyright the MNE-Python contributors. import mne sample_data_folder = mne . datasets . sample . data_path () sample_data_raw_file = sample_data_folder / \"MEG\" / \"sample\" / \"sample_audvis_raw.fif\" raw = mne . io . read_raw_fif ( sample_data_raw_file , verbose = False ) . crop ( tmax = 120 ) To create the Epochs data structure, we\u2019ll extract the event IDs stored in the stim channel , map those integer event IDs to more descriptive condition labels using an event dictionary, and pass those to the Epochs constructor, along with the Raw data and the desired temporal limits of our epochs, tmin and tmax (for a detailed explanation of these steps, see The Epochs data structure: discontinuous data ). events = mne . find_events ( raw , stim_channel = \"STI 014\" ) event_dict = { \"auditory/left\" : 1 , \"auditory/right\" : 2 , \"visual/left\" : 3 , \"visual/right\" : 4 , \"face\" : 5 , \"button\" : 32 , } epochs = mne . Epochs ( raw , events , tmin =- 0.2 , tmax = 0.5 , event_id = event_dict , preload = True ) del raw Finding events on: STI 014 176 events found on stim channel STI 014 Event IDs: [ 1 2 3 4 5 32] Not setting metadata 176 matching events found Setting baseline interval to [-0.19979521315838786, 0.0] s Applying baseline correction (mode: mean) Created an SSP operator (subspace dimension = 3) 3 projection items activated Loading data for 176 events and 421 original time points ... 1 bad epochs dropped Plotting Epochs as time series # Interactivity in pipelines and scripts To use the interactive features of the plot method when running your code non-interactively, pass the block=True parameter, which halts the Python interpreter until the figure window is closed. That way, any channels or epochs that you mark as \u201cbad\u201d will be taken into account in subsequent processing steps. To visualize epoched data as time series (one time series per channel), the mne.Epochs.plot method is available. It creates an interactive window where you can scroll through epochs and channels, enable/disable any unapplied SSP projectors to see how they affect the signal, and even manually mark bad channels (by clicking the channel name) or bad epochs (by clicking the data) for later dropping. Channels marked \u201cbad\u201d will be shown in light grey color and will be added to epochs.info['bads'] ; epochs marked as bad will be indicated as 'USER' in epochs.drop_log . Here we\u2019ll plot only the \u201ccatch\u201d trials from the sample dataset , and pass in our events array so that the button press responses also get marked (we\u2019ll plot them in red, and plot the \u201cface\u201d events defining time zero for each epoch in blue). We also need to pass in our event_dict so that the plot method will know what we mean by \u201cbutton\u201d \u2014 this is because subsetting the conditions by calling epochs['face'] automatically purges the dropped entries from epochs.event_id : catch_trials_and_buttonpresses = mne . pick_events ( events , include = [ 5 , 32 ]) epochs [ \"face\" ] . plot ( events = catch_trials_and_buttonpresses , event_id = event_dict , event_color = dict ( button = \"red\" , face = \"blue\" ), ) To see all sensors at once, we can use butterfly mode and group by selection: epochs [ \"face\" ] . plot ( events = catch_trials_and_buttonpresses , event_id = event_dict , event_color = dict ( button = \"red\" , face = \"blue\" ), group_by = \"selection\" , butterfly = True , ) Plotting projectors from an Epochs object # In the plot above we can see heartbeat artifacts in the magnetometer channels, so before we continue let\u2019s load ECG projectors from disk and apply them to the data: ecg_proj_file = sample_data_folder / \"MEG\" / \"sample\" / \"sample_audvis_ecg-proj.fif\" ecg_projs = mne . read_proj ( ecg_proj_file ) epochs . add_proj ( ecg_projs ) epochs . apply_proj () Read a total of 6 projection items: ECG-planar-999--0.200-0.400-PCA-01 (1 x 203) idle ECG-planar-999--0.200-0.400-PCA-02 (1 x 203) idle ECG-axial-999--0.200-0.400-PCA-01 (1 x 102) idle ECG-axial-999--0.200-0.400-PCA-02 (1 x 102) idle ECG-eeg-999--0.200-0.400-PCA-01 (1 x 59) idle ECG-eeg-999--0.200-0.400-PCA-02 (1 x 59) idle 6 projection items deactivated Created an SSP operator (subspace dimension = 9) 9 projection items activated SSP projectors applied... General MNE object type Epochs Measurement date 2002-12-03 at 19:01:10 UTC Participant Unknown Experimenter MEG Acquisition Total number of events 175 Events counts auditory/left: 41 auditory/right: 40 button: 8 face: 8 visual/left: 40 visual/right: 38 Time range -0.200 \u2013 0.499 s Baseline -0.200 \u2013 0.000 s Sampling frequency 600.61 Hz Time points 421 Metadata No metadata set Channels Magnetometers 102 Gradiometers 203 and 1 bad EEG 59 and 1 bad EOG 1 Stimulus 9 Head & sensor digitization 146 points Filters Highpass 0.10 Hz Lowpass 172.18 Hz Projections PCA-v1 (on) PCA-v2 (on) PCA-v3 (on) ECG-planar-999--0.200-0.400-PCA-01 (on) ECG-planar-999--0.200-0.400-PCA-02 (on) ECG-axial-999--0.200-0.400-PCA-01 (on) ECG-axial-999--0.200-0.400-PCA-02 (on) ECG-eeg-999--0.200-0.400-PCA-01 (on) ECG-eeg-999--0.200-0.400-PCA-02 (on) Just as we saw in the Plotting projectors from Raw objects section, we can plot the projectors present in an Epochs object using the same plot_projs_topomap method. Since the original three empty-room magnetometer projectors were inherited from the Raw file, and we added two ECG projectors for each sensor type, we should see nine projector topomaps: epochs . plot_projs_topomap ( vlim = \"joint\" ) Note that these field maps illustrate aspects of the signal that have already been removed (because projectors in Raw data are applied by default when epoching, and because we called apply_proj after adding additional ECG projectors from file). You can check this by examining the 'active' field of the projectors: print ( all ( proj [ \"active\" ] for proj in epochs . info [ \"projs\" ])) True Plotting sensor locations # Just like Raw objects, Epochs objects keep track of sensor locations, which can be visualized with the plot_sensors method: epochs . plot_sensors ( kind = \"3d\" , ch_type = \"all\" ) epochs . plot_sensors ( kind = \"topomap\" , ch_type = \"all\" ) Plotting the power spectrum of Epochs # Again, just like Raw objects, Epochs objects can be converted to spectral density via compute_psd() , which can then be plotted using the EpochsSpectrum \u2019s plot() method. epochs [ \"auditory\" ] . compute_psd () . plot ( picks = \"eeg\" , exclude = \"bads\" , amplitude = False ) Using multitaper spectrum estimation with 7 DPSS windows Plotting power spectral density (dB=True). Averaging across epochs before plotting... It is also possible to plot spectral power estimates across sensors as a scalp topography, using the EpochsSpectrum \u2019s plot_topomap() method. The default parameters will plot five frequency bands (\u03b4, \u03b8, \u03b1, \u03b2, \u03b3), will compute power based on magnetometer channels (if present), and will plot the power estimates on a dB-like log-scale: spectrum = epochs [ \"visual/right\" ] . compute_psd () spectrum . plot_topomap () Using multitaper spectrum estimation with 7 DPSS windows Averaging across epochs before plotting... Note Prior to the addition of the EpochsSpectrum class, the above plots were possible via: epochs [ 'auditory' ] . plot_psd ( picks = 'eeg' ) epochs [ 'visual/right' ] . plot_psd_topomap () The plot_psd() and plot_psd_topomap methods of Epochs objects are still provided to support legacy analysis scripts, but new code should instead use the EpochsSpectrum object API. Just like plot_projs_topomap , EpochsSpectrum.plot_topomap() has a vlim='joint' option for fixing the colorbar limits jointly across all subplots, to give a better sense of the relative magnitude in each frequency band. You can change which channel type is used via the ch_type parameter, and if you want to view different frequency bands than the defaults, the bands parameter takes a dict , with keys providing a subplot title and values providing either single frequency bins to plot, or lower/upper frequency band edges: bands = { \"10 Hz\" : 10 , \"15 Hz\" : 15 , \"20 Hz\" : 20 , \"10-20 Hz\" : ( 10 , 20 )} spectrum . plot_topomap ( bands = bands , vlim = \"joint\" , ch_type = \"grad\" ) Averaging across epochs before plotting... If you prefer untransformed power estimates, you can pass dB=False . It is also possible to normalize the power estimates by dividing by the total power across all frequencies, by passing normalize=True . See the docstring of plot_topomap for details. Plotting Epochs as an image map # A convenient way to visualize many epochs simultaneously is to plot them as an image map, with each row of pixels in the image representing a single epoch, the horizontal axis representing time, and each pixel\u2019s color representing the signal value at that time sample for that epoch. Of course, this requires either a separate image map for each channel, or some way of combining information across channels. The latter is possible using the plot_image method; the former can be achieved with the plot_image method (one channel at a time) or with the plot_topo_image method (all sensors at once). By default, the image map generated by plot_image will be accompanied by a scalebar indicating the range of the colormap, and a time series showing the average signal across epochs and a bootstrapped 95% confidence band around the mean. plot_image is a highly customizable method with many parameters, including customization of the auxiliary colorbar and averaged time series subplots. See the docstrings of plot_image and mne.viz.plot_compare_evokeds (which is used to plot the average time series) for full details. Here we\u2019ll show the mean across magnetometers for all epochs with an auditory stimulus: epochs [ \"auditory\" ] . plot_image ( picks = \"mag\" , combine = \"mean\" ) Not setting metadata 81 matching events found No baseline correction applied 0 projection items activated combining channels using \"mean\" To plot image maps for individual sensors or a small group of sensors, use the picks parameter. Passing combine=None (the default) will yield separate plots for each sensor in picks ; passing combine='gfp' will plot the global field power (useful for combining sensors that respond with opposite polarity). epochs [ \"auditory\" ] . plot_image ( picks = [ \"MEG 0242\" , \"MEG 0243\" ]) epochs [ \"auditory\" ] . plot_image ( picks = [ \"MEG 0242\" , \"MEG 0243\" ], combine = \"gfp\" ) Not setting metadata 81 matching events found No baseline correction applied 0 projection items activated Not setting metadata 81 matching events found No baseline correction applied 0 projection items activated Not setting metadata 81 matching events found No baseline correction applied 0 projection items activated combining channels using RMS (grad channels) To plot an image map for all sensors, use plot_topo_image , which is optimized for plotting a large number of image maps simultaneously, and (in interactive sessions) allows you to click on each small image map to pop open a separate figure with the full-sized image plot (as if you had called plot_image on just that sensor). At the small scale shown in this tutorial it\u2019s hard to see much useful detail in these plots; it\u2019s often best when plotting interactively to maximize the topo image plots to fullscreen. The default is a figure with black background, so here we specify a white background and black foreground text. By default plot_topo_image will show magnetometers and gradiometers on the same plot (and hence not show a colorbar, since the sensors are on different scales) so we\u2019ll also pass a Layout restricting each plot to one channel type. First, however, we\u2019ll also drop any epochs that have unusually high signal levels, because they can cause the colormap limits to be too extreme and therefore mask smaller signal fluctuations of interest. reject_criteria = dict ( mag = 3000e-15 , # 3000 fT grad = 3000e-13 , # 3000 fT/cm eeg = 150e-6 , ) # 150 \u00b5V epochs . drop_bad ( reject = reject_criteria ) for ch_type , title in dict ( mag = \"Magnetometers\" , grad = \"Gradiometers\" ) . items (): layout = mne . channels . find_layout ( epochs . info , ch_type = ch_type ) epochs [ \"auditory/left\" ] . plot_topo_image ( layout = layout , fig_facecolor = \"w\" , font_color = \"k\" , title = title ) Rejecting epoch based on EEG : ['EEG 001', 'EEG 002', 'EEG 003', 'EEG 004', 'EEG 005', 'EEG 006', 'EEG 007', 'EEG 015', 'EEG 016', 'EEG 023', 'EEG 039', 'EEG 041', 'EEG 044', 'EEG 045', 'EEG 046', 'EEG 047', 'EEG 048', 'EEG 049', 'EEG 050', 'EEG 051', 'EEG 052', 'EEG 054', 'EEG 055', 'EEG 056', 'EEG 057', 'EEG 058', 'EEG 059'] Rejecting epoch based on EEG : ['EEG 001', 'EEG 002', 'EEG 003', 'EEG 007', 'EEG 048', 'EEG 055'] Rejecting epoch based on EEG : ['EEG 007'] Rejecting epoch based on EEG : ['EEG 003', 'EEG 007'] Rejecting epoch based on MAG : ['MEG 1711'] Rejecting epoch based on EEG : ['EEG 001', 'EEG 002', 'EEG 003', 'EEG 007'] Rejecting epoch based on EEG : ['EEG 001', 'EEG 002', 'EEG 007'] Rejecting epoch based on MAG : ['MEG 1711'] 8 bad epochs dropped To plot image maps for all EEG sensors, pass an EEG layout as the layout parameter of plot_topo_image . Note also here the use of the sigma parameter, which smooths each image map along the vertical dimension (across epochs) which can make it easier to see patterns across the small image maps (by smearing noisy epochs onto their neighbors, while reinforcing parts of the image where adjacent epochs are similar). However, sigma can also disguise epochs that have persistent extreme values and maybe should have been excluded, so it should be used with caution. layout = mne . channels . find_layout ( epochs . info , ch_type = \"eeg\" ) epochs [ \"auditory/left\" ] . plot_topo_image ( layout = layout , fig_facecolor = \"w\" , font_color = \"k\" , sigma = 1 ) Total running time of the script: (0 minutes 30.751 seconds) Download Jupyter notebook: 20_visualize_epochs.ipynb Download Python source code: 20_visualize_epochs.py Download zipped: 20_visualize_epochs.zip Gallery generated by Sphinx-Gallery previous Regression-based baseline correction next Working with Epoch metadata On this page Plotting Epochs as time series Plotting projectors from an Epochs object Plotting sensor locations Plotting the power spectrum of Epochs Plotting Epochs as an image map \u00a9 Copyright 2012\u20132025, MNE Developers. Last updated 2025-08-16 04:20 UTC Built with the PyData Sphinx Theme 0.16.1.",
  "_binary": false
}
```

### 5. 5
**Status**: success

```json
{
  "status": "success",
  "url": "https://github.com/thuml/Large-Time-Series-Model",
  "title": "GitHub - thuml/Large-Time-Series-Model: Official code, datasets and checkpoints for \"Timer: Generative Pre-trained Transformers Are Large Time Series Models\" (ICML 2024) and subsequent works",
  "extracted_text": "GitHub - thuml/Large-Time-Series-Model: Official code, datasets and checkpoints for \"Timer: Generative Pre-trained Transformers Are Large Time Series Models\" (ICML 2024) and subsequent works Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace MCP Registry View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events & Webinars Ebooks & Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert thuml / Large-Time-Series-Model Public Notifications You must be signed in to change notification settings Fork 78 Star 778 Official code, datasets and checkpoints for \"Timer: Generative Pre-trained Transformers Are Large Time Series Models\" (ICML 2024) and subsequent works arxiv.org/abs/2402.02368 License MIT license 778 stars 78 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 4 Pull requests 0 Actions Projects 0 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights thuml/Large-Time-Series-Model main Branches Tags Go to file Code Open more actions menu Folders and files Name Name Last commit message Last commit date Latest commit History 103 Commits data_provider data_provider examples examples exp exp figures figures layers layers models models scripts scripts utils utils .gitignore .gitignore LICENSE LICENSE README.md README.md requirements.txt requirements.txt run.py run.py View all files Repository files navigation README MIT license Large Time-Series Model This repo provides official code, datasets, and checkpoints for Timer: Generative Pre-trained Transformers Are Large Time Series Models . [Poster] [Slides] and our subsequent works [Tutorial (CN)] , [Tutorial (EN)] . Updates \ud83d\udea9 News (2025.5) Sundial , a family of generative time series foundation models has been accepted as ICML 2025 Oral (Top 1%). Get your zero-shot probabilistic predictions within milliseconds! [HuggingFace] [Quickstart] . \ud83d\udea9 News (2025.2) We release an open codebase OpenLTM , which contains the whole pipeline to pre-train and fine-tune large time-series models. \ud83d\udea9 News (2024.12) Timer-XL for unified forecasting is accepted as ICLR 2025 . We released a pre-trained model for zero-shot forecasting [HuggingFace] [Quickstart] [Checkpoint] . \ud83d\udea9 News (2024.10) We release the pre-training dataset UTSD on HuggingFace or you can use the numpy format UTSD and this dataloader . \ud83d\udea9 News (2024.5) Accepted by ICML 2024 , a camera-ready version of 31 pages . \ud83d\udea9 News (2024.2) Releasing model checkpoints and code for fine-tuning on different tasks [README] . Introduction Tim e Series Transfor mer (Timer) is a pre-trained Transformer for general time series analysis. Quick Start We provide out-of-the-box models for zero-shot forecasting (no training and no GPU needed). Example of Timer (Zero-Shot Forecasting) import torch from transformers import AutoModelForCausalLM # load pretrain model model = AutoModelForCausalLM.from_pretrained('thuml/timer-base-84m', trust_remote_code=True) # prepare input batch_size, lookback_length = 1, 2880 seqs = torch.randn(batch_size, lookback_length) # generate forecast prediction_length = 96 normed_output = model.generate(normed_seqs, max_new_tokens=prediction_length) print(output.shape) Example of Sundial (Generative Forecasting) import torch from transformers import AutoModelForCausalLM # load pretrain model # supports different lookback/forecast lengths model = AutoModelForCausalLM.from_pretrained('thuml/sundial-base-128m', trust_remote_code=True) # prepare input batch_size, lookback_length = 1, 2880 seqs = torch.randn(batch_size, lookback_length) # Note that Sundial can generate multiple probable predictions forecast_length = 96 num_samples = 20 output = model.generate(seqs, max_new_tokens=forecast_length, num_samples=num_samples) # use raw predictions for mean/quantiles/confidence-interval estimation print(output.shape) More usage examples are provided here . See our HuggingFace for more information. Model Adaption For developers interested in fine-tuning large time-series models or pre-training on customized datasets , please use OpenLTM , including code scripts and checkpoint of various models. For developers interested in applying large time-series models on other time series analysis tasks (e.g., imputation and anomaly detection), this repo contains scripts and checkpoints [README] . Datasets We collect Unified Time Series Datasets (UTSD), which encompass well-curated time series to facilitate the research on large time-series models. Our dataset is released in HuggingFace . Usage You can access the data from HuggingFace and load the data in the style of TSLib : # huggingface-cli login # export HF_ENDPOINT=https://hf-mirror.com python ./scripts/UTSD/download_dataset.py # dataloader python ./scripts/UTSD/utsdataset.py We provide code for evaluating time series datasets, which you can use to evaluate your Huggingface formatted dataset: python ./scripts/UTSD/dataset_evaluation.py --root_path <dataset root path> --log_path <output log path> If you meet troubles when accessing the data, you can also download UTSD in numpy from [Tsinghua Cloud] and use UTSD_Npy dataloader [here] . Introduction ICML 2024 We propose Timer , a decoder-only pre-trained time series Transformer. We propose single-series sequence (S3) format , converting diverse series into unified 1D sequences. The predictive model can also be adapted for forecasting, imputation, and anomaly detection [README] . ICLR 2025 We proposed Timer-XL for unified time series forecasting. It can be used for supervised training or large-scale pre-training , explicitly modeling multi-dimensional time series [GitHub] . ICML 2025 Oral We proposed Sundial , a family of generative time series foundation models, which is pre-trained on a trillion (10^12) time points. The model can be applied for both point and probabilistic forecasting, making zero-shot forecasting within milliseconds [GitHub] . Citation If you find this repo helpful, please cite our paper. @inproceedings{liutimer, title={Timer: Generative Pre-trained Transformers Are Large Time Series Models}, author={Liu, Yong and Zhang, Haoran and Li, Chenyu and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng}, booktitle={Forty-first International Conference on Machine Learning} } @article{liu2024timer, title={Timer-XL: Long-Context Transformers for Unified Time Series Forecasting}, author={Liu, Yong and Qin, Guo and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng}, journal={arXiv preprint arXiv:2410.04803}, year={2024} } @article{liu2025sundial, title={Sundial: A Family of Highly Capable Time Series Foundation Models}, author={Liu, Yong and Qin, Guo and Shi, Zhiyuan and Chen, Zhi and Yang, Caiyin and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng}, journal={arXiv preprint arXiv:2502.00816}, year={2025} } Acknowledgment We appreciate the following GitHub repos a lot for their valuable code and datasets: Time-Series-Library ( https://github.com/thuml/Time-Series-Library ) AutoTimes ( https://github.com/thuml/AutoTimes ) LoTSA Data ( https://huggingface.co/datasets/Salesforce/lotsa_data ) UCR Anomaly Archive ( https://arxiv.org/pdf/2009.13807 ) Contributors If you have any questions or want to use the code, feel free to contact: Yong Liu ( liuyong21@mails.tsinghua.edu.cn ) Haoran Zhang ( zhang-hr24@mails.tsinghua.edu.cn ) Chenyu Li ( lichenyu20@mails.tsinghua.edu.cn ) Guo Qin ( qinguo24@mails.tsinghua.edu.cn ) About Official code, datasets and checkpoints for \"Timer: Generative Pre-trained Transformers Are Large Time Series Models\" (ICML 2024) and subsequent works arxiv.org/abs/2402.02368 Topics time-series transformer large-models time-series-analysis foundation-models Resources Readme License MIT license Uh oh! There was an error while loading. Please reload this page . Activity Custom properties Stars 778 stars Watchers 11 watching Forks 78 forks Report repository Releases No releases published Packages 0 No packages published Uh oh! There was an error while loading. Please reload this page . Contributors 2 WenWeiTHU Yong Liu ZDandsomSP ZDand Languages Python 93.3% Shell 6.7% Footer \u00a9 2025 GitHub, Inc. Footer navigation Terms Privacy Security Status Community Docs Contact Manage cookies Do not share my personal information You can\u2019t perform that action at this time.",
  "_binary": false
}
```

### 6. 6
**Status**: success

```json
{
  "status": "success",
  "output": {
    "title": "Research Results",
    "content": "Based on the research findings, provide a direct answer to: Download Epoch AI's dataset of large-scale AI models. From this, extract a time series showing how the maximum amount of compute used to train any AI system has changed over time. Each entry in your response must represent a training run which, at the time it happened, set a new record for the maximum amount of compute used to train an AI system.\n\nInclude:\n- The specific information requested\n- Supporting evidence\n- Source citations\n- Clear, factual presentation"
  }
}
```


## Summary

The agent has completed the research task. Please review the results above.